CHORD FRB beamformer
Compiling kernel...
// PTX CompilerJob of kernel #frb(CuDeviceVector{Int32, 1}, CuDeviceVector{Float16x2, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Float16x2, 1}, CuDeviceVector{Int32, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Float16x2, 1}) for sm_86, minthreads=768, blocks_per_sm=1

//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_86
.address_size 64

	// .globl	_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE // -- Begin function _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE
.extern .func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
;
.extern .func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[8]
)
;
.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception1[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE
.visible .entry _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE(
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_0[8],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_1[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_2[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_4[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_5[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_6[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_7[32],
	.param .align 8 .b8 _Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_8[32]
)
.reqntid 768, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<196>;
	.reg .b16 	%rs<3786>;
	.reg .f32 	%f<202>;
	.reg .b32 	%r<29365>;
	.reg .b64 	%rd<1489>;

// %bb.0:                               // %conversion
	// begin inline asm
	mov.u32 %r1198, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p1, %r1198, 76895;
	@%p1 bra 	LBB0_2;
	bra.uni 	LBB0_1;
LBB0_2:                                 // %L11
	ld.param.u64 	%rd38, [_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_5];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r1205, %r1, 768;
	mov.u32 	%r2, %tid.x;
	or.b32  	%r1206, %r2, %r1205;
	mov.u32 	%r3, %tid.y;
	shl.b32 	%r4, %r3, 5;
	add.s32 	%r1207, %r4, %r1206;
	mul.wide.u32 	%rd44, %r1207, 4;
	add.s64 	%rd5, %rd38, %rd44;
	mov.u32 	%r1208, 1;
	st.global.u32 	[%rd5], %r1208;
	and.b32  	%r5, %r2, 3;
	shr.u32 	%r6, %r2, 2;
	mul.lo.s32 	%r1209, %r5, %r6;
	and.b32  	%r1210, %r1209, 7;
	cvt.rn.f32.u32 	%f18, %r1210;
	mov.f32 	%f19, 0f40800000;
	div.approx.f32 	%f20, %f18, %f19;
	add.f32 	%f21, %f20, %f20;
	mov.b32 	%r1211, %f21;
	and.b32  	%r1212, %r1211, -2147483648;
	or.b32  	%r1213, %r1212, 1056964608;
	mov.b32 	%f22, %r1213;
	add.f32 	%f23, %f21, %f22;
	cvt.rzi.f32.f32 	%f24, %f23;
	abs.f32 	%f25, %f21;
	setp.gt.f32 	%p2, %f25, 0f4B000000;
	selp.f32 	%f26, %f21, %f24, %p2;
	cvt.rzi.f32.f32 	%f27, %f21;
	setp.lt.f32 	%p3, %f25, 0f3F000000;
	selp.f32 	%f28, %f27, %f26, %p3;
	cvt.rzi.s32.f32 	%r1214, %f28;
	fma.rn.f32 	%f29, %f28, 0fBF000000, %f20;
	mul.f32 	%f30, %f29, 0f34222169;
	fma.rn.f32 	%f31, %f29, 0f40490FDA, %f30;
	mul.f32 	%f32, %f31, %f31;
	fma.rn.f32 	%f33, %f32, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f34, %f33, %f32, 0f3D2AAABB;
	fma.rn.f32 	%f35, %f34, %f32, 0fBEFFFFFF;
	fma.rn.f32 	%f36, %f35, %f32, 0f3F800000;
	fma.rn.f32 	%f37, %f32, %f31, 0f00000000;
	fma.rn.f32 	%f38, %f32, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f39, %f38, %f32, 0fBE2AAAA8;
	fma.rn.f32 	%f40, %f39, %f37, %f31;
	and.b32  	%r1215, %r1214, 1;
	setp.eq.b32 	%p4, %r1215, 1;
	selp.f32 	%f41, %f36, %f40, %p4;
	selp.f32 	%f42, %f40, %f36, %p4;
	and.b32  	%r1216, %r1214, 2;
	setp.eq.s32 	%p5, %r1216, 0;
	neg.f32 	%f43, %f41;
	selp.f32 	%f44, %f41, %f43, %p5;
	add.s32 	%r1217, %r1214, 1;
	and.b32  	%r1218, %r1217, 2;
	mov.f32 	%f200, 0f00000000;
	cvt.rzi.f32.f32 	%f47, %f20;
	setp.eq.f32 	%p7, %f47, %f20;
	mul.f32 	%f48, %f20, 0f00000000;
	shl.b32 	%r9, %r2, 1;
	and.b32  	%r10, %r9, 6;
	or.b32  	%r11, %r10, 1;
	setp.eq.s32 	%p9, %r10, 6;
	mov.f32 	%f192, 0f41C00000;
	mul.lo.s32 	%r28844, %r10, %r6;
	mov.f32 	%f194, %f200;
	mov.f32 	%f195, %f200;
	@%p9 bra 	LBB0_4;
// %bb.3:                               // %L206
	cvt.u16.u32 	%rs5, %r28844;
	and.b16  	%rs6, %rs5, 255;
	mul.lo.s16 	%rs7, %rs6, 171;
	shr.u16 	%rs8, %rs7, 13;
	mul.lo.s16 	%rs9, %rs8, 48;
	sub.s16 	%rs10, %rs5, %rs9;
	and.b16  	%rs11, %rs10, 255;
	cvt.rn.f32.u16 	%f53, %rs11;
	div.approx.f32 	%f55, %f53, %f192;
	add.f32 	%f56, %f55, %f55;
	mov.b32 	%r1220, %f56;
	and.b32  	%r1221, %r1220, -2147483648;
	or.b32  	%r1222, %r1221, 1056964608;
	mov.b32 	%f57, %r1222;
	add.f32 	%f58, %f56, %f57;
	cvt.rzi.f32.f32 	%f59, %f58;
	abs.f32 	%f60, %f56;
	setp.gt.f32 	%p10, %f60, 0f4B000000;
	selp.f32 	%f61, %f56, %f59, %p10;
	cvt.rzi.f32.f32 	%f62, %f56;
	setp.lt.f32 	%p11, %f60, 0f3F000000;
	selp.f32 	%f63, %f62, %f61, %p11;
	cvt.rzi.s32.f32 	%r1223, %f63;
	fma.rn.f32 	%f64, %f63, 0fBF000000, %f55;
	mul.f32 	%f65, %f64, 0f34222169;
	fma.rn.f32 	%f66, %f64, 0f40490FDA, %f65;
	mul.f32 	%f67, %f66, %f66;
	fma.rn.f32 	%f68, %f67, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f69, %f68, %f67, 0f3D2AAABB;
	fma.rn.f32 	%f70, %f69, %f67, 0fBEFFFFFF;
	fma.rn.f32 	%f71, %f70, %f67, 0f3F800000;
	fma.rn.f32 	%f72, %f67, %f66, 0f00000000;
	fma.rn.f32 	%f73, %f67, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f74, %f73, %f67, 0fBE2AAAA8;
	fma.rn.f32 	%f75, %f74, %f72, %f66;
	and.b32  	%r1224, %r1223, 1;
	setp.eq.b32 	%p12, %r1224, 1;
	selp.f32 	%f76, %f71, %f75, %p12;
	selp.f32 	%f77, %f75, %f71, %p12;
	and.b32  	%r1225, %r1223, 2;
	setp.eq.s32 	%p13, %r1225, 0;
	neg.f32 	%f78, %f76;
	selp.f32 	%f79, %f76, %f78, %p13;
	add.s32 	%r1226, %r1223, 1;
	and.b32  	%r1227, %r1226, 2;
	setp.eq.s32 	%p14, %r1227, 0;
	mov.f32 	%f80, 0f00000000;
	sub.f32 	%f81, %f80, %f77;
	selp.f32 	%f82, %f77, %f81, %p14;
	cvt.rzi.f32.f32 	%f83, %f55;
	setp.eq.f32 	%p15, %f83, %f55;
	mul.f32 	%f84, %f55, 0f00000000;
	selp.f32 	%f195, %f84, %f79, %p15;
	abs.f32 	%f85, %f55;
	setp.gt.f32 	%p16, %f85, 0f4B800000;
	add.f32 	%f86, %f195, 0f3F800000;
	selp.f32 	%f194, %f86, %f82, %p16;
LBB0_4:                                 // %L221
	setp.eq.s32 	%p6, %r1218, 0;
	sub.f32 	%f45, %f200, %f42;
	selp.f32 	%f49, %f48, %f44, %p7;
	abs.f32 	%f50, %f20;
	setp.gt.u32 	%p17, %r11, 5;
	mul.lo.s32 	%r28845, %r11, %r6;
	mov.f32 	%f196, %f200;
	mov.f32 	%f197, %f200;
	@%p17 bra 	LBB0_6;
// %bb.5:                               // %L225
	cvt.u16.u32 	%rs12, %r28845;
	and.b16  	%rs13, %rs12, 255;
	mul.lo.s16 	%rs14, %rs13, 171;
	shr.u16 	%rs15, %rs14, 13;
	mul.lo.s16 	%rs16, %rs15, 48;
	sub.s16 	%rs17, %rs12, %rs16;
	and.b16  	%rs18, %rs17, 255;
	cvt.rn.f32.u16 	%f88, %rs18;
	div.approx.f32 	%f90, %f88, %f192;
	add.f32 	%f91, %f90, %f90;
	mov.b32 	%r1229, %f91;
	and.b32  	%r1230, %r1229, -2147483648;
	or.b32  	%r1231, %r1230, 1056964608;
	mov.b32 	%f92, %r1231;
	add.f32 	%f93, %f91, %f92;
	cvt.rzi.f32.f32 	%f94, %f93;
	abs.f32 	%f95, %f91;
	setp.gt.f32 	%p18, %f95, 0f4B000000;
	selp.f32 	%f96, %f91, %f94, %p18;
	cvt.rzi.f32.f32 	%f97, %f91;
	setp.lt.f32 	%p19, %f95, 0f3F000000;
	selp.f32 	%f98, %f97, %f96, %p19;
	cvt.rzi.s32.f32 	%r1232, %f98;
	fma.rn.f32 	%f99, %f98, 0fBF000000, %f90;
	mul.f32 	%f100, %f99, 0f34222169;
	fma.rn.f32 	%f101, %f99, 0f40490FDA, %f100;
	mul.f32 	%f102, %f101, %f101;
	fma.rn.f32 	%f103, %f102, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f104, %f103, %f102, 0f3D2AAABB;
	fma.rn.f32 	%f105, %f104, %f102, 0fBEFFFFFF;
	fma.rn.f32 	%f106, %f105, %f102, 0f3F800000;
	fma.rn.f32 	%f107, %f102, %f101, 0f00000000;
	fma.rn.f32 	%f108, %f102, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f109, %f108, %f102, 0fBE2AAAA8;
	fma.rn.f32 	%f110, %f109, %f107, %f101;
	and.b32  	%r1233, %r1232, 1;
	setp.eq.b32 	%p20, %r1233, 1;
	selp.f32 	%f111, %f106, %f110, %p20;
	selp.f32 	%f112, %f110, %f106, %p20;
	and.b32  	%r1234, %r1232, 2;
	setp.eq.s32 	%p21, %r1234, 0;
	neg.f32 	%f113, %f111;
	selp.f32 	%f114, %f111, %f113, %p21;
	add.s32 	%r1235, %r1232, 1;
	and.b32  	%r1236, %r1235, 2;
	setp.eq.s32 	%p22, %r1236, 0;
	mov.f32 	%f115, 0f00000000;
	sub.f32 	%f116, %f115, %f112;
	selp.f32 	%f117, %f112, %f116, %p22;
	cvt.rzi.f32.f32 	%f118, %f90;
	setp.eq.f32 	%p23, %f118, %f90;
	mul.f32 	%f119, %f90, 0f00000000;
	selp.f32 	%f197, %f119, %f114, %p23;
	abs.f32 	%f120, %f90;
	setp.gt.f32 	%p24, %f120, 0f4B800000;
	add.f32 	%f121, %f197, 0f3F800000;
	selp.f32 	%f196, %f121, %f117, %p24;
LBB0_6:                                 // %L240
	selp.f32 	%f46, %f42, %f45, %p6;
	setp.gt.f32 	%p8, %f50, 0f4B800000;
	add.f32 	%f51, %f49, 0f3F800000;
	setp.gt.u32 	%p26, %r2, 23;
	or.pred  	%p27, %p9, %p26;
	mov.f32 	%f193, 0f40400000;
	mov.f32 	%f198, %f200;
	mov.f32 	%f199, %f200;
	@%p27 bra 	LBB0_8;
// %bb.7:                               // %L282
	cvt.u16.u32 	%rs19, %r28844;
	and.b16  	%rs20, %rs19, 255;
	mul.lo.s16 	%rs21, %rs20, 171;
	shr.u16 	%rs22, %rs21, 10;
	mul.lo.s16 	%rs23, %rs22, 6;
	sub.s16 	%rs24, %rs19, %rs23;
	and.b16  	%rs25, %rs24, 255;
	cvt.rn.f32.u16 	%f123, %rs25;
	div.approx.f32 	%f125, %f123, %f193;
	add.f32 	%f126, %f125, %f125;
	mov.b32 	%r1244, %f126;
	and.b32  	%r1245, %r1244, -2147483648;
	or.b32  	%r1246, %r1245, 1056964608;
	mov.b32 	%f127, %r1246;
	add.f32 	%f128, %f126, %f127;
	cvt.rzi.f32.f32 	%f129, %f128;
	abs.f32 	%f130, %f126;
	setp.gt.f32 	%p28, %f130, 0f4B000000;
	selp.f32 	%f131, %f126, %f129, %p28;
	cvt.rzi.f32.f32 	%f132, %f126;
	setp.lt.f32 	%p29, %f130, 0f3F000000;
	selp.f32 	%f133, %f132, %f131, %p29;
	cvt.rzi.s32.f32 	%r1247, %f133;
	fma.rn.f32 	%f134, %f133, 0fBF000000, %f125;
	mul.f32 	%f135, %f134, 0f34222169;
	fma.rn.f32 	%f136, %f134, 0f40490FDA, %f135;
	mul.f32 	%f137, %f136, %f136;
	fma.rn.f32 	%f138, %f137, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f139, %f138, %f137, 0f3D2AAABB;
	fma.rn.f32 	%f140, %f139, %f137, 0fBEFFFFFF;
	fma.rn.f32 	%f141, %f140, %f137, 0f3F800000;
	fma.rn.f32 	%f142, %f137, %f136, 0f00000000;
	fma.rn.f32 	%f143, %f137, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f144, %f143, %f137, 0fBE2AAAA8;
	fma.rn.f32 	%f145, %f144, %f142, %f136;
	and.b32  	%r1248, %r1247, 1;
	setp.eq.b32 	%p30, %r1248, 1;
	selp.f32 	%f146, %f141, %f145, %p30;
	selp.f32 	%f147, %f145, %f141, %p30;
	and.b32  	%r1249, %r1247, 2;
	setp.eq.s32 	%p31, %r1249, 0;
	neg.f32 	%f148, %f146;
	selp.f32 	%f149, %f146, %f148, %p31;
	add.s32 	%r1250, %r1247, 1;
	and.b32  	%r1251, %r1250, 2;
	setp.eq.s32 	%p32, %r1251, 0;
	mov.f32 	%f150, 0f00000000;
	sub.f32 	%f151, %f150, %f147;
	selp.f32 	%f152, %f147, %f151, %p32;
	cvt.rzi.f32.f32 	%f153, %f125;
	setp.eq.f32 	%p33, %f153, %f125;
	mul.f32 	%f154, %f125, 0f00000000;
	selp.f32 	%f199, %f154, %f149, %p33;
	abs.f32 	%f155, %f125;
	setp.gt.f32 	%p34, %f155, 0f4B800000;
	add.f32 	%f156, %f199, 0f3F800000;
	selp.f32 	%f198, %f156, %f152, %p34;
LBB0_8:                                 // %L297
	selp.f32 	%f52, %f51, %f46, %p8;
	or.pred  	%p37, %p17, %p26;
	mov.f32 	%f201, %f200;
	@%p37 bra 	LBB0_10;
// %bb.9:                               // %L305
	cvt.u16.u32 	%rs26, %r28845;
	and.b16  	%rs27, %rs26, 255;
	mul.lo.s16 	%rs28, %rs27, 171;
	shr.u16 	%rs29, %rs28, 10;
	mul.lo.s16 	%rs30, %rs29, 6;
	sub.s16 	%rs31, %rs26, %rs30;
	and.b16  	%rs32, %rs31, 255;
	cvt.rn.f32.u16 	%f158, %rs32;
	div.approx.f32 	%f160, %f158, %f193;
	add.f32 	%f161, %f160, %f160;
	mov.b32 	%r1253, %f161;
	and.b32  	%r1254, %r1253, -2147483648;
	or.b32  	%r1255, %r1254, 1056964608;
	mov.b32 	%f162, %r1255;
	add.f32 	%f163, %f161, %f162;
	cvt.rzi.f32.f32 	%f164, %f163;
	abs.f32 	%f165, %f161;
	setp.gt.f32 	%p38, %f165, 0f4B000000;
	selp.f32 	%f166, %f161, %f164, %p38;
	cvt.rzi.f32.f32 	%f167, %f161;
	setp.lt.f32 	%p39, %f165, 0f3F000000;
	selp.f32 	%f168, %f167, %f166, %p39;
	cvt.rzi.s32.f32 	%r1256, %f168;
	fma.rn.f32 	%f169, %f168, 0fBF000000, %f160;
	mul.f32 	%f170, %f169, 0f34222169;
	fma.rn.f32 	%f171, %f169, 0f40490FDA, %f170;
	mul.f32 	%f172, %f171, %f171;
	fma.rn.f32 	%f173, %f172, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f174, %f173, %f172, 0f3D2AAABB;
	fma.rn.f32 	%f175, %f174, %f172, 0fBEFFFFFF;
	fma.rn.f32 	%f176, %f175, %f172, 0f3F800000;
	fma.rn.f32 	%f177, %f172, %f171, 0f00000000;
	fma.rn.f32 	%f178, %f172, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f179, %f178, %f172, 0fBE2AAAA8;
	fma.rn.f32 	%f180, %f179, %f177, %f171;
	and.b32  	%r1257, %r1256, 1;
	setp.eq.b32 	%p40, %r1257, 1;
	selp.f32 	%f181, %f176, %f180, %p40;
	selp.f32 	%f182, %f180, %f176, %p40;
	and.b32  	%r1258, %r1256, 2;
	setp.eq.s32 	%p41, %r1258, 0;
	neg.f32 	%f183, %f181;
	selp.f32 	%f184, %f181, %f183, %p41;
	add.s32 	%r1259, %r1256, 1;
	and.b32  	%r1260, %r1259, 2;
	setp.eq.s32 	%p42, %r1260, 0;
	mov.f32 	%f185, 0f00000000;
	sub.f32 	%f186, %f185, %f182;
	selp.f32 	%f187, %f182, %f186, %p42;
	cvt.rzi.f32.f32 	%f188, %f160;
	setp.eq.f32 	%p43, %f188, %f160;
	mul.f32 	%f189, %f160, 0f00000000;
	selp.f32 	%f201, %f189, %f184, %p43;
	abs.f32 	%f190, %f160;
	setp.gt.f32 	%p44, %f190, 0f4B800000;
	add.f32 	%f191, %f201, 0f3F800000;
	selp.f32 	%f200, %f191, %f187, %p44;
LBB0_10:                                // %L320
	mov.b32 	%r1201, %f52;
	mov.b32 	%r1204, %f49;
	mov.b32 	%r1262, %f198;
	mov.b32 	%r1263, %f200;
	mov.b32 	%r1268, %f199;
	mov.b32 	%r1269, %f201;
	mov.u32 	%r28847, 999999999;
	cvt.u16.u32 	%rs3778, %r2;
	@%p26 bra 	LBB0_12;
// %bb.11:                              // %L363
	ld.param.u64 	%rd1, [_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_1];
	and.b16  	%rs34, %rs3778, 255;
	mul.lo.s16 	%rs35, %rs34, 171;
	shr.u16 	%rs36, %rs35, 12;
	mul.lo.s16 	%rs37, %rs36, 24;
	sub.s16 	%rs38, %rs3778, %rs37;
	cvt.u32.u16 	%r1274, %rs38;
	and.b32  	%r1275, %r1274, 255;
	mad.lo.s32 	%r1276, %r1275, 24, %r3;
	cvt.u16.u32 	%rs39, %r1276;
	mul.hi.u16 	%rs40, %rs39, -7281;
	shr.u16 	%rs41, %rs40, 9;
	mul.lo.s16 	%rs42, %rs41, 576;
	sub.s16 	%rs43, %rs39, %rs42;
	cvt.u32.u16 	%r1277, %rs43;
	mul.wide.u32 	%rd45, %r1277, 4;
	add.s64 	%rd46, %rd1, %rd45;
	ld.global.u32 	%r28847, [%rd46];
LBB0_12:                                // %L440
	xor.b32  	%r1200, %r1204, -2147483648;
	mov.b32 	%r1238, %f194;
	mov.b32 	%r1239, %f196;
	mov.b32 	%r1241, %f195;
	mov.b32 	%r1242, %f197;
	xor.b32  	%r1265, %r1268, -2147483648;
	xor.b32  	%r1266, %r1269, -2147483648;
	and.b32  	%r20, %r2, 24;
	setp.ne.s32 	%p46, %r20, 24;
	cvt.u16.u32 	%rs3784, %r6;
	@%p46 bra 	LBB0_14;
// %bb.13:                              // %L440.L785_crit_edge
	mul.lo.s16 	%rs57, %rs3784, 171;
	shr.u16 	%rs58, %rs57, 10;
	mul.lo.s16 	%rs59, %rs58, 6;
	sub.s16 	%rs60, %rs3784, %rs59;
	cvt.u32.u16 	%r1294, %rs60;
	and.b32  	%r28851, %r1294, 255;
	cvt.u16.u32 	%rs61, %r3;
	and.b16  	%rs62, %rs61, 255;
	mul.lo.s16 	%rs63, %rs62, 171;
	shr.u16 	%rs64, %rs63, 10;
	cvt.u32.u16 	%r28850, %rs64;
	mul.lo.s16 	%rs65, %rs64, -6;
	add.s16 	%rs66, %rs65, %rs61;
	cvt.u32.u16 	%r1296, %rs66;
	and.b32  	%r28849, %r1296, 255;
	mul.wide.u16 	%r28848, %rs64, 6;
	mov.u32 	%r28852, 0;
	mov.u32 	%r28853, %r28852;
	bra.uni 	LBB0_15;
LBB0_14:                                // %L469
	ld.param.u64 	%rd2, [_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_2];
	mul.lo.s32 	%r1278, %r1, 576;
	cvt.u16.u32 	%rs44, %r3;
	and.b16  	%rs45, %rs44, 255;
	mul.lo.s16 	%rs46, %rs45, 171;
	shr.u16 	%rs47, %rs46, 10;
	mul.lo.s16 	%rs48, %rs47, -6;
	add.s16 	%rs49, %rs48, %rs44;
	cvt.u32.u16 	%r1280, %rs49;
	and.b32  	%r28849, %r1280, 255;
	or.b32  	%r1281, %r1278, %r28849;
	mul.lo.s32 	%r1282, %r5, 144;
	add.s32 	%r1283, %r1281, %r1282;
	mul.lo.s16 	%rs51, %rs3784, 171;
	shr.u16 	%rs52, %rs51, 10;
	mul.lo.s16 	%rs53, %rs52, 6;
	sub.s16 	%rs54, %rs3784, %rs53;
	cvt.u32.u16 	%r1284, %rs54;
	and.b32  	%r28851, %r1284, 255;
	and.b16  	%rs55, %rs54, 255;
	mul.wide.u16 	%r1285, %rs55, 24;
	add.s32 	%r1286, %r1283, %r1285;
	cvt.u32.u16 	%r28850, %rs47;
	mul.wide.u16 	%r28848, %rs47, 6;
	add.s32 	%r1287, %r1286, %r28848;
	mul.wide.u32 	%rd47, %r1287, 4;
	add.s64 	%rd48, %rd2, %rd47;
	ld.global.u32 	%r28852, [%rd48];
	add.s32 	%r1288, %r1278, 48384;
	or.b32  	%r1289, %r1288, %r28849;
	add.s32 	%r1290, %r1289, %r1282;
	add.s32 	%r1291, %r1290, %r1285;
	add.s32 	%r1292, %r1291, %r28848;
	mul.wide.u32 	%rd49, %r1292, 4;
	add.s64 	%rd50, %rd2, %rd49;
	ld.global.u32 	%r28853, [%rd50];
LBB0_15:                                // %L785
	ld.param.u64 	%rd3, [_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_3];
	ld.param.u64 	%rd4, [_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_4];
	// begin inline asm
	cvt.rn.f16x2.f32 %r1199, %r1201, %r1200;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1202, %r1204, %r1201;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1237, %r1239, %r1238;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1240, %r1242, %r1241;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1261, %r1263, %r1262;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1264, %r1266, %r1265;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1267, %r1269, %r1268;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r1270, %r1263, %r1262;
	// end inline asm
	mov.u32 	%r28846, 0;
	shl.b32 	%r1298, %r1, 7;
	shr.u32 	%r37, %r2, 4;
	mad.lo.s32 	%r38, %r37, 10752, %r1298;
	shl.b32 	%r39, %r2, 2;
	and.b32  	%r40, %r39, 60;
	and.b32  	%r1299, %r9, 30;
	or.b32  	%r1300, %r37, %r1299;
	mul.lo.s32 	%r41, %r1300, 257;
	or.b32  	%r1301, %r9, %r37;
	or.b32  	%r1302, %r1301, 32;
	mul.lo.s32 	%r42, %r1302, 257;
	and.b16  	%rs69, %rs3778, 255;
	mul.lo.s16 	%rs70, %rs69, 171;
	shr.u16 	%rs71, %rs70, 12;
	mul.lo.s16 	%rs72, %rs71, 24;
	sub.s16 	%rs73, %rs3778, %rs72;
	cvt.u32.u16 	%r1303, %rs73;
	and.b32  	%r43, %r1303, 255;
	and.b32  	%r44, %r4, 224;
	shr.u32 	%r1304, %r3, 3;
	mul.lo.s32 	%r45, %r1304, 257;
	add.s32 	%r1305, %r3, 24;
	shr.u32 	%r1306, %r1305, 3;
	mul.lo.s32 	%r46, %r1306, 257;
	add.s32 	%r1307, %r3, 48;
	shr.u32 	%r1308, %r1307, 3;
	mul.lo.s32 	%r47, %r1308, 257;
	add.s32 	%r1309, %r3, 72;
	shr.u32 	%r1310, %r1309, 3;
	mul.lo.s32 	%r48, %r1310, 257;
	or.b32  	%r1311, %r1304, 12;
	mul.lo.s32 	%r49, %r1311, 257;
	add.s32 	%r1312, %r3, 120;
	shr.u32 	%r1313, %r1312, 3;
	mul.lo.s32 	%r50, %r1313, 257;
	add.s32 	%r1314, %r3, 144;
	shr.u32 	%r1315, %r1314, 3;
	mul.lo.s32 	%r51, %r1315, 257;
	add.s32 	%r1316, %r3, 168;
	shr.u32 	%r1317, %r1316, 3;
	mul.lo.s32 	%r52, %r1317, 257;
	or.b32  	%r1318, %r1304, 24;
	mul.lo.s32 	%r53, %r1318, 257;
	add.s32 	%r1319, %r3, 216;
	shr.u32 	%r1320, %r1319, 3;
	mul.lo.s32 	%r54, %r1320, 257;
	add.s32 	%r1321, %r3, 240;
	shr.u32 	%r1322, %r1321, 3;
	mul.lo.s32 	%r55, %r1322, 257;
	add.s32 	%r1323, %r3, 264;
	shr.u32 	%r1324, %r1323, 3;
	mul.lo.s32 	%r56, %r1324, 257;
	or.b32  	%r1325, %r1304, 36;
	mul.lo.s32 	%r57, %r1325, 257;
	add.s32 	%r1326, %r3, 312;
	shr.u32 	%r1327, %r1326, 3;
	mul.lo.s32 	%r58, %r1327, 257;
	add.s32 	%r1328, %r3, 336;
	shr.u32 	%r1329, %r1328, 3;
	mul.lo.s32 	%r59, %r1329, 257;
	add.s32 	%r1330, %r3, 360;
	shr.u32 	%r1331, %r1330, 3;
	mul.lo.s32 	%r60, %r1331, 257;
	or.b32  	%r1332, %r1304, 48;
	mul.lo.s32 	%r61, %r1332, 257;
	add.s32 	%r1333, %r3, 408;
	shr.u32 	%r1334, %r1333, 3;
	mul.lo.s32 	%r62, %r1334, 257;
	add.s32 	%r1335, %r3, 432;
	shr.u32 	%r1336, %r1335, 3;
	mul.lo.s32 	%r63, %r1336, 257;
	add.s32 	%r1337, %r3, 456;
	shr.u32 	%r1338, %r1337, 3;
	mul.lo.s32 	%r64, %r1338, 257;
	or.b32  	%r1339, %r1304, 60;
	mul.lo.s32 	%r65, %r1339, 257;
	add.s32 	%r1340, %r3, 504;
	bfe.u32 	%r1341, %r1340, 3, 6;
	mul.lo.s32 	%r66, %r1341, 257;
	mul.lo.s32 	%r67, %r28851, 801;
	mul.lo.s32 	%r68, %r28850, 198;
	mul.lo.s32 	%r69, %r5, 4806;
	mul.lo.s32 	%r70, %r28849, 33;
	and.b32  	%r1342, %r2, 1;
	mul.lo.s32 	%r71, %r1342, 4112;
	add.s32 	%r1343, %r28848, %r71;
	and.b32  	%r1344, %r6, 1;
	mul.lo.s32 	%r72, %r1344, 1028;
	add.s32 	%r1345, %r1343, %r72;
	add.s32 	%r1346, %r1345, %r28849;
	bfe.u32 	%r1347, %r2, 1, 1;
	mul.lo.s32 	%r73, %r1347, 2056;
	add.s32 	%r1348, %r1346, %r73;
	bfe.u32 	%r1349, %r2, 3, 1;
	mul.lo.s32 	%r74, %r1349, 514;
	add.s32 	%r1350, %r1348, %r74;
	mul.lo.s32 	%r75, %r37, 257;
	add.s32 	%r1351, %r1350, %r75;
	mul.wide.u32 	%rd51, %r1351, 4;
	mov.u64 	%rd52, shmem;
	add.s64 	%rd6, %rd52, %rd51;
	cvt.u64.u32 	%rd53, %r75;
	cvt.u64.u32 	%rd54, %r74;
	cvt.u64.u32 	%rd55, %r73;
	cvt.u64.u32 	%rd56, %r28849;
	cvt.u64.u32 	%rd57, %r72;
	cvt.u64.u32 	%rd58, %r28848;
	cvt.u64.u32 	%rd59, %r71;
	add.s64 	%rd60, %rd59, %rd58;
	add.s64 	%rd61, %rd60, %rd57;
	add.s64 	%rd62, %rd61, %rd56;
	add.s64 	%rd63, %rd62, %rd55;
	add.s64 	%rd64, %rd63, %rd54;
	add.s64 	%rd65, %rd64, %rd53;
	shl.b64 	%rd66, %rd65, 2;
	add.s64 	%rd7, %rd52, %rd66;
	or.b32  	%r76, %r28848, 32;
	add.s32 	%r1352, %r76, %r71;
	add.s32 	%r1353, %r1352, %r72;
	add.s32 	%r1354, %r1353, %r28849;
	add.s32 	%r1355, %r1354, %r73;
	add.s32 	%r1356, %r1355, %r74;
	add.s32 	%r1357, %r1356, %r75;
	mul.wide.s32 	%rd67, %r1357, 4;
	add.s64 	%rd8, %rd52, %rd67;
	add.s32 	%r1358, %r1357, 8256;
	mul.wide.s32 	%rd68, %r1358, 4;
	add.s64 	%rd9, %rd52, %rd68;
	mul.lo.s32 	%r77, %r5, 6;
	and.b32  	%r1359, %r3, 1;
	mul.lo.s32 	%r78, %r1359, 4112;
	add.s32 	%r1360, %r78, %r77;
	bfe.u32 	%r1361, %r3, 2, 1;
	mul.lo.s32 	%r79, %r1361, 1028;
	add.s32 	%r1362, %r1360, %r79;
	add.s32 	%r1363, %r1362, %r28851;
	bfe.u32 	%r1364, %r3, 1, 1;
	mul.lo.s32 	%r80, %r1364, 2056;
	add.s32 	%r1365, %r1363, %r80;
	bfe.u32 	%r1366, %r3, 3, 1;
	mul.lo.s32 	%r81, %r1366, 514;
	add.s32 	%r1367, %r1365, %r81;
	shr.u32 	%r1368, %r3, 4;
	mul.lo.s32 	%r82, %r1368, 257;
	add.s32 	%r1369, %r1367, %r82;
	mul.wide.u32 	%rd69, %r1369, 4;
	add.s64 	%rd10, %rd52, %rd69;
	cvt.u64.u32 	%rd70, %r82;
	cvt.u64.u32 	%rd71, %r81;
	cvt.u64.u32 	%rd72, %r80;
	cvt.u64.u32 	%rd73, %r28851;
	cvt.u64.u32 	%rd74, %r79;
	cvt.u64.u32 	%rd75, %r78;
	cvt.u64.u32 	%rd76, %r77;
	add.s64 	%rd77, %rd76, %rd75;
	add.s64 	%rd78, %rd77, %rd74;
	add.s64 	%rd79, %rd78, %rd73;
	add.s64 	%rd80, %rd79, %rd72;
	add.s64 	%rd81, %rd80, %rd71;
	add.s64 	%rd82, %rd81, %rd70;
	shl.b64 	%rd83, %rd82, 2;
	add.s64 	%rd11, %rd52, %rd83;
	mul.lo.s32 	%r83, %r1, 1152;
	cvt.u16.u32 	%rs2, %r3;
	shl.b16 	%rs74, %rs2, 1;
	and.b16  	%rs75, %rs74, 254;
	mul.lo.s16 	%rs76, %rs75, 171;
	shr.u16 	%rs77, %rs76, 13;
	mul.lo.s16 	%rs78, %rs77, 48;
	sub.s16 	%rs79, %rs74, %rs78;
	and.b16  	%rs80, %rs79, 254;
	mul.wide.u16 	%r84, %rs80, 24;
	or.b16  	%rs81, %rs74, 1;
	and.b16  	%rs82, %rs81, 255;
	mul.lo.s16 	%rs83, %rs82, 171;
	shr.u16 	%rs84, %rs83, 13;
	mul.lo.s16 	%rs85, %rs84, 48;
	sub.s16 	%rs86, %rs81, %rs85;
	and.b16  	%rs87, %rs86, 255;
	mul.wide.u16 	%r85, %rs87, 24;
	cvt.u64.u32 	%rd84, %r70;
	cvt.u64.u32 	%rd85, %r69;
	cvt.u64.u32 	%rd86, %r68;
	cvt.u64.u32 	%rd87, %r67;
	add.s64 	%rd88, %rd87, %rd86;
	add.s64 	%rd89, %rd88, %rd85;
	add.s64 	%rd90, %rd89, %rd84;
	shl.b64 	%rd91, %rd90, 2;
	add.s64 	%rd12, %rd52, %rd91;
	add.s32 	%r1370, %r71, 64;
	add.s32 	%r1371, %r1370, %r28848;
	add.s32 	%r1372, %r1371, %r72;
	add.s32 	%r1373, %r1372, %r28849;
	add.s32 	%r1374, %r1373, %r73;
	add.s32 	%r1375, %r1374, %r74;
	add.s32 	%r1376, %r1375, %r75;
	mul.wide.u32 	%rd92, %r1376, 4;
	add.s64 	%rd13, %rd52, %rd92;
	add.s32 	%r1377, %r71, 8320;
	add.s32 	%r1378, %r1377, %r28848;
	add.s32 	%r1379, %r1378, %r72;
	add.s32 	%r1380, %r1379, %r28849;
	add.s32 	%r1381, %r1380, %r73;
	add.s32 	%r1382, %r1381, %r74;
	add.s32 	%r1383, %r1382, %r75;
	mul.wide.u32 	%rd93, %r1383, 4;
	add.s64 	%rd14, %rd52, %rd93;
	add.s32 	%r1384, %r1370, %r76;
	add.s32 	%r1385, %r1384, %r72;
	add.s32 	%r1386, %r1385, %r28849;
	add.s32 	%r1387, %r1386, %r73;
	add.s32 	%r1388, %r1387, %r74;
	add.s32 	%r1389, %r1388, %r75;
	mul.wide.s32 	%rd94, %r1389, 4;
	add.s64 	%rd15, %rd52, %rd94;
	add.s32 	%r1390, %r1377, %r76;
	add.s32 	%r1391, %r1390, %r72;
	add.s32 	%r1392, %r1391, %r28849;
	add.s32 	%r1393, %r1392, %r73;
	add.s32 	%r1394, %r1393, %r74;
	add.s32 	%r1395, %r1394, %r75;
	mul.wide.s32 	%rd95, %r1395, 4;
	add.s64 	%rd16, %rd52, %rd95;
	add.s32 	%r1396, %r71, 128;
	add.s32 	%r1397, %r1396, %r28848;
	add.s32 	%r1398, %r1397, %r72;
	add.s32 	%r1399, %r1398, %r28849;
	add.s32 	%r1400, %r1399, %r73;
	add.s32 	%r1401, %r1400, %r74;
	add.s32 	%r1402, %r1401, %r75;
	mul.wide.u32 	%rd96, %r1402, 4;
	add.s64 	%rd17, %rd52, %rd96;
	add.s32 	%r1403, %r71, 8384;
	add.s32 	%r1404, %r1403, %r28848;
	add.s32 	%r1405, %r1404, %r72;
	add.s32 	%r1406, %r1405, %r28849;
	add.s32 	%r1407, %r1406, %r73;
	add.s32 	%r1408, %r1407, %r74;
	add.s32 	%r1409, %r1408, %r75;
	mul.wide.u32 	%rd97, %r1409, 4;
	add.s64 	%rd18, %rd52, %rd97;
	add.s32 	%r1410, %r1396, %r76;
	add.s32 	%r1411, %r1410, %r72;
	add.s32 	%r1412, %r1411, %r28849;
	add.s32 	%r1413, %r1412, %r73;
	add.s32 	%r1414, %r1413, %r74;
	add.s32 	%r1415, %r1414, %r75;
	mul.wide.s32 	%rd98, %r1415, 4;
	add.s64 	%rd19, %rd52, %rd98;
	add.s32 	%r1416, %r1403, %r76;
	add.s32 	%r1417, %r1416, %r72;
	add.s32 	%r1418, %r1417, %r28849;
	add.s32 	%r1419, %r1418, %r73;
	add.s32 	%r1420, %r1419, %r74;
	add.s32 	%r1421, %r1420, %r75;
	mul.wide.s32 	%rd99, %r1421, 4;
	add.s64 	%rd20, %rd52, %rd99;
	add.s32 	%r1422, %r71, 192;
	add.s32 	%r1423, %r1422, %r28848;
	add.s32 	%r1424, %r1423, %r72;
	add.s32 	%r1425, %r1424, %r28849;
	add.s32 	%r1426, %r1425, %r73;
	add.s32 	%r1427, %r1426, %r74;
	add.s32 	%r1428, %r1427, %r75;
	mul.wide.u32 	%rd100, %r1428, 4;
	add.s64 	%rd21, %rd52, %rd100;
	add.s32 	%r1429, %r71, 8448;
	add.s32 	%r1430, %r1429, %r28848;
	add.s32 	%r1431, %r1430, %r72;
	add.s32 	%r1432, %r1431, %r28849;
	add.s32 	%r1433, %r1432, %r73;
	add.s32 	%r1434, %r1433, %r74;
	add.s32 	%r1435, %r1434, %r75;
	mul.wide.u32 	%rd101, %r1435, 4;
	add.s64 	%rd22, %rd52, %rd101;
	add.s32 	%r1436, %r1422, %r76;
	add.s32 	%r1437, %r1436, %r72;
	add.s32 	%r1438, %r1437, %r28849;
	add.s32 	%r1439, %r1438, %r73;
	add.s32 	%r1440, %r1439, %r74;
	add.s32 	%r1441, %r1440, %r75;
	mul.wide.s32 	%rd102, %r1441, 4;
	add.s64 	%rd23, %rd52, %rd102;
	add.s32 	%r1442, %r1429, %r76;
	add.s32 	%r1443, %r1442, %r72;
	add.s32 	%r1444, %r1443, %r28849;
	add.s32 	%r1445, %r1444, %r73;
	add.s32 	%r1446, %r1445, %r74;
	add.s32 	%r1447, %r1446, %r75;
	mul.wide.s32 	%rd103, %r1447, 4;
	add.s64 	%rd24, %rd52, %rd103;
	add.s32 	%r1448, %r1369, 64;
	mul.wide.u32 	%rd104, %r1448, 4;
	add.s64 	%rd25, %rd52, %rd104;
	add.s32 	%r1449, %r1369, 8320;
	mul.wide.u32 	%rd105, %r1449, 4;
	add.s64 	%rd26, %rd52, %rd105;
	add.s32 	%r1450, %r1369, 96;
	mul.wide.u32 	%rd106, %r1450, 4;
	add.s64 	%rd27, %rd52, %rd106;
	add.s32 	%r1451, %r1369, 8352;
	mul.wide.u32 	%rd107, %r1451, 4;
	add.s64 	%rd28, %rd52, %rd107;
	add.s32 	%r1452, %r1369, 128;
	mul.wide.u32 	%rd108, %r1452, 4;
	add.s64 	%rd29, %rd52, %rd108;
	add.s32 	%r1453, %r1369, 8384;
	mul.wide.u32 	%rd109, %r1453, 4;
	add.s64 	%rd30, %rd52, %rd109;
	add.s32 	%r1454, %r1369, 160;
	mul.wide.u32 	%rd110, %r1454, 4;
	add.s64 	%rd31, %rd52, %rd110;
	add.s32 	%r1455, %r1369, 8416;
	mul.wide.u32 	%rd111, %r1455, 4;
	add.s64 	%rd32, %rd52, %rd111;
	add.s32 	%r1456, %r1369, 192;
	mul.wide.u32 	%rd112, %r1456, 4;
	add.s64 	%rd33, %rd52, %rd112;
	add.s32 	%r1457, %r1369, 8448;
	mul.wide.u32 	%rd113, %r1457, 4;
	add.s64 	%rd34, %rd52, %rd113;
	add.s32 	%r1458, %r1369, 224;
	mul.wide.u32 	%rd114, %r1458, 4;
	add.s64 	%rd35, %rd52, %rd114;
	add.s32 	%r1459, %r1369, 8480;
	mul.wide.u32 	%rd115, %r1459, 4;
	add.s64 	%rd36, %rd52, %rd115;
	and.b16  	%rs1, %rs73, 255;
	mov.u16 	%rs3785, 23;
	setp.gt.u32 	%p47, %r3, 7;
	setp.eq.s32 	%p48, %r37, 1;
	setp.lt.u32 	%p50, %r2, 24;
	mov.u32 	%r28854, %r28846;
	mov.u32 	%r28855, %r28846;
	mov.u32 	%r28856, %r28846;
	mov.u32 	%r28857, %r28846;
	mov.u32 	%r28858, %r28846;
	mov.u32 	%r28859, %r28846;
	mov.u32 	%r28860, %r28846;
	bra.uni 	LBB0_16;
LBB0_256:                               // %L176885
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28857, 0;
	mov.u32 	%r28858, %r28857;
	mov.u32 	%r28859, %r28857;
	mov.u32 	%r28860, %r28857;
	mov.u32 	%r28855, %r28857;
LBB0_257:                               // %L176886
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	add.s32 	%r28854, %r28854, 48;
	add.s16 	%rs3785, %rs3785, 48;
	setp.eq.s32 	%p194, %r28854, 2064;
	@%p194 bra 	LBB0_258;
LBB0_16:                                // %L788
                                        // =>This Inner Loop Header: Depth=1
	add.s16 	%rs88, %rs2, %rs3785;
	add.s16 	%rs89, %rs88, -23;
	mul.hi.s16 	%rs90, %rs89, 16257;
	shr.u16 	%rs91, %rs90, 15;
	shr.s16 	%rs92, %rs90, 9;
	add.s16 	%rs93, %rs92, %rs91;
	mul.lo.s16 	%rs94, %rs93, 2064;
	sub.s16 	%rs95, %rs89, %rs94;
	cvt.s32.s16 	%r1653, %rs95;
	mad.lo.s32 	%r1654, %r1653, 21504, %r38;
	or.b32  	%r1655, %r40, %r1654;
	mul.wide.s32 	%rd116, %r1655, 4;
	add.s64 	%rd117, %rd3, %rd116;
	ld.global.v4.u32 	{%r1656, %r1657, %r1658, %r1659}, [%rd117];
	or.b32  	%r1660, %r1654, %r39;
	or.b32  	%r1661, %r1660, 64;
	mul.wide.s32 	%rd118, %r1661, 4;
	add.s64 	%rd119, %rd3, %rd118;
	ld.global.v4.u32 	{%r1662, %r1663, %r1664, %r1665}, [%rd119];
	add.s16 	%rs96, %rs88, 1;
	mul.hi.s16 	%rs97, %rs96, 16257;
	shr.u16 	%rs98, %rs97, 15;
	shr.s16 	%rs99, %rs97, 9;
	add.s16 	%rs100, %rs99, %rs98;
	mul.lo.s16 	%rs101, %rs100, 2064;
	sub.s16 	%rs102, %rs96, %rs101;
	cvt.s32.s16 	%r1666, %rs102;
	mad.lo.s32 	%r1667, %r1666, 21504, %r38;
	or.b32  	%r1668, %r40, %r1667;
	mul.wide.s32 	%rd120, %r1668, 4;
	add.s64 	%rd121, %rd3, %rd120;
	ld.global.v4.u32 	{%r1669, %r1670, %r1671, %r1672}, [%rd121];
	or.b32  	%r1673, %r1667, %r39;
	or.b32  	%r1674, %r1673, 64;
	mul.wide.s32 	%rd122, %r1674, 4;
	add.s64 	%rd123, %rd3, %rd122;
	ld.global.v4.u32 	{%r1675, %r1676, %r1677, %r1678}, [%rd123];
	selp.b32 	%r1679, %r1656, %r1658, %p48;
	shfl.sync.bfly.b32	%r1680, %r1679, 16, 31, -1;
	selp.b32 	%r1462, %r1680, %r1656, %p48;
	selp.b32 	%r1467, %r1658, %r1680, %p48;
	selp.b32 	%r1681, %r1657, %r1659, %p48;
	shfl.sync.bfly.b32	%r1682, %r1681, 16, 31, -1;
	selp.b32 	%r1470, %r1682, %r1657, %p48;
	selp.b32 	%r1475, %r1659, %r1682, %p48;
	selp.b32 	%r1683, %r1662, %r1664, %p48;
	shfl.sync.bfly.b32	%r1684, %r1683, 16, 31, -1;
	selp.b32 	%r1478, %r1684, %r1662, %p48;
	selp.b32 	%r1483, %r1664, %r1684, %p48;
	selp.b32 	%r1685, %r1663, %r1665, %p48;
	shfl.sync.bfly.b32	%r1686, %r1685, 16, 31, -1;
	selp.b32 	%r1486, %r1686, %r1663, %p48;
	selp.b32 	%r1491, %r1665, %r1686, %p48;
	selp.b32 	%r1687, %r1669, %r1671, %p48;
	shfl.sync.bfly.b32	%r1688, %r1687, 16, 31, -1;
	selp.b32 	%r1494, %r1688, %r1669, %p48;
	selp.b32 	%r1499, %r1671, %r1688, %p48;
	selp.b32 	%r1689, %r1670, %r1672, %p48;
	shfl.sync.bfly.b32	%r1690, %r1689, 16, 31, -1;
	selp.b32 	%r1502, %r1690, %r1670, %p48;
	selp.b32 	%r1507, %r1672, %r1690, %p48;
	selp.b32 	%r1691, %r1675, %r1677, %p48;
	shfl.sync.bfly.b32	%r1692, %r1691, 16, 31, -1;
	selp.b32 	%r1510, %r1692, %r1675, %p48;
	selp.b32 	%r1515, %r1677, %r1692, %p48;
	selp.b32 	%r1693, %r1676, %r1678, %p48;
	shfl.sync.bfly.b32	%r1694, %r1693, 16, 31, -1;
	selp.b32 	%r1518, %r1694, %r1676, %p48;
	selp.b32 	%r1523, %r1678, %r1694, %p48;
	shl.b32 	%r1463, %r1467, 4;
	mov.u32 	%r1461, 252645135;
	// begin inline asm
	lop3.b32 %r1525, %r1461, %r1462, %r1463, 202;
	// end inline asm
	shr.u32 	%r1466, %r1462, 4;
	// begin inline asm
	lop3.b32 %r1541, %r1461, %r1466, %r1467, 202;
	// end inline asm
	shl.b32 	%r1471, %r1475, 4;
	// begin inline asm
	lop3.b32 %r1533, %r1461, %r1470, %r1471, 202;
	// end inline asm
	shr.u32 	%r1474, %r1470, 4;
	// begin inline asm
	lop3.b32 %r1549, %r1461, %r1474, %r1475, 202;
	// end inline asm
	shl.b32 	%r1479, %r1483, 4;
	// begin inline asm
	lop3.b32 %r1557, %r1461, %r1478, %r1479, 202;
	// end inline asm
	shr.u32 	%r1482, %r1478, 4;
	// begin inline asm
	lop3.b32 %r1573, %r1461, %r1482, %r1483, 202;
	// end inline asm
	shl.b32 	%r1487, %r1491, 4;
	// begin inline asm
	lop3.b32 %r1565, %r1461, %r1486, %r1487, 202;
	// end inline asm
	shr.u32 	%r1490, %r1486, 4;
	// begin inline asm
	lop3.b32 %r1581, %r1461, %r1490, %r1491, 202;
	// end inline asm
	shl.b32 	%r1495, %r1499, 4;
	// begin inline asm
	lop3.b32 %r1526, %r1461, %r1494, %r1495, 202;
	// end inline asm
	shr.u32 	%r1498, %r1494, 4;
	// begin inline asm
	lop3.b32 %r1542, %r1461, %r1498, %r1499, 202;
	// end inline asm
	shl.b32 	%r1503, %r1507, 4;
	// begin inline asm
	lop3.b32 %r1534, %r1461, %r1502, %r1503, 202;
	// end inline asm
	shr.u32 	%r1506, %r1502, 4;
	// begin inline asm
	lop3.b32 %r1550, %r1461, %r1506, %r1507, 202;
	// end inline asm
	shl.b32 	%r1511, %r1515, 4;
	// begin inline asm
	lop3.b32 %r1558, %r1461, %r1510, %r1511, 202;
	// end inline asm
	shr.u32 	%r1514, %r1510, 4;
	// begin inline asm
	lop3.b32 %r1574, %r1461, %r1514, %r1515, 202;
	// end inline asm
	shl.b32 	%r1519, %r1523, 4;
	// begin inline asm
	lop3.b32 %r1566, %r1461, %r1518, %r1519, 202;
	// end inline asm
	shr.u32 	%r1522, %r1518, 4;
	// begin inline asm
	lop3.b32 %r1582, %r1461, %r1522, %r1523, 202;
	// end inline asm
	mov.u32 	%r1527, 25152;
	// begin inline asm
	prmt.b32 %r1589, %r1525, %r1526, %r1527;
	// end inline asm
	mov.u32 	%r1531, 29521;
	// begin inline asm
	prmt.b32 %r1621, %r1525, %r1526, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1597, %r1533, %r1534, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1629, %r1533, %r1534, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1590, %r1541, %r1542, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1622, %r1541, %r1542, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1598, %r1549, %r1550, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1630, %r1549, %r1550, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1605, %r1557, %r1558, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1637, %r1557, %r1558, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1613, %r1565, %r1566, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1645, %r1565, %r1566, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1606, %r1573, %r1574, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1638, %r1573, %r1574, %r1531;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1614, %r1581, %r1582, %r1527;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1646, %r1581, %r1582, %r1531;
	// end inline asm
	mov.u32 	%r1647, 21520;
	// begin inline asm
	prmt.b32 %r1588, %r1589, %r1590, %r1647;
	// end inline asm
	mov.u32 	%r1651, 30258;
	// begin inline asm
	prmt.b32 %r1592, %r1589, %r1590, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1596, %r1597, %r1598, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1600, %r1597, %r1598, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1604, %r1605, %r1606, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1608, %r1605, %r1606, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1612, %r1613, %r1614, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1616, %r1613, %r1614, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1620, %r1621, %r1622, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1624, %r1621, %r1622, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1628, %r1629, %r1630, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1632, %r1629, %r1630, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1636, %r1637, %r1638, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1640, %r1637, %r1638, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1644, %r1645, %r1646, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1648, %r1645, %r1646, %r1651;
	// end inline asm
	mul.hi.s16 	%rs103, %rs89, 10923;
	shr.u16 	%rs104, %rs103, 15;
	shr.s16 	%rs105, %rs103, 2;
	add.s16 	%rs106, %rs105, %rs104;
	mul.lo.s16 	%rs107, %rs106, 24;
	sub.s16 	%rs108, %rs89, %rs107;
	cvt.s32.s16 	%r1695, %rs108;
	add.s32 	%r1696, %r41, %r1695;
	mul.wide.s32 	%rd124, %r1696, 4;
	add.s64 	%rd126, %rd52, %rd124;
	st.shared.u32 	[%rd126], %r1588;
	st.shared.u32 	[%rd126+512], %r1596;
	st.shared.u32 	[%rd126+256], %r1592;
	st.shared.u32 	[%rd126+768], %r1600;
	add.s32 	%r1697, %r42, %r1695;
	mul.wide.s32 	%rd127, %r1697, 4;
	add.s64 	%rd128, %rd52, %rd127;
	st.shared.u32 	[%rd128], %r1604;
	st.shared.u32 	[%rd128+512], %r1612;
	st.shared.u32 	[%rd128+256], %r1608;
	st.shared.u32 	[%rd128+768], %r1616;
	st.shared.u32 	[%rd126+128], %r1620;
	st.shared.u32 	[%rd126+640], %r1628;
	st.shared.u32 	[%rd126+384], %r1624;
	st.shared.u32 	[%rd126+896], %r1632;
	st.shared.u32 	[%rd128+128], %r1636;
	st.shared.u32 	[%rd128+640], %r1644;
	st.shared.u32 	[%rd128+384], %r1640;
	st.shared.u32 	[%rd128+896], %r1648;
	bar.sync 	0;
	add.s16 	%rs109, %rs1, %rs3785;
	add.s16 	%rs110, %rs109, -23;
	mul.hi.s16 	%rs111, %rs110, 10923;
	shr.u16 	%rs112, %rs111, 15;
	shr.s16 	%rs113, %rs111, 2;
	add.s16 	%rs114, %rs113, %rs112;
	mul.lo.s16 	%rs115, %rs114, 24;
	sub.s16 	%rs116, %rs110, %rs115;
	cvt.s32.s16 	%r1698, %rs116;
	add.s32 	%r1699, %r44, %r1698;
	add.s32 	%r1700, %r1699, %r45;
	mul.wide.s32 	%rd129, %r1700, 4;
	add.s64 	%rd130, %rd52, %rd129;
	ld.shared.u32 	%r1701, [%rd130];
	add.s32 	%r1702, %r1699, %r46;
	mul.wide.u32 	%rd131, %r1702, 4;
	add.s64 	%rd132, %rd52, %rd131;
	ld.shared.u32 	%r1703, [%rd132];
	add.s32 	%r1704, %r1699, %r47;
	mul.wide.u32 	%rd133, %r1704, 4;
	add.s64 	%rd134, %rd52, %rd133;
	ld.shared.u32 	%r1705, [%rd134];
	add.s32 	%r1706, %r1699, %r48;
	mul.wide.u32 	%rd135, %r1706, 4;
	add.s64 	%rd136, %rd52, %rd135;
	ld.shared.u32 	%r1707, [%rd136];
	add.s32 	%r1708, %r1699, %r49;
	mul.wide.s32 	%rd137, %r1708, 4;
	add.s64 	%rd138, %rd52, %rd137;
	ld.shared.u32 	%r1709, [%rd138];
	add.s32 	%r1710, %r1699, %r50;
	mul.wide.u32 	%rd139, %r1710, 4;
	add.s64 	%rd140, %rd52, %rd139;
	ld.shared.u32 	%r1711, [%rd140];
	add.s32 	%r1712, %r1699, %r51;
	mul.wide.u32 	%rd141, %r1712, 4;
	add.s64 	%rd142, %rd52, %rd141;
	ld.shared.u32 	%r1713, [%rd142];
	add.s32 	%r1714, %r1699, %r52;
	mul.wide.u32 	%rd143, %r1714, 4;
	add.s64 	%rd144, %rd52, %rd143;
	ld.shared.u32 	%r1715, [%rd144];
	add.s32 	%r1716, %r1699, %r53;
	mul.wide.s32 	%rd145, %r1716, 4;
	add.s64 	%rd146, %rd52, %rd145;
	ld.shared.u32 	%r1717, [%rd146];
	add.s32 	%r1718, %r1699, %r54;
	mul.wide.u32 	%rd147, %r1718, 4;
	add.s64 	%rd148, %rd52, %rd147;
	ld.shared.u32 	%r1719, [%rd148];
	add.s32 	%r1720, %r1699, %r55;
	mul.wide.u32 	%rd149, %r1720, 4;
	add.s64 	%rd150, %rd52, %rd149;
	ld.shared.u32 	%r1721, [%rd150];
	add.s32 	%r1722, %r1699, %r56;
	mul.wide.u32 	%rd151, %r1722, 4;
	add.s64 	%rd152, %rd52, %rd151;
	ld.shared.u32 	%r1723, [%rd152];
	add.s32 	%r1724, %r1699, %r57;
	mul.wide.s32 	%rd153, %r1724, 4;
	add.s64 	%rd154, %rd52, %rd153;
	ld.shared.u32 	%r1725, [%rd154];
	add.s32 	%r1726, %r1699, %r58;
	mul.wide.u32 	%rd155, %r1726, 4;
	add.s64 	%rd156, %rd52, %rd155;
	ld.shared.u32 	%r1727, [%rd156];
	add.s32 	%r1728, %r1699, %r59;
	mul.wide.u32 	%rd157, %r1728, 4;
	add.s64 	%rd158, %rd52, %rd157;
	ld.shared.u32 	%r1729, [%rd158];
	add.s32 	%r1730, %r1699, %r60;
	mul.wide.u32 	%rd159, %r1730, 4;
	add.s64 	%rd160, %rd52, %rd159;
	ld.shared.u32 	%r1731, [%rd160];
	add.s32 	%r1732, %r1699, %r61;
	mul.wide.s32 	%rd161, %r1732, 4;
	add.s64 	%rd162, %rd52, %rd161;
	ld.shared.u32 	%r1733, [%rd162];
	add.s32 	%r1734, %r1699, %r62;
	mul.wide.u32 	%rd163, %r1734, 4;
	add.s64 	%rd164, %rd52, %rd163;
	ld.shared.u32 	%r1735, [%rd164];
	add.s32 	%r1736, %r1699, %r63;
	mul.wide.u32 	%rd165, %r1736, 4;
	add.s64 	%rd166, %rd52, %rd165;
	ld.shared.u32 	%r1737, [%rd166];
	add.s32 	%r1738, %r1699, %r64;
	mul.wide.u32 	%rd167, %r1738, 4;
	add.s64 	%rd168, %rd52, %rd167;
	ld.shared.u32 	%r1739, [%rd168];
	add.s32 	%r1740, %r1699, %r65;
	mul.wide.s32 	%rd169, %r1740, 4;
	add.s64 	%rd170, %rd52, %rd169;
	ld.shared.u32 	%r1741, [%rd170];
	add.s32 	%r1742, %r1699, %r66;
	mul.wide.s32 	%rd171, %r1742, 4;
	add.s64 	%rd172, %rd52, %rd171;
	ld.shared.u32 	%r1743, [%rd172];
	bar.sync 	0;
	shfl.sync.idx.b32	%r1744, %r28847, 0, 31, -1;
	shfl.sync.idx.b32	%r1745, %r28847, 1, 31, -1;
	shfl.sync.idx.b32	%r1746, %r28847, 2, 31, -1;
	shfl.sync.idx.b32	%r1747, %r28847, 3, 31, -1;
	shfl.sync.idx.b32	%r1748, %r28847, 4, 31, -1;
	shfl.sync.idx.b32	%r1749, %r28847, 5, 31, -1;
	shfl.sync.idx.b32	%r1750, %r28847, 6, 31, -1;
	shfl.sync.idx.b32	%r1751, %r28847, 7, 31, -1;
	shfl.sync.idx.b32	%r1752, %r28847, 8, 31, -1;
	shfl.sync.idx.b32	%r1753, %r28847, 9, 31, -1;
	shfl.sync.idx.b32	%r1754, %r28847, 10, 31, -1;
	shfl.sync.idx.b32	%r1755, %r28847, 11, 31, -1;
	shfl.sync.idx.b32	%r1756, %r28847, 12, 31, -1;
	shfl.sync.idx.b32	%r1757, %r28847, 13, 31, -1;
	shfl.sync.idx.b32	%r1758, %r28847, 14, 31, -1;
	shfl.sync.idx.b32	%r1759, %r28847, 15, 31, -1;
	shfl.sync.idx.b32	%r1760, %r28847, 16, 31, -1;
	shfl.sync.idx.b32	%r1761, %r28847, 17, 31, -1;
	shfl.sync.idx.b32	%r1762, %r28847, 18, 31, -1;
	shfl.sync.idx.b32	%r1763, %r28847, 19, 31, -1;
	shfl.sync.idx.b32	%r1764, %r28847, 20, 31, -1;
	shfl.sync.idx.b32	%r1765, %r28847, 21, 31, -1;
	shfl.sync.idx.b32	%r1766, %r28847, 22, 31, -1;
	shfl.sync.idx.b32	%r1767, %r28847, 23, 31, -1;
	add.s32 	%r1768, %r1744, %r1698;
	mul.wide.s32 	%rd173, %r1768, 4;
	add.s64 	%rd174, %rd52, %rd173;
	st.shared.u32 	[%rd174], %r1701;
	add.s32 	%r1769, %r1745, %r1698;
	mul.wide.s32 	%rd175, %r1769, 4;
	add.s64 	%rd176, %rd52, %rd175;
	st.shared.u32 	[%rd176], %r1703;
	add.s32 	%r1770, %r1746, %r1698;
	mul.wide.s32 	%rd177, %r1770, 4;
	add.s64 	%rd178, %rd52, %rd177;
	st.shared.u32 	[%rd178], %r1705;
	add.s32 	%r1771, %r1747, %r1698;
	mul.wide.s32 	%rd179, %r1771, 4;
	add.s64 	%rd180, %rd52, %rd179;
	st.shared.u32 	[%rd180], %r1707;
	add.s32 	%r1772, %r1748, %r1698;
	mul.wide.s32 	%rd181, %r1772, 4;
	add.s64 	%rd182, %rd52, %rd181;
	st.shared.u32 	[%rd182], %r1709;
	add.s32 	%r1773, %r1749, %r1698;
	mul.wide.s32 	%rd183, %r1773, 4;
	add.s64 	%rd184, %rd52, %rd183;
	st.shared.u32 	[%rd184], %r1711;
	add.s32 	%r1774, %r1750, %r1698;
	mul.wide.s32 	%rd185, %r1774, 4;
	add.s64 	%rd186, %rd52, %rd185;
	st.shared.u32 	[%rd186], %r1713;
	add.s32 	%r1775, %r1751, %r1698;
	mul.wide.s32 	%rd187, %r1775, 4;
	add.s64 	%rd188, %rd52, %rd187;
	st.shared.u32 	[%rd188], %r1715;
	add.s32 	%r1776, %r1752, %r1698;
	mul.wide.s32 	%rd189, %r1776, 4;
	add.s64 	%rd190, %rd52, %rd189;
	st.shared.u32 	[%rd190], %r1717;
	add.s32 	%r1777, %r1753, %r1698;
	mul.wide.s32 	%rd191, %r1777, 4;
	add.s64 	%rd192, %rd52, %rd191;
	st.shared.u32 	[%rd192], %r1719;
	add.s32 	%r1778, %r1754, %r1698;
	mul.wide.s32 	%rd193, %r1778, 4;
	add.s64 	%rd194, %rd52, %rd193;
	st.shared.u32 	[%rd194], %r1721;
	add.s32 	%r1779, %r1755, %r1698;
	mul.wide.s32 	%rd195, %r1779, 4;
	add.s64 	%rd196, %rd52, %rd195;
	st.shared.u32 	[%rd196], %r1723;
	add.s32 	%r1780, %r1756, %r1698;
	mul.wide.s32 	%rd197, %r1780, 4;
	add.s64 	%rd198, %rd52, %rd197;
	st.shared.u32 	[%rd198], %r1725;
	add.s32 	%r1781, %r1757, %r1698;
	mul.wide.s32 	%rd199, %r1781, 4;
	add.s64 	%rd200, %rd52, %rd199;
	st.shared.u32 	[%rd200], %r1727;
	add.s32 	%r1782, %r1758, %r1698;
	mul.wide.s32 	%rd201, %r1782, 4;
	add.s64 	%rd202, %rd52, %rd201;
	st.shared.u32 	[%rd202], %r1729;
	add.s32 	%r1783, %r1759, %r1698;
	mul.wide.s32 	%rd203, %r1783, 4;
	add.s64 	%rd204, %rd52, %rd203;
	st.shared.u32 	[%rd204], %r1731;
	add.s32 	%r1784, %r1760, %r1698;
	mul.wide.s32 	%rd205, %r1784, 4;
	add.s64 	%rd206, %rd52, %rd205;
	st.shared.u32 	[%rd206], %r1733;
	add.s32 	%r1785, %r1761, %r1698;
	mul.wide.s32 	%rd207, %r1785, 4;
	add.s64 	%rd208, %rd52, %rd207;
	st.shared.u32 	[%rd208], %r1735;
	add.s32 	%r1786, %r1762, %r1698;
	mul.wide.s32 	%rd209, %r1786, 4;
	add.s64 	%rd210, %rd52, %rd209;
	st.shared.u32 	[%rd210], %r1737;
	add.s32 	%r1787, %r1763, %r1698;
	mul.wide.s32 	%rd211, %r1787, 4;
	add.s64 	%rd212, %rd52, %rd211;
	st.shared.u32 	[%rd212], %r1739;
	add.s32 	%r1788, %r1764, %r1698;
	mul.wide.s32 	%rd213, %r1788, 4;
	add.s64 	%rd214, %rd52, %rd213;
	st.shared.u32 	[%rd214], %r1741;
	selp.b32 	%r1789, 0, %r1743, %p47;
	add.s32 	%r1790, %r1765, %r1698;
	mul.wide.s32 	%rd215, %r1790, 4;
	add.s64 	%rd216, %rd52, %rd215;
	st.shared.u32 	[%rd216], %r1789;
	add.s32 	%r1791, %r1766, %r1698;
	mul.wide.s32 	%rd217, %r1791, 4;
	add.s64 	%rd218, %rd52, %rd217;
	st.shared.u32 	[%rd218], %r28846;
	add.s32 	%r1792, %r1767, %r1698;
	mul.wide.s32 	%rd219, %r1792, 4;
	add.s64 	%rd220, %rd52, %rd219;
	st.shared.u32 	[%rd220], %r28846;
	bar.sync 	0;
	add.s16 	%rs3779, %rs3785, -6;
	add.s16 	%rs3780, %rs3785, -5;
	add.s16 	%rs3781, %rs3785, -4;
	add.s16 	%rs3782, %rs3785, -2;
	add.s16 	%rs3783, %rs3785, -1;
	mov.u32 	%r28861, %r28846;
	mov.u32 	%r28862, %r28846;
	mov.u32 	%r28863, %r28846;
	mov.u32 	%r28864, %r28846;
	mov.u32 	%r28865, %r28846;
	mov.u32 	%r28866, %r28846;
	mov.u32 	%r28867, %r28846;
	mov.u32 	%r28868, %r28846;
	mov.u32 	%r28869, %r28846;
	mov.u32 	%r28870, %r28846;
	mov.u32 	%r28871, %r28846;
	mov.u32 	%r28872, %r28846;
	mov.u32 	%r28873, %r28846;
	mov.u32 	%r28874, %r28846;
	mov.u32 	%r28875, %r28846;
	mov.u32 	%r28876, %r28846;
	mov.u32 	%r28877, %r28846;
	mov.u32 	%r28878, %r28846;
	mov.u32 	%r28879, %r28846;
	mov.u32 	%r28880, %r28846;
	mov.u32 	%r28881, %r28846;
	mov.u32 	%r28882, %r28846;
	mov.u32 	%r28883, %r28846;
	mov.u32 	%r28884, %r28846;
	@%p46 bra 	LBB0_261;
	bra.uni 	LBB0_17;
LBB0_261:                               // %oksrem3271
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s16 	%rs117, %rs3785, -23;
	mul.hi.s16 	%rs118, %rs117, 10923;
	shr.u16 	%rs119, %rs118, 15;
	shr.s16 	%rs120, %rs118, 2;
	add.s16 	%rs121, %rs120, %rs119;
	mul.lo.s16 	%rs122, %rs121, 24;
	sub.s16 	%rs123, %rs117, %rs122;
	cvt.s32.s16 	%r1793, %rs123;
	add.s32 	%r1794, %r67, %r1793;
	add.s32 	%r1795, %r1794, %r68;
	add.s32 	%r1796, %r1795, %r69;
	add.s32 	%r1797, %r1796, %r70;
	mul.wide.s32 	%rd221, %r1797, 4;
	add.s64 	%rd223, %rd52, %rd221;
	ld.shared.u32 	%r28861, [%rd223];
	ld.shared.u32 	%r28862, [%rd12+4];
	ld.shared.u32 	%r28863, [%rd12+8];
	ld.shared.u32 	%r28864, [%rd12+12];
	ld.shared.u32 	%r28865, [%rd12+16];
	ld.shared.u32 	%r28866, [%rd12+20];
	ld.shared.u32 	%r28867, [%rd12+24];
	ld.shared.u32 	%r28868, [%rd12+28];
	ld.shared.u32 	%r28869, [%rd12+32];
	ld.shared.u32 	%r28870, [%rd12+36];
	ld.shared.u32 	%r28871, [%rd12+40];
	ld.shared.u32 	%r28872, [%rd12+44];
	ld.shared.u32 	%r28873, [%rd12+48];
	ld.shared.u32 	%r28874, [%rd12+52];
	ld.shared.u32 	%r28875, [%rd12+56];
	ld.shared.u32 	%r28876, [%rd12+60];
	add.s16 	%rs124, %rs3785, -7;
	mul.hi.s16 	%rs125, %rs124, 10923;
	shr.u16 	%rs126, %rs125, 15;
	shr.s16 	%rs127, %rs125, 2;
	add.s16 	%rs128, %rs127, %rs126;
	mul.lo.s16 	%rs129, %rs128, 24;
	sub.s16 	%rs130, %rs124, %rs129;
	cvt.s32.s16 	%r1798, %rs130;
	add.s32 	%r1799, %r67, %r1798;
	add.s32 	%r1800, %r1799, %r68;
	add.s32 	%r1801, %r1800, %r69;
	add.s32 	%r1802, %r1801, %r70;
	mul.wide.s32 	%rd224, %r1802, 4;
	add.s64 	%rd225, %rd52, %rd224;
	ld.shared.u32 	%r28877, [%rd225];
	mul.hi.s16 	%rs132, %rs3779, 10923;
	shr.u16 	%rs133, %rs132, 15;
	shr.s16 	%rs134, %rs132, 2;
	add.s16 	%rs135, %rs134, %rs133;
	mul.lo.s16 	%rs136, %rs135, 24;
	sub.s16 	%rs137, %rs3779, %rs136;
	cvt.s32.s16 	%r1803, %rs137;
	add.s32 	%r1804, %r67, %r1803;
	add.s32 	%r1805, %r1804, %r68;
	add.s32 	%r1806, %r1805, %r69;
	add.s32 	%r1807, %r1806, %r70;
	mul.wide.s32 	%rd226, %r1807, 4;
	add.s64 	%rd227, %rd52, %rd226;
	ld.shared.u32 	%r28878, [%rd227];
	mul.hi.s16 	%rs139, %rs3780, 10923;
	shr.u16 	%rs140, %rs139, 15;
	shr.s16 	%rs141, %rs139, 2;
	add.s16 	%rs142, %rs141, %rs140;
	mul.lo.s16 	%rs143, %rs142, 24;
	sub.s16 	%rs144, %rs3780, %rs143;
	cvt.s32.s16 	%r1808, %rs144;
	add.s32 	%r1809, %r67, %r1808;
	add.s32 	%r1810, %r1809, %r68;
	add.s32 	%r1811, %r1810, %r69;
	add.s32 	%r1812, %r1811, %r70;
	mul.wide.s32 	%rd228, %r1812, 4;
	add.s64 	%rd229, %rd52, %rd228;
	ld.shared.u32 	%r28879, [%rd229];
	mul.hi.s16 	%rs146, %rs3781, 10923;
	shr.u16 	%rs147, %rs146, 15;
	shr.s16 	%rs148, %rs146, 2;
	add.s16 	%rs149, %rs148, %rs147;
	mul.lo.s16 	%rs150, %rs149, 24;
	sub.s16 	%rs151, %rs3781, %rs150;
	cvt.s32.s16 	%r1813, %rs151;
	add.s32 	%r1814, %r67, %r1813;
	add.s32 	%r1815, %r1814, %r68;
	add.s32 	%r1816, %r1815, %r69;
	add.s32 	%r1817, %r1816, %r70;
	mul.wide.s32 	%rd230, %r1817, 4;
	add.s64 	%rd231, %rd52, %rd230;
	ld.shared.u32 	%r28880, [%rd231];
	add.s16 	%rs152, %rs3785, -3;
	mul.hi.s16 	%rs153, %rs152, 10923;
	shr.u16 	%rs154, %rs153, 15;
	shr.s16 	%rs155, %rs153, 2;
	add.s16 	%rs156, %rs155, %rs154;
	mul.lo.s16 	%rs157, %rs156, 24;
	sub.s16 	%rs158, %rs152, %rs157;
	cvt.s32.s16 	%r1818, %rs158;
	add.s32 	%r1819, %r67, %r1818;
	add.s32 	%r1820, %r1819, %r68;
	add.s32 	%r1821, %r1820, %r69;
	add.s32 	%r1822, %r1821, %r70;
	mul.wide.s32 	%rd232, %r1822, 4;
	add.s64 	%rd233, %rd52, %rd232;
	ld.shared.u32 	%r28881, [%rd233];
	mul.hi.s16 	%rs160, %rs3782, 10923;
	shr.u16 	%rs161, %rs160, 15;
	shr.s16 	%rs162, %rs160, 2;
	add.s16 	%rs163, %rs162, %rs161;
	mul.lo.s16 	%rs164, %rs163, 24;
	sub.s16 	%rs165, %rs3782, %rs164;
	cvt.s32.s16 	%r1823, %rs165;
	add.s32 	%r1824, %r67, %r1823;
	add.s32 	%r1825, %r1824, %r68;
	add.s32 	%r1826, %r1825, %r69;
	add.s32 	%r1827, %r1826, %r70;
	mul.wide.s32 	%rd234, %r1827, 4;
	add.s64 	%rd235, %rd52, %rd234;
	ld.shared.u32 	%r28882, [%rd235];
	mul.hi.s16 	%rs167, %rs3783, 10923;
	shr.u16 	%rs168, %rs167, 15;
	shr.s16 	%rs169, %rs167, 2;
	add.s16 	%rs170, %rs169, %rs168;
	mul.lo.s16 	%rs171, %rs170, 24;
	sub.s16 	%rs172, %rs3783, %rs171;
	cvt.s32.s16 	%r1828, %rs172;
	add.s32 	%r1829, %r67, %r1828;
	add.s32 	%r1830, %r1829, %r68;
	add.s32 	%r1831, %r1830, %r69;
	add.s32 	%r1832, %r1831, %r70;
	mul.wide.s32 	%rd236, %r1832, 4;
	add.s64 	%rd237, %rd52, %rd236;
	ld.shared.u32 	%r28883, [%rd237];
	mul.hi.s16 	%rs173, %rs3785, 10923;
	shr.u16 	%rs174, %rs173, 15;
	shr.s16 	%rs175, %rs173, 2;
	add.s16 	%rs176, %rs175, %rs174;
	mul.lo.s16 	%rs177, %rs176, 24;
	sub.s16 	%rs178, %rs3785, %rs177;
	cvt.s32.s16 	%r1833, %rs178;
	add.s32 	%r1834, %r67, %r1833;
	add.s32 	%r1835, %r1834, %r68;
	add.s32 	%r1836, %r1835, %r69;
	add.s32 	%r1837, %r1836, %r70;
	mul.wide.s32 	%rd238, %r1837, 4;
	add.s64 	%rd239, %rd52, %rd238;
	ld.shared.u32 	%r28884, [%rd239];
LBB0_17:                                // %L12780
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	mov.u16 	%rs216, 25600;
	// begin inline asm
	mov.b32 %r1843, {%rs216, %rs216};
	// end inline asm
	mov.u16 	%rs218, 21504;
	// begin inline asm
	mov.b32 %r1854, {%rs218, %rs218};
	// end inline asm
	xor.b32  	%r1853, %r28861, -2004318072;
	mov.u32 	%r1979, 983055;
	// begin inline asm
	lop3.b32 %r1840, %r1979, %r1853, %r1843, 202;
	// end inline asm
	mov.u16 	%rs222, 18432;
	// begin inline asm
	mov.b32 %r1844, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1845, %r1843, %r1844;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1848, %r1840, %r1845;
	// end inline asm
	mov.u32 	%r1990, 15728880;
	// begin inline asm
	lop3.b32 %r1851, %r1990, %r1853, %r1854, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1855, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1856, %r1854, %r1855;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1859, %r1851, %r1856;
	// end inline asm
	// begin inline asm
	mov.b32 %r1889, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r1900, {%rs218, %rs218};
	// end inline asm
	xor.b32  	%r1899, %r28862, -2004318072;
	// begin inline asm
	lop3.b32 %r1886, %r1979, %r1899, %r1889, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1890, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1891, %r1889, %r1890;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1894, %r1886, %r1891;
	// end inline asm
	// begin inline asm
	lop3.b32 %r1897, %r1990, %r1899, %r1900, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1901, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1902, %r1900, %r1901;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1905, %r1897, %r1902;
	// end inline asm
	// begin inline asm
	mov.b32 %r1935, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r1946, {%rs218, %rs218};
	// end inline asm
	xor.b32  	%r1945, %r28863, -2004318072;
	// begin inline asm
	lop3.b32 %r1932, %r1979, %r1945, %r1935, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1936, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1937, %r1935, %r1936;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1940, %r1932, %r1937;
	// end inline asm
	// begin inline asm
	lop3.b32 %r1943, %r1990, %r1945, %r1946, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1947, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1948, %r1946, %r1947;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1951, %r1943, %r1948;
	// end inline asm
	// begin inline asm
	mov.b32 %r1981, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r1992, {%rs218, %rs218};
	// end inline asm
	xor.b32  	%r1991, %r28864, -2004318072;
	// begin inline asm
	lop3.b32 %r1978, %r1979, %r1991, %r1981, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1982, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1983, %r1981, %r1982;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1986, %r1978, %r1983;
	// end inline asm
	// begin inline asm
	lop3.b32 %r1989, %r1990, %r1991, %r1992, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1993, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1994, %r1992, %r1993;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1997, %r1989, %r1994;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r1848;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2942, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r1859;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2945, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r1894;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2948, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r1905;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2951, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r1940;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2954, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r1951;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2957, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r1986;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2960, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r1997;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2963, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3026, %r3029}, {%r1202, %r1199}, {%r2942}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3035, %r3038}, {%r1202, %r1199}, {%r2945}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3044, %r3047}, {%r1202, %r1199}, {%r2948}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3053, %r3056}, {%r1202, %r1199}, {%r2951}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3062, %r3065}, {%r1202, %r1199}, {%r2954}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3071, %r3074}, {%r1202, %r1199}, {%r2957}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3080, %r3083}, {%r1202, %r1199}, {%r2960}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3089, %r3092}, {%r1202, %r1199}, {%r2963}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3022, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3024, %r3022, %r3026;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3027, %r1237, %r3029, %r3024;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3031, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3033, %r3031, %r3035;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3036, %r1237, %r3038, %r3033;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3040, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3042, %r3040, %r3044;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3045, %r1237, %r3047, %r3042;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3049, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3051, %r3049, %r3053;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3054, %r1237, %r3056, %r3051;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3058, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3060, %r3058, %r3062;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3063, %r1237, %r3065, %r3060;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3067, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3069, %r3067, %r3071;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3072, %r1237, %r3074, %r3069;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3076, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3078, %r3076, %r3080;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3081, %r1237, %r3083, %r3078;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3085, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3087, %r3085, %r3089;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3090, %r1237, %r3092, %r3087;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3094, %r1240, %r3029;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3097, %r1237, %r3026, %r3094;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3101, %r1240, %r3038;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3104, %r1237, %r3035, %r3101;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3108, %r1240, %r3047;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3111, %r1237, %r3044, %r3108;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3115, %r1240, %r3056;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3118, %r1237, %r3053, %r3115;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3122, %r1240, %r3065;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3125, %r1237, %r3062, %r3122;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3129, %r1240, %r3074;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3132, %r1237, %r3071, %r3129;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3136, %r1240, %r3083;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3139, %r1237, %r3080, %r3136;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3143, %r1240, %r3092;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3146, %r1237, %r3089, %r3143;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3231, %r3232}, {%r1270, %r1264, %r1267, %r1261}, {%r3097, %r3027}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3239, %r3240}, {%r1270, %r1264, %r1267, %r1261}, {%r3104, %r3036}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3247, %r3248}, {%r1270, %r1264, %r1267, %r1261}, {%r3111, %r3045}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3255, %r3256}, {%r1270, %r1264, %r1267, %r1261}, {%r3118, %r3054}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3263, %r3264}, {%r1270, %r1264, %r1267, %r1261}, {%r3125, %r3063}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3271, %r3272}, {%r1270, %r1264, %r1267, %r1261}, {%r3132, %r3072}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3279, %r3280}, {%r1270, %r1264, %r1267, %r1261}, {%r3139, %r3081}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3287, %r3288}, {%r1270, %r1264, %r1267, %r1261}, {%r3146, %r3090}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	prmt.b32 %r3230, %r3231, %r3232, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3234, %r3231, %r3232, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3238, %r3239, %r3240, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3242, %r3239, %r3240, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3246, %r3247, %r3248, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3250, %r3247, %r3248, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3254, %r3255, %r3256, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3258, %r3255, %r3256, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3262, %r3263, %r3264, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3266, %r3263, %r3264, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3270, %r3271, %r3272, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3274, %r3271, %r3272, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3278, %r3279, %r3280, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3282, %r3279, %r3280, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3286, %r3287, %r3288, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r3290, %r3287, %r3288, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r3230;
	st.shared.u32 	[%rd7+33024], %r3234;
	st.shared.u32 	[%rd8], %r3238;
	st.shared.u32 	[%rd9], %r3242;
	st.shared.u32 	[%rd13], %r3246;
	st.shared.u32 	[%rd14], %r3250;
	st.shared.u32 	[%rd15], %r3254;
	st.shared.u32 	[%rd16], %r3258;
	st.shared.u32 	[%rd17], %r3262;
	st.shared.u32 	[%rd18], %r3266;
	st.shared.u32 	[%rd19], %r3270;
	st.shared.u32 	[%rd20], %r3274;
	st.shared.u32 	[%rd21], %r3278;
	st.shared.u32 	[%rd22], %r3282;
	st.shared.u32 	[%rd23], %r3286;
	st.shared.u32 	[%rd24], %r3290;
	bar.sync 	0;
	mov.u32 	%r28885, %r28846;
	mov.u32 	%r28886, %r28846;
	mov.u32 	%r28887, %r28846;
	mov.u32 	%r28888, %r28846;
	@%p50 bra 	LBB0_262;
	bra.uni 	LBB0_18;
LBB0_262:                               // %oksrem6886
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28885, [%rd10];
	ld.shared.u32 	%r28886, [%rd11+33024];
	ld.shared.u32 	%r28887, [%rd11+128];
	ld.shared.u32 	%r28888, [%rd11+33152];
LBB0_18:                                // %L20067
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3327, %r3330}, {%r1202, %r1199}, {%r28885}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3336, %r3339}, {%r1202, %r1199}, {%r28886}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3345, %r3348}, {%r1202, %r1199}, {%r28887}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3354, %r3357}, {%r1202, %r1199}, {%r28888}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3323, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3325, %r3323, %r3327;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3328, %r1237, %r3330, %r3325;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3332, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3334, %r3332, %r3336;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3337, %r1237, %r3339, %r3334;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3341, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3343, %r3341, %r3345;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3346, %r1237, %r3348, %r3343;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3350, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3352, %r3350, %r3354;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3355, %r1237, %r3357, %r3352;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3359, %r1240, %r3330;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3362, %r1237, %r3327, %r3359;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3366, %r1240, %r3339;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3369, %r1237, %r3336, %r3366;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3373, %r1240, %r3348;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3376, %r1237, %r3345, %r3373;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3380, %r1240, %r3357;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3383, %r1237, %r3354, %r3380;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3428, %r3432}, {%r1270, %r1264, %r1267, %r1261}, {%r3362, %r3328}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3436, %r3440}, {%r1270, %r1264, %r1267, %r1261}, {%r3369, %r3337}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3444, %r3448}, {%r1270, %r1264, %r1267, %r1261}, {%r3376, %r3346}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3452, %r3456}, {%r1270, %r1264, %r1267, %r1261}, {%r3383, %r3355}, {%r28846, %r28846};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3427, %r3428, %r3428, %r28860;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28894, %r3432, %r3432, %r3427;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3435, %r3436, %r3436, %r28859;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28893, %r3440, %r3440, %r3435;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3443, %r3444, %r3444, %r28858;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28892, %r3448, %r3448, %r3443;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3451, %r3452, %r3452, %r28857;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28891, %r3456, %r3456, %r3451;
	// end inline asm
	add.s32 	%r28889, %r28855, 1;
	setp.ne.s32 	%p51, %r28889, 40;
	@%p51 bra 	LBB0_22;
// %bb.19:                              // %L20250
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_21;
// %bb.20:                              // %L20315
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r3459, %r28856, 193536;
	add.s32 	%r3460, %r83, %r3459;
	or.b32  	%r3461, %r3460, %r43;
	add.s32 	%r3462, %r3461, %r84;
	mul.wide.u32 	%rd240, %r3462, 4;
	add.s64 	%rd241, %rd4, %rd240;
	st.global.u32 	[%rd241], %r28894;
	add.s32 	%r3463, %r3461, %r85;
	mul.wide.u32 	%rd242, %r3463, 4;
	add.s64 	%rd243, %rd4, %rd242;
	st.global.u32 	[%rd243], %r28893;
	cvt.u64.u32 	%rd244, %r84;
	cvt.u64.u32 	%rd245, %r43;
	cvt.u64.u32 	%rd246, %r83;
	cvt.u64.u32 	%rd247, %r3459;
	add.s64 	%rd248, %rd247, %rd246;
	add.s64 	%rd249, %rd248, %rd245;
	add.s64 	%rd250, %rd249, %rd244;
	shl.b64 	%rd251, %rd250, 2;
	add.s64 	%rd252, %rd4, %rd251;
	st.global.u32 	[%rd252+387072], %r28892;
	cvt.u64.u32 	%rd253, %r85;
	add.s64 	%rd254, %rd249, %rd253;
	shl.b64 	%rd255, %rd254, 2;
	add.s64 	%rd256, %rd4, %rd255;
	st.global.u32 	[%rd256+387072], %r28891;
LBB0_21:                                // %L20828
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28889, 0;
	mov.u32 	%r28891, %r28889;
	mov.u32 	%r28892, %r28889;
	mov.u32 	%r28893, %r28889;
	mov.u32 	%r28894, %r28889;
LBB0_22:                                // %L20829
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r3465, 0;
	mov.u32 	%r28895, %r3465;
	mov.u32 	%r28896, %r3465;
	mov.u32 	%r28897, %r3465;
	mov.u32 	%r28898, %r3465;
	@%p50 bra 	LBB0_263;
	bra.uni 	LBB0_23;
LBB0_263:                               // %oksrem7584
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28895, [%rd25];
	ld.shared.u32 	%r28896, [%rd26];
	ld.shared.u32 	%r28897, [%rd27];
	ld.shared.u32 	%r28898, [%rd28];
LBB0_23:                                // %L21948
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3498, %r3501}, {%r1202, %r1199}, {%r28895}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3507, %r3510}, {%r1202, %r1199}, {%r28896}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3516, %r3519}, {%r1202, %r1199}, {%r28897}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3525, %r3528}, {%r1202, %r1199}, {%r28898}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3494, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3496, %r3494, %r3498;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3499, %r1237, %r3501, %r3496;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3503, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3505, %r3503, %r3507;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3508, %r1237, %r3510, %r3505;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3512, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3514, %r3512, %r3516;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3517, %r1237, %r3519, %r3514;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3521, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3523, %r3521, %r3525;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3526, %r1237, %r3528, %r3523;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3530, %r1240, %r3501;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3533, %r1237, %r3498, %r3530;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3537, %r1240, %r3510;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3540, %r1237, %r3507, %r3537;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3544, %r1240, %r3519;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3547, %r1237, %r3516, %r3544;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3551, %r1240, %r3528;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3554, %r1237, %r3525, %r3551;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3599, %r3603}, {%r1270, %r1264, %r1267, %r1261}, {%r3533, %r3499}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3607, %r3611}, {%r1270, %r1264, %r1267, %r1261}, {%r3540, %r3508}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3615, %r3619}, {%r1270, %r1264, %r1267, %r1261}, {%r3547, %r3517}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3623, %r3627}, {%r1270, %r1264, %r1267, %r1261}, {%r3554, %r3526}, {%r3465, %r3465};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3598, %r3599, %r3599, %r28894;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28904, %r3603, %r3603, %r3598;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3606, %r3607, %r3607, %r28893;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28903, %r3611, %r3611, %r3606;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3614, %r3615, %r3615, %r28892;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28902, %r3619, %r3619, %r3614;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3622, %r3623, %r3623, %r28891;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28901, %r3627, %r3627, %r3622;
	// end inline asm
	add.s32 	%r28899, %r28889, 1;
	setp.ne.s32 	%p54, %r28899, 40;
	@%p54 bra 	LBB0_27;
// %bb.24:                              // %L22131
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_26;
// %bb.25:                              // %L22196
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r3630, %r28856, 193536;
	add.s32 	%r3631, %r83, %r3630;
	or.b32  	%r3632, %r3631, %r43;
	add.s32 	%r3633, %r3632, %r84;
	mul.wide.u32 	%rd257, %r3633, 4;
	add.s64 	%rd258, %rd4, %rd257;
	st.global.u32 	[%rd258], %r28904;
	add.s32 	%r3634, %r3632, %r85;
	mul.wide.u32 	%rd259, %r3634, 4;
	add.s64 	%rd260, %rd4, %rd259;
	st.global.u32 	[%rd260], %r28903;
	cvt.u64.u32 	%rd261, %r84;
	cvt.u64.u32 	%rd262, %r43;
	cvt.u64.u32 	%rd263, %r83;
	cvt.u64.u32 	%rd264, %r3630;
	add.s64 	%rd265, %rd264, %rd263;
	add.s64 	%rd266, %rd265, %rd262;
	add.s64 	%rd267, %rd266, %rd261;
	shl.b64 	%rd268, %rd267, 2;
	add.s64 	%rd269, %rd4, %rd268;
	st.global.u32 	[%rd269+387072], %r28902;
	cvt.u64.u32 	%rd270, %r85;
	add.s64 	%rd271, %rd266, %rd270;
	shl.b64 	%rd272, %rd271, 2;
	add.s64 	%rd273, %rd4, %rd272;
	st.global.u32 	[%rd273+387072], %r28901;
LBB0_26:                                // %L22709
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28899, 0;
	mov.u32 	%r28901, %r28899;
	mov.u32 	%r28902, %r28899;
	mov.u32 	%r28903, %r28899;
	mov.u32 	%r28904, %r28899;
LBB0_27:                                // %L22710
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r3636, 0;
	mov.u32 	%r28905, %r3636;
	mov.u32 	%r28906, %r3636;
	mov.u32 	%r28907, %r3636;
	mov.u32 	%r28908, %r3636;
	@%p50 bra 	LBB0_264;
	bra.uni 	LBB0_28;
LBB0_264:                               // %oksrem8282
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28905, [%rd29];
	ld.shared.u32 	%r28906, [%rd30];
	ld.shared.u32 	%r28907, [%rd31];
	ld.shared.u32 	%r28908, [%rd32];
LBB0_28:                                // %L23829
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3669, %r3672}, {%r1202, %r1199}, {%r28905}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3678, %r3681}, {%r1202, %r1199}, {%r28906}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3687, %r3690}, {%r1202, %r1199}, {%r28907}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3696, %r3699}, {%r1202, %r1199}, {%r28908}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3665, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3667, %r3665, %r3669;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3670, %r1237, %r3672, %r3667;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3674, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3676, %r3674, %r3678;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3679, %r1237, %r3681, %r3676;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3683, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3685, %r3683, %r3687;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3688, %r1237, %r3690, %r3685;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3692, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3694, %r3692, %r3696;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3697, %r1237, %r3699, %r3694;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3701, %r1240, %r3672;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3704, %r1237, %r3669, %r3701;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3708, %r1240, %r3681;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3711, %r1237, %r3678, %r3708;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3715, %r1240, %r3690;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3718, %r1237, %r3687, %r3715;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3722, %r1240, %r3699;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3725, %r1237, %r3696, %r3722;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3770, %r3774}, {%r1270, %r1264, %r1267, %r1261}, {%r3704, %r3670}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3778, %r3782}, {%r1270, %r1264, %r1267, %r1261}, {%r3711, %r3679}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3786, %r3790}, {%r1270, %r1264, %r1267, %r1261}, {%r3718, %r3688}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3794, %r3798}, {%r1270, %r1264, %r1267, %r1261}, {%r3725, %r3697}, {%r3636, %r3636};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3769, %r3770, %r3770, %r28904;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28914, %r3774, %r3774, %r3769;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3777, %r3778, %r3778, %r28903;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28913, %r3782, %r3782, %r3777;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3785, %r3786, %r3786, %r28902;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28912, %r3790, %r3790, %r3785;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3793, %r3794, %r3794, %r28901;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28911, %r3798, %r3798, %r3793;
	// end inline asm
	add.s32 	%r28909, %r28899, 1;
	setp.ne.s32 	%p57, %r28909, 40;
	@%p57 bra 	LBB0_32;
// %bb.29:                              // %L24012
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_31;
// %bb.30:                              // %L24077
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r3801, %r28856, 193536;
	add.s32 	%r3802, %r83, %r3801;
	or.b32  	%r3803, %r3802, %r43;
	add.s32 	%r3804, %r3803, %r84;
	mul.wide.u32 	%rd274, %r3804, 4;
	add.s64 	%rd275, %rd4, %rd274;
	st.global.u32 	[%rd275], %r28914;
	add.s32 	%r3805, %r3803, %r85;
	mul.wide.u32 	%rd276, %r3805, 4;
	add.s64 	%rd277, %rd4, %rd276;
	st.global.u32 	[%rd277], %r28913;
	cvt.u64.u32 	%rd278, %r84;
	cvt.u64.u32 	%rd279, %r43;
	cvt.u64.u32 	%rd280, %r83;
	cvt.u64.u32 	%rd281, %r3801;
	add.s64 	%rd282, %rd281, %rd280;
	add.s64 	%rd283, %rd282, %rd279;
	add.s64 	%rd284, %rd283, %rd278;
	shl.b64 	%rd285, %rd284, 2;
	add.s64 	%rd286, %rd4, %rd285;
	st.global.u32 	[%rd286+387072], %r28912;
	cvt.u64.u32 	%rd287, %r85;
	add.s64 	%rd288, %rd283, %rd287;
	shl.b64 	%rd289, %rd288, 2;
	add.s64 	%rd290, %rd4, %rd289;
	st.global.u32 	[%rd290+387072], %r28911;
LBB0_31:                                // %L24590
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28909, 0;
	mov.u32 	%r28911, %r28909;
	mov.u32 	%r28912, %r28909;
	mov.u32 	%r28913, %r28909;
	mov.u32 	%r28914, %r28909;
LBB0_32:                                // %L24591
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r3807, 0;
	mov.u32 	%r28915, %r3807;
	mov.u32 	%r28916, %r3807;
	mov.u32 	%r28917, %r3807;
	mov.u32 	%r28918, %r3807;
	@%p50 bra 	LBB0_265;
	bra.uni 	LBB0_33;
LBB0_265:                               // %oksrem8980
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28915, [%rd33];
	ld.shared.u32 	%r28916, [%rd34];
	ld.shared.u32 	%r28917, [%rd35];
	ld.shared.u32 	%r28918, [%rd36];
LBB0_33:                                // %L25710
                                        //   in Loop: Header=BB0_16 Depth=1
	xor.b32  	%r125, %r28865, -2004318072;
	xor.b32  	%r127, %r28866, -2004318072;
	xor.b32  	%r129, %r28867, -2004318072;
	xor.b32  	%r131, %r28868, -2004318072;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3840, %r3843}, {%r1202, %r1199}, {%r28915}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3849, %r3852}, {%r1202, %r1199}, {%r28916}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3858, %r3861}, {%r1202, %r1199}, {%r28917}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3867, %r3870}, {%r1202, %r1199}, {%r28918}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3836, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3838, %r3836, %r3840;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3841, %r1237, %r3843, %r3838;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3845, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3847, %r3845, %r3849;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3850, %r1237, %r3852, %r3847;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3854, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3856, %r3854, %r3858;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3859, %r1237, %r3861, %r3856;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3863, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3865, %r3863, %r3867;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3868, %r1237, %r3870, %r3865;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3872, %r1240, %r3843;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3875, %r1237, %r3840, %r3872;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3879, %r1240, %r3852;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3882, %r1237, %r3849, %r3879;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3886, %r1240, %r3861;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3889, %r1237, %r3858, %r3886;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3893, %r1240, %r3870;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3896, %r1237, %r3867, %r3893;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3941, %r3945}, {%r1270, %r1264, %r1267, %r1261}, {%r3875, %r3841}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3949, %r3953}, {%r1270, %r1264, %r1267, %r1261}, {%r3882, %r3850}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3957, %r3961}, {%r1270, %r1264, %r1267, %r1261}, {%r3889, %r3859}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3965, %r3969}, {%r1270, %r1264, %r1267, %r1261}, {%r3896, %r3868}, {%r3807, %r3807};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3940, %r3941, %r3941, %r28914;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28924, %r3945, %r3945, %r3940;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3948, %r3949, %r3949, %r28913;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28923, %r3953, %r3953, %r3948;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3956, %r3957, %r3957, %r28912;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28922, %r3961, %r3961, %r3956;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3964, %r3965, %r3965, %r28911;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28921, %r3969, %r3969, %r3964;
	// end inline asm
	add.s32 	%r28919, %r28909, 1;
	setp.ne.s32 	%p60, %r28919, 40;
	@%p60 bra 	LBB0_37;
// %bb.34:                              // %L25893
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_36;
// %bb.35:                              // %L25958
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r3972, %r28856, 193536;
	add.s32 	%r3973, %r83, %r3972;
	or.b32  	%r3974, %r3973, %r43;
	add.s32 	%r3975, %r3974, %r84;
	mul.wide.u32 	%rd291, %r3975, 4;
	add.s64 	%rd292, %rd4, %rd291;
	st.global.u32 	[%rd292], %r28924;
	add.s32 	%r3976, %r3974, %r85;
	mul.wide.u32 	%rd293, %r3976, 4;
	add.s64 	%rd294, %rd4, %rd293;
	st.global.u32 	[%rd294], %r28923;
	cvt.u64.u32 	%rd295, %r84;
	cvt.u64.u32 	%rd296, %r43;
	cvt.u64.u32 	%rd297, %r83;
	cvt.u64.u32 	%rd298, %r3972;
	add.s64 	%rd299, %rd298, %rd297;
	add.s64 	%rd300, %rd299, %rd296;
	add.s64 	%rd301, %rd300, %rd295;
	shl.b64 	%rd302, %rd301, 2;
	add.s64 	%rd303, %rd4, %rd302;
	st.global.u32 	[%rd303+387072], %r28922;
	cvt.u64.u32 	%rd304, %r85;
	add.s64 	%rd305, %rd300, %rd304;
	shl.b64 	%rd306, %rd305, 2;
	add.s64 	%rd307, %rd4, %rd306;
	st.global.u32 	[%rd307+387072], %r28921;
LBB0_36:                                // %L26471
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28919, 0;
	mov.u32 	%r28921, %r28919;
	mov.u32 	%r28922, %r28919;
	mov.u32 	%r28923, %r28919;
	mov.u32 	%r28924, %r28919;
LBB0_37:                                // %L26472
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r4167, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r4178, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r4164, %r1979, %r125, %r4167, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4168, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4169, %r4167, %r4168;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4172, %r4164, %r4169;
	// end inline asm
	// begin inline asm
	lop3.b32 %r4175, %r1990, %r125, %r4178, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4179, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4180, %r4178, %r4179;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4183, %r4175, %r4180;
	// end inline asm
	// begin inline asm
	mov.b32 %r4213, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r4224, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r4210, %r1979, %r127, %r4213, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4214, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4215, %r4213, %r4214;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4218, %r4210, %r4215;
	// end inline asm
	// begin inline asm
	lop3.b32 %r4221, %r1990, %r127, %r4224, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4225, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4226, %r4224, %r4225;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4229, %r4221, %r4226;
	// end inline asm
	// begin inline asm
	mov.b32 %r4259, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r4270, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r4256, %r1979, %r129, %r4259, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4260, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4261, %r4259, %r4260;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4264, %r4256, %r4261;
	// end inline asm
	// begin inline asm
	lop3.b32 %r4267, %r1990, %r129, %r4270, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4271, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4272, %r4270, %r4271;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4275, %r4267, %r4272;
	// end inline asm
	// begin inline asm
	mov.b32 %r4305, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r4316, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r4302, %r1979, %r131, %r4305, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4306, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4307, %r4305, %r4306;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4310, %r4302, %r4307;
	// end inline asm
	// begin inline asm
	lop3.b32 %r4313, %r1990, %r131, %r4316, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r4317, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r4318, %r4316, %r4317;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r4321, %r4313, %r4318;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r4172;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5082, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r4183;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5085, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r4218;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5088, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r4229;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5091, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r4264;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5094, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r4275;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5097, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r4310;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5100, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r4321;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r5103, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r5434, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5166, %r5169}, {%r1202, %r1199}, {%r5082}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5175, %r5178}, {%r1202, %r1199}, {%r5085}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5184, %r5187}, {%r1202, %r1199}, {%r5088}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5193, %r5196}, {%r1202, %r1199}, {%r5091}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5202, %r5205}, {%r1202, %r1199}, {%r5094}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5211, %r5214}, {%r1202, %r1199}, {%r5097}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5220, %r5223}, {%r1202, %r1199}, {%r5100}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5229, %r5232}, {%r1202, %r1199}, {%r5103}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5162, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5164, %r5162, %r5166;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5167, %r1237, %r5169, %r5164;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5171, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5173, %r5171, %r5175;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5176, %r1237, %r5178, %r5173;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5180, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5182, %r5180, %r5184;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5185, %r1237, %r5187, %r5182;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5189, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5191, %r5189, %r5193;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5194, %r1237, %r5196, %r5191;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5198, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5200, %r5198, %r5202;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5203, %r1237, %r5205, %r5200;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5207, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5209, %r5207, %r5211;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5212, %r1237, %r5214, %r5209;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5216, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5218, %r5216, %r5220;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5221, %r1237, %r5223, %r5218;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5225, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5227, %r5225, %r5229;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5230, %r1237, %r5232, %r5227;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5234, %r1240, %r5169;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5237, %r1237, %r5166, %r5234;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5241, %r1240, %r5178;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5244, %r1237, %r5175, %r5241;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5248, %r1240, %r5187;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5251, %r1237, %r5184, %r5248;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5255, %r1240, %r5196;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5258, %r1237, %r5193, %r5255;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5262, %r1240, %r5205;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5265, %r1237, %r5202, %r5262;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5269, %r1240, %r5214;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5272, %r1237, %r5211, %r5269;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5276, %r1240, %r5223;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5279, %r1237, %r5220, %r5276;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5283, %r1240, %r5232;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5286, %r1237, %r5229, %r5283;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5371, %r5372}, {%r1270, %r1264, %r1267, %r1261}, {%r5237, %r5167}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5379, %r5380}, {%r1270, %r1264, %r1267, %r1261}, {%r5244, %r5176}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5387, %r5388}, {%r1270, %r1264, %r1267, %r1261}, {%r5251, %r5185}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5395, %r5396}, {%r1270, %r1264, %r1267, %r1261}, {%r5258, %r5194}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5403, %r5404}, {%r1270, %r1264, %r1267, %r1261}, {%r5265, %r5203}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5411, %r5412}, {%r1270, %r1264, %r1267, %r1261}, {%r5272, %r5212}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5419, %r5420}, {%r1270, %r1264, %r1267, %r1261}, {%r5279, %r5221}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5427, %r5428}, {%r1270, %r1264, %r1267, %r1261}, {%r5286, %r5230}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	prmt.b32 %r5370, %r5371, %r5372, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5374, %r5371, %r5372, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5378, %r5379, %r5380, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5382, %r5379, %r5380, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5386, %r5387, %r5388, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5390, %r5387, %r5388, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5394, %r5395, %r5396, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5398, %r5395, %r5396, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5402, %r5403, %r5404, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5406, %r5403, %r5404, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5410, %r5411, %r5412, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5414, %r5411, %r5412, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5418, %r5419, %r5420, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5422, %r5419, %r5420, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5426, %r5427, %r5428, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r5430, %r5427, %r5428, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r5370;
	st.shared.u32 	[%rd7+33024], %r5374;
	st.shared.u32 	[%rd8], %r5378;
	st.shared.u32 	[%rd9], %r5382;
	st.shared.u32 	[%rd13], %r5386;
	st.shared.u32 	[%rd14], %r5390;
	st.shared.u32 	[%rd15], %r5394;
	st.shared.u32 	[%rd16], %r5398;
	st.shared.u32 	[%rd17], %r5402;
	st.shared.u32 	[%rd18], %r5406;
	st.shared.u32 	[%rd19], %r5410;
	st.shared.u32 	[%rd20], %r5414;
	st.shared.u32 	[%rd21], %r5418;
	st.shared.u32 	[%rd22], %r5422;
	st.shared.u32 	[%rd23], %r5426;
	st.shared.u32 	[%rd24], %r5430;
	bar.sync 	0;
	mov.u32 	%r28925, %r5434;
	mov.u32 	%r28926, %r5434;
	mov.u32 	%r28927, %r5434;
	mov.u32 	%r28928, %r5434;
	@%p50 bra 	LBB0_266;
	bra.uni 	LBB0_38;
LBB0_266:                               // %oksrem11582
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28925, [%rd10];
	ld.shared.u32 	%r28926, [%rd11+33024];
	ld.shared.u32 	%r28927, [%rd11+128];
	ld.shared.u32 	%r28928, [%rd11+33152];
LBB0_38:                                // %L33741
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5467, %r5470}, {%r1202, %r1199}, {%r28925}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5476, %r5479}, {%r1202, %r1199}, {%r28926}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5485, %r5488}, {%r1202, %r1199}, {%r28927}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5494, %r5497}, {%r1202, %r1199}, {%r28928}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5463, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5465, %r5463, %r5467;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5468, %r1237, %r5470, %r5465;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5472, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5474, %r5472, %r5476;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5477, %r1237, %r5479, %r5474;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5481, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5483, %r5481, %r5485;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5486, %r1237, %r5488, %r5483;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5490, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5492, %r5490, %r5494;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5495, %r1237, %r5497, %r5492;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5499, %r1240, %r5470;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5502, %r1237, %r5467, %r5499;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5506, %r1240, %r5479;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5509, %r1237, %r5476, %r5506;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5513, %r1240, %r5488;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5516, %r1237, %r5485, %r5513;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5520, %r1240, %r5497;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5523, %r1237, %r5494, %r5520;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5568, %r5572}, {%r1270, %r1264, %r1267, %r1261}, {%r5502, %r5468}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5576, %r5580}, {%r1270, %r1264, %r1267, %r1261}, {%r5509, %r5477}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5584, %r5588}, {%r1270, %r1264, %r1267, %r1261}, {%r5516, %r5486}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5592, %r5596}, {%r1270, %r1264, %r1267, %r1261}, {%r5523, %r5495}, {%r5434, %r5434};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5567, %r5568, %r5568, %r28924;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28934, %r5572, %r5572, %r5567;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5575, %r5576, %r5576, %r28923;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28933, %r5580, %r5580, %r5575;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5583, %r5584, %r5584, %r28922;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28932, %r5588, %r5588, %r5583;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5591, %r5592, %r5592, %r28921;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28931, %r5596, %r5596, %r5591;
	// end inline asm
	add.s32 	%r28929, %r28919, 1;
	setp.ne.s32 	%p63, %r28929, 40;
	@%p63 bra 	LBB0_42;
// %bb.39:                              // %L33924
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_41;
// %bb.40:                              // %L33989
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r5599, %r28856, 193536;
	add.s32 	%r5600, %r83, %r5599;
	or.b32  	%r5601, %r5600, %r43;
	add.s32 	%r5602, %r5601, %r84;
	mul.wide.u32 	%rd308, %r5602, 4;
	add.s64 	%rd309, %rd4, %rd308;
	st.global.u32 	[%rd309], %r28934;
	add.s32 	%r5603, %r5601, %r85;
	mul.wide.u32 	%rd310, %r5603, 4;
	add.s64 	%rd311, %rd4, %rd310;
	st.global.u32 	[%rd311], %r28933;
	cvt.u64.u32 	%rd312, %r84;
	cvt.u64.u32 	%rd313, %r43;
	cvt.u64.u32 	%rd314, %r83;
	cvt.u64.u32 	%rd315, %r5599;
	add.s64 	%rd316, %rd315, %rd314;
	add.s64 	%rd317, %rd316, %rd313;
	add.s64 	%rd318, %rd317, %rd312;
	shl.b64 	%rd319, %rd318, 2;
	add.s64 	%rd320, %rd4, %rd319;
	st.global.u32 	[%rd320+387072], %r28932;
	cvt.u64.u32 	%rd321, %r85;
	add.s64 	%rd322, %rd317, %rd321;
	shl.b64 	%rd323, %rd322, 2;
	add.s64 	%rd324, %rd4, %rd323;
	st.global.u32 	[%rd324+387072], %r28931;
LBB0_41:                                // %L34502
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28929, 0;
	mov.u32 	%r28931, %r28929;
	mov.u32 	%r28932, %r28929;
	mov.u32 	%r28933, %r28929;
	mov.u32 	%r28934, %r28929;
LBB0_42:                                // %L34503
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r5605, 0;
	mov.u32 	%r28935, %r5605;
	mov.u32 	%r28936, %r5605;
	mov.u32 	%r28937, %r5605;
	mov.u32 	%r28938, %r5605;
	@%p50 bra 	LBB0_267;
	bra.uni 	LBB0_43;
LBB0_267:                               // %oksrem12280
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28935, [%rd25];
	ld.shared.u32 	%r28936, [%rd26];
	ld.shared.u32 	%r28937, [%rd27];
	ld.shared.u32 	%r28938, [%rd28];
LBB0_43:                                // %L35622
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5638, %r5641}, {%r1202, %r1199}, {%r28935}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5647, %r5650}, {%r1202, %r1199}, {%r28936}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5656, %r5659}, {%r1202, %r1199}, {%r28937}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5665, %r5668}, {%r1202, %r1199}, {%r28938}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5634, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5636, %r5634, %r5638;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5639, %r1237, %r5641, %r5636;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5643, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5645, %r5643, %r5647;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5648, %r1237, %r5650, %r5645;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5652, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5654, %r5652, %r5656;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5657, %r1237, %r5659, %r5654;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5661, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5663, %r5661, %r5665;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5666, %r1237, %r5668, %r5663;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5670, %r1240, %r5641;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5673, %r1237, %r5638, %r5670;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5677, %r1240, %r5650;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5680, %r1237, %r5647, %r5677;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5684, %r1240, %r5659;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5687, %r1237, %r5656, %r5684;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5691, %r1240, %r5668;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5694, %r1237, %r5665, %r5691;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5739, %r5743}, {%r1270, %r1264, %r1267, %r1261}, {%r5673, %r5639}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5747, %r5751}, {%r1270, %r1264, %r1267, %r1261}, {%r5680, %r5648}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5755, %r5759}, {%r1270, %r1264, %r1267, %r1261}, {%r5687, %r5657}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5763, %r5767}, {%r1270, %r1264, %r1267, %r1261}, {%r5694, %r5666}, {%r5605, %r5605};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5738, %r5739, %r5739, %r28934;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28944, %r5743, %r5743, %r5738;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5746, %r5747, %r5747, %r28933;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28943, %r5751, %r5751, %r5746;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5754, %r5755, %r5755, %r28932;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28942, %r5759, %r5759, %r5754;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5762, %r5763, %r5763, %r28931;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28941, %r5767, %r5767, %r5762;
	// end inline asm
	add.s32 	%r28939, %r28929, 1;
	setp.ne.s32 	%p66, %r28939, 40;
	@%p66 bra 	LBB0_47;
// %bb.44:                              // %L35805
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_46;
// %bb.45:                              // %L35870
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r5770, %r28856, 193536;
	add.s32 	%r5771, %r83, %r5770;
	or.b32  	%r5772, %r5771, %r43;
	add.s32 	%r5773, %r5772, %r84;
	mul.wide.u32 	%rd325, %r5773, 4;
	add.s64 	%rd326, %rd4, %rd325;
	st.global.u32 	[%rd326], %r28944;
	add.s32 	%r5774, %r5772, %r85;
	mul.wide.u32 	%rd327, %r5774, 4;
	add.s64 	%rd328, %rd4, %rd327;
	st.global.u32 	[%rd328], %r28943;
	cvt.u64.u32 	%rd329, %r84;
	cvt.u64.u32 	%rd330, %r43;
	cvt.u64.u32 	%rd331, %r83;
	cvt.u64.u32 	%rd332, %r5770;
	add.s64 	%rd333, %rd332, %rd331;
	add.s64 	%rd334, %rd333, %rd330;
	add.s64 	%rd335, %rd334, %rd329;
	shl.b64 	%rd336, %rd335, 2;
	add.s64 	%rd337, %rd4, %rd336;
	st.global.u32 	[%rd337+387072], %r28942;
	cvt.u64.u32 	%rd338, %r85;
	add.s64 	%rd339, %rd334, %rd338;
	shl.b64 	%rd340, %rd339, 2;
	add.s64 	%rd341, %rd4, %rd340;
	st.global.u32 	[%rd341+387072], %r28941;
LBB0_46:                                // %L36383
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28939, 0;
	mov.u32 	%r28941, %r28939;
	mov.u32 	%r28942, %r28939;
	mov.u32 	%r28943, %r28939;
	mov.u32 	%r28944, %r28939;
LBB0_47:                                // %L36384
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r5776, 0;
	mov.u32 	%r28945, %r5776;
	mov.u32 	%r28946, %r5776;
	mov.u32 	%r28947, %r5776;
	mov.u32 	%r28948, %r5776;
	@%p50 bra 	LBB0_268;
	bra.uni 	LBB0_48;
LBB0_268:                               // %oksrem12978
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28945, [%rd29];
	ld.shared.u32 	%r28946, [%rd30];
	ld.shared.u32 	%r28947, [%rd31];
	ld.shared.u32 	%r28948, [%rd32];
LBB0_48:                                // %L37503
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5809, %r5812}, {%r1202, %r1199}, {%r28945}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5818, %r5821}, {%r1202, %r1199}, {%r28946}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5827, %r5830}, {%r1202, %r1199}, {%r28947}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5836, %r5839}, {%r1202, %r1199}, {%r28948}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5805, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5807, %r5805, %r5809;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5810, %r1237, %r5812, %r5807;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5814, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5816, %r5814, %r5818;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5819, %r1237, %r5821, %r5816;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5823, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5825, %r5823, %r5827;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5828, %r1237, %r5830, %r5825;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5832, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5834, %r5832, %r5836;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5837, %r1237, %r5839, %r5834;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5841, %r1240, %r5812;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5844, %r1237, %r5809, %r5841;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5848, %r1240, %r5821;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5851, %r1237, %r5818, %r5848;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5855, %r1240, %r5830;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5858, %r1237, %r5827, %r5855;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5862, %r1240, %r5839;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5865, %r1237, %r5836, %r5862;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5910, %r5914}, {%r1270, %r1264, %r1267, %r1261}, {%r5844, %r5810}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5918, %r5922}, {%r1270, %r1264, %r1267, %r1261}, {%r5851, %r5819}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5926, %r5930}, {%r1270, %r1264, %r1267, %r1261}, {%r5858, %r5828}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r5934, %r5938}, {%r1270, %r1264, %r1267, %r1261}, {%r5865, %r5837}, {%r5776, %r5776};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5909, %r5910, %r5910, %r28944;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28954, %r5914, %r5914, %r5909;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5917, %r5918, %r5918, %r28943;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28953, %r5922, %r5922, %r5917;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5925, %r5926, %r5926, %r28942;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28952, %r5930, %r5930, %r5925;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5933, %r5934, %r5934, %r28941;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28951, %r5938, %r5938, %r5933;
	// end inline asm
	add.s32 	%r28949, %r28939, 1;
	setp.ne.s32 	%p69, %r28949, 40;
	@%p69 bra 	LBB0_52;
// %bb.49:                              // %L37686
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_51;
// %bb.50:                              // %L37751
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r5941, %r28856, 193536;
	add.s32 	%r5942, %r83, %r5941;
	or.b32  	%r5943, %r5942, %r43;
	add.s32 	%r5944, %r5943, %r84;
	mul.wide.u32 	%rd342, %r5944, 4;
	add.s64 	%rd343, %rd4, %rd342;
	st.global.u32 	[%rd343], %r28954;
	add.s32 	%r5945, %r5943, %r85;
	mul.wide.u32 	%rd344, %r5945, 4;
	add.s64 	%rd345, %rd4, %rd344;
	st.global.u32 	[%rd345], %r28953;
	cvt.u64.u32 	%rd346, %r84;
	cvt.u64.u32 	%rd347, %r43;
	cvt.u64.u32 	%rd348, %r83;
	cvt.u64.u32 	%rd349, %r5941;
	add.s64 	%rd350, %rd349, %rd348;
	add.s64 	%rd351, %rd350, %rd347;
	add.s64 	%rd352, %rd351, %rd346;
	shl.b64 	%rd353, %rd352, 2;
	add.s64 	%rd354, %rd4, %rd353;
	st.global.u32 	[%rd354+387072], %r28952;
	cvt.u64.u32 	%rd355, %r85;
	add.s64 	%rd356, %rd351, %rd355;
	shl.b64 	%rd357, %rd356, 2;
	add.s64 	%rd358, %rd4, %rd357;
	st.global.u32 	[%rd358+387072], %r28951;
LBB0_51:                                // %L38264
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28949, 0;
	mov.u32 	%r28951, %r28949;
	mov.u32 	%r28952, %r28949;
	mov.u32 	%r28953, %r28949;
	mov.u32 	%r28954, %r28949;
LBB0_52:                                // %L38265
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r5947, 0;
	mov.u32 	%r28955, %r5947;
	mov.u32 	%r28956, %r5947;
	mov.u32 	%r28957, %r5947;
	mov.u32 	%r28958, %r5947;
	@%p50 bra 	LBB0_269;
	bra.uni 	LBB0_53;
LBB0_269:                               // %oksrem13676
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28955, [%rd33];
	ld.shared.u32 	%r28956, [%rd34];
	ld.shared.u32 	%r28957, [%rd35];
	ld.shared.u32 	%r28958, [%rd36];
LBB0_53:                                // %L39384
                                        //   in Loop: Header=BB0_16 Depth=1
	xor.b32  	%r133, %r28869, -2004318072;
	xor.b32  	%r135, %r28870, -2004318072;
	xor.b32  	%r137, %r28871, -2004318072;
	xor.b32  	%r139, %r28872, -2004318072;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5980, %r5983}, {%r1202, %r1199}, {%r28955}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5989, %r5992}, {%r1202, %r1199}, {%r28956}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r5998, %r6001}, {%r1202, %r1199}, {%r28957}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r6007, %r6010}, {%r1202, %r1199}, {%r28958}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5976, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5978, %r5976, %r5980;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5981, %r1237, %r5983, %r5978;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5985, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5987, %r5985, %r5989;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5990, %r1237, %r5992, %r5987;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r5994, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r5996, %r5994, %r5998;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r5999, %r1237, %r6001, %r5996;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r6003, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r6005, %r6003, %r6007;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6008, %r1237, %r6010, %r6005;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r6012, %r1240, %r5983;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6015, %r1237, %r5980, %r6012;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r6019, %r1240, %r5992;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6022, %r1237, %r5989, %r6019;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r6026, %r1240, %r6001;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6029, %r1237, %r5998, %r6026;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r6033, %r1240, %r6010;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6036, %r1237, %r6007, %r6033;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r6081, %r6085}, {%r1270, %r1264, %r1267, %r1261}, {%r6015, %r5981}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r6089, %r6093}, {%r1270, %r1264, %r1267, %r1261}, {%r6022, %r5990}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r6097, %r6101}, {%r1270, %r1264, %r1267, %r1261}, {%r6029, %r5999}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r6105, %r6109}, {%r1270, %r1264, %r1267, %r1261}, {%r6036, %r6008}, {%r5947, %r5947};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6080, %r6081, %r6081, %r28954;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28964, %r6085, %r6085, %r6080;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6088, %r6089, %r6089, %r28953;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28963, %r6093, %r6093, %r6088;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6096, %r6097, %r6097, %r28952;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28962, %r6101, %r6101, %r6096;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r6104, %r6105, %r6105, %r28951;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28961, %r6109, %r6109, %r6104;
	// end inline asm
	add.s32 	%r28959, %r28949, 1;
	setp.ne.s32 	%p72, %r28959, 40;
	@%p72 bra 	LBB0_57;
// %bb.54:                              // %L39567
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_56;
// %bb.55:                              // %L39632
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r6112, %r28856, 193536;
	add.s32 	%r6113, %r83, %r6112;
	or.b32  	%r6114, %r6113, %r43;
	add.s32 	%r6115, %r6114, %r84;
	mul.wide.u32 	%rd359, %r6115, 4;
	add.s64 	%rd360, %rd4, %rd359;
	st.global.u32 	[%rd360], %r28964;
	add.s32 	%r6116, %r6114, %r85;
	mul.wide.u32 	%rd361, %r6116, 4;
	add.s64 	%rd362, %rd4, %rd361;
	st.global.u32 	[%rd362], %r28963;
	cvt.u64.u32 	%rd363, %r84;
	cvt.u64.u32 	%rd364, %r43;
	cvt.u64.u32 	%rd365, %r83;
	cvt.u64.u32 	%rd366, %r6112;
	add.s64 	%rd367, %rd366, %rd365;
	add.s64 	%rd368, %rd367, %rd364;
	add.s64 	%rd369, %rd368, %rd363;
	shl.b64 	%rd370, %rd369, 2;
	add.s64 	%rd371, %rd4, %rd370;
	st.global.u32 	[%rd371+387072], %r28962;
	cvt.u64.u32 	%rd372, %r85;
	add.s64 	%rd373, %rd368, %rd372;
	shl.b64 	%rd374, %rd373, 2;
	add.s64 	%rd375, %rd4, %rd374;
	st.global.u32 	[%rd375+387072], %r28961;
LBB0_56:                                // %L40145
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28959, 0;
	mov.u32 	%r28961, %r28959;
	mov.u32 	%r28962, %r28959;
	mov.u32 	%r28963, %r28959;
	mov.u32 	%r28964, %r28959;
LBB0_57:                                // %L40146
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r6491, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r6502, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r6488, %r1979, %r133, %r6491, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6492, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6493, %r6491, %r6492;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6496, %r6488, %r6493;
	// end inline asm
	// begin inline asm
	lop3.b32 %r6499, %r1990, %r133, %r6502, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6503, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6504, %r6502, %r6503;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6507, %r6499, %r6504;
	// end inline asm
	// begin inline asm
	mov.b32 %r6537, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r6548, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r6534, %r1979, %r135, %r6537, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6538, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6539, %r6537, %r6538;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6542, %r6534, %r6539;
	// end inline asm
	// begin inline asm
	lop3.b32 %r6545, %r1990, %r135, %r6548, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6549, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6550, %r6548, %r6549;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6553, %r6545, %r6550;
	// end inline asm
	// begin inline asm
	mov.b32 %r6583, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r6594, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r6580, %r1979, %r137, %r6583, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6584, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6585, %r6583, %r6584;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6588, %r6580, %r6585;
	// end inline asm
	// begin inline asm
	lop3.b32 %r6591, %r1990, %r137, %r6594, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6595, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6596, %r6594, %r6595;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6599, %r6591, %r6596;
	// end inline asm
	// begin inline asm
	mov.b32 %r6629, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r6640, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r6626, %r1979, %r139, %r6629, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6630, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6631, %r6629, %r6630;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6634, %r6626, %r6631;
	// end inline asm
	// begin inline asm
	lop3.b32 %r6637, %r1990, %r139, %r6640, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r6641, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r6642, %r6640, %r6641;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r6645, %r6637, %r6642;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r6496;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7222, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r6507;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7225, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r6542;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7228, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r6553;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7231, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r6588;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7234, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r6599;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7237, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r6634;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7240, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r6645;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r7243, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r7574, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7306, %r7309}, {%r1202, %r1199}, {%r7222}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7315, %r7318}, {%r1202, %r1199}, {%r7225}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7324, %r7327}, {%r1202, %r1199}, {%r7228}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7333, %r7336}, {%r1202, %r1199}, {%r7231}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7342, %r7345}, {%r1202, %r1199}, {%r7234}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7351, %r7354}, {%r1202, %r1199}, {%r7237}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7360, %r7363}, {%r1202, %r1199}, {%r7240}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7369, %r7372}, {%r1202, %r1199}, {%r7243}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7302, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7304, %r7302, %r7306;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7307, %r1237, %r7309, %r7304;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7311, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7313, %r7311, %r7315;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7316, %r1237, %r7318, %r7313;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7320, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7322, %r7320, %r7324;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7325, %r1237, %r7327, %r7322;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7329, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7331, %r7329, %r7333;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7334, %r1237, %r7336, %r7331;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7338, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7340, %r7338, %r7342;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7343, %r1237, %r7345, %r7340;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7347, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7349, %r7347, %r7351;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7352, %r1237, %r7354, %r7349;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7356, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7358, %r7356, %r7360;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7361, %r1237, %r7363, %r7358;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7365, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7367, %r7365, %r7369;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7370, %r1237, %r7372, %r7367;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7374, %r1240, %r7309;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7377, %r1237, %r7306, %r7374;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7381, %r1240, %r7318;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7384, %r1237, %r7315, %r7381;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7388, %r1240, %r7327;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7391, %r1237, %r7324, %r7388;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7395, %r1240, %r7336;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7398, %r1237, %r7333, %r7395;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7402, %r1240, %r7345;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7405, %r1237, %r7342, %r7402;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7409, %r1240, %r7354;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7412, %r1237, %r7351, %r7409;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7416, %r1240, %r7363;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7419, %r1237, %r7360, %r7416;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7423, %r1240, %r7372;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7426, %r1237, %r7369, %r7423;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7511, %r7512}, {%r1270, %r1264, %r1267, %r1261}, {%r7377, %r7307}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7519, %r7520}, {%r1270, %r1264, %r1267, %r1261}, {%r7384, %r7316}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7527, %r7528}, {%r1270, %r1264, %r1267, %r1261}, {%r7391, %r7325}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7535, %r7536}, {%r1270, %r1264, %r1267, %r1261}, {%r7398, %r7334}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7543, %r7544}, {%r1270, %r1264, %r1267, %r1261}, {%r7405, %r7343}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7551, %r7552}, {%r1270, %r1264, %r1267, %r1261}, {%r7412, %r7352}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7559, %r7560}, {%r1270, %r1264, %r1267, %r1261}, {%r7419, %r7361}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7567, %r7568}, {%r1270, %r1264, %r1267, %r1261}, {%r7426, %r7370}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	prmt.b32 %r7510, %r7511, %r7512, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7514, %r7511, %r7512, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7518, %r7519, %r7520, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7522, %r7519, %r7520, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7526, %r7527, %r7528, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7530, %r7527, %r7528, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7534, %r7535, %r7536, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7538, %r7535, %r7536, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7542, %r7543, %r7544, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7546, %r7543, %r7544, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7550, %r7551, %r7552, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7554, %r7551, %r7552, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7558, %r7559, %r7560, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7562, %r7559, %r7560, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7566, %r7567, %r7568, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r7570, %r7567, %r7568, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r7510;
	st.shared.u32 	[%rd7+33024], %r7514;
	st.shared.u32 	[%rd8], %r7518;
	st.shared.u32 	[%rd9], %r7522;
	st.shared.u32 	[%rd13], %r7526;
	st.shared.u32 	[%rd14], %r7530;
	st.shared.u32 	[%rd15], %r7534;
	st.shared.u32 	[%rd16], %r7538;
	st.shared.u32 	[%rd17], %r7542;
	st.shared.u32 	[%rd18], %r7546;
	st.shared.u32 	[%rd19], %r7550;
	st.shared.u32 	[%rd20], %r7554;
	st.shared.u32 	[%rd21], %r7558;
	st.shared.u32 	[%rd22], %r7562;
	st.shared.u32 	[%rd23], %r7566;
	st.shared.u32 	[%rd24], %r7570;
	bar.sync 	0;
	mov.u32 	%r28965, %r7574;
	mov.u32 	%r28966, %r7574;
	mov.u32 	%r28967, %r7574;
	mov.u32 	%r28968, %r7574;
	@%p50 bra 	LBB0_270;
	bra.uni 	LBB0_58;
LBB0_270:                               // %oksrem16278
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28965, [%rd10];
	ld.shared.u32 	%r28966, [%rd11+33024];
	ld.shared.u32 	%r28967, [%rd11+128];
	ld.shared.u32 	%r28968, [%rd11+33152];
LBB0_58:                                // %L47415
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7607, %r7610}, {%r1202, %r1199}, {%r28965}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7616, %r7619}, {%r1202, %r1199}, {%r28966}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7625, %r7628}, {%r1202, %r1199}, {%r28967}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7634, %r7637}, {%r1202, %r1199}, {%r28968}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7603, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7605, %r7603, %r7607;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7608, %r1237, %r7610, %r7605;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7612, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7614, %r7612, %r7616;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7617, %r1237, %r7619, %r7614;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7621, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7623, %r7621, %r7625;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7626, %r1237, %r7628, %r7623;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7630, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7632, %r7630, %r7634;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7635, %r1237, %r7637, %r7632;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7639, %r1240, %r7610;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7642, %r1237, %r7607, %r7639;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7646, %r1240, %r7619;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7649, %r1237, %r7616, %r7646;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7653, %r1240, %r7628;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7656, %r1237, %r7625, %r7653;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7660, %r1240, %r7637;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7663, %r1237, %r7634, %r7660;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7708, %r7712}, {%r1270, %r1264, %r1267, %r1261}, {%r7642, %r7608}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7716, %r7720}, {%r1270, %r1264, %r1267, %r1261}, {%r7649, %r7617}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7724, %r7728}, {%r1270, %r1264, %r1267, %r1261}, {%r7656, %r7626}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7732, %r7736}, {%r1270, %r1264, %r1267, %r1261}, {%r7663, %r7635}, {%r7574, %r7574};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7707, %r7708, %r7708, %r28964;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28974, %r7712, %r7712, %r7707;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7715, %r7716, %r7716, %r28963;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28973, %r7720, %r7720, %r7715;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7723, %r7724, %r7724, %r28962;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28972, %r7728, %r7728, %r7723;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7731, %r7732, %r7732, %r28961;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28971, %r7736, %r7736, %r7731;
	// end inline asm
	add.s32 	%r28969, %r28959, 1;
	setp.ne.s32 	%p75, %r28969, 40;
	@%p75 bra 	LBB0_62;
// %bb.59:                              // %L47598
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_61;
// %bb.60:                              // %L47663
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r7739, %r28856, 193536;
	add.s32 	%r7740, %r83, %r7739;
	or.b32  	%r7741, %r7740, %r43;
	add.s32 	%r7742, %r7741, %r84;
	mul.wide.u32 	%rd376, %r7742, 4;
	add.s64 	%rd377, %rd4, %rd376;
	st.global.u32 	[%rd377], %r28974;
	add.s32 	%r7743, %r7741, %r85;
	mul.wide.u32 	%rd378, %r7743, 4;
	add.s64 	%rd379, %rd4, %rd378;
	st.global.u32 	[%rd379], %r28973;
	cvt.u64.u32 	%rd380, %r84;
	cvt.u64.u32 	%rd381, %r43;
	cvt.u64.u32 	%rd382, %r83;
	cvt.u64.u32 	%rd383, %r7739;
	add.s64 	%rd384, %rd383, %rd382;
	add.s64 	%rd385, %rd384, %rd381;
	add.s64 	%rd386, %rd385, %rd380;
	shl.b64 	%rd387, %rd386, 2;
	add.s64 	%rd388, %rd4, %rd387;
	st.global.u32 	[%rd388+387072], %r28972;
	cvt.u64.u32 	%rd389, %r85;
	add.s64 	%rd390, %rd385, %rd389;
	shl.b64 	%rd391, %rd390, 2;
	add.s64 	%rd392, %rd4, %rd391;
	st.global.u32 	[%rd392+387072], %r28971;
LBB0_61:                                // %L48176
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28969, 0;
	mov.u32 	%r28971, %r28969;
	mov.u32 	%r28972, %r28969;
	mov.u32 	%r28973, %r28969;
	mov.u32 	%r28974, %r28969;
LBB0_62:                                // %L48177
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r7745, 0;
	mov.u32 	%r28975, %r7745;
	mov.u32 	%r28976, %r7745;
	mov.u32 	%r28977, %r7745;
	mov.u32 	%r28978, %r7745;
	@%p50 bra 	LBB0_271;
	bra.uni 	LBB0_63;
LBB0_271:                               // %oksrem16976
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28975, [%rd25];
	ld.shared.u32 	%r28976, [%rd26];
	ld.shared.u32 	%r28977, [%rd27];
	ld.shared.u32 	%r28978, [%rd28];
LBB0_63:                                // %L49296
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7778, %r7781}, {%r1202, %r1199}, {%r28975}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7787, %r7790}, {%r1202, %r1199}, {%r28976}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7796, %r7799}, {%r1202, %r1199}, {%r28977}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7805, %r7808}, {%r1202, %r1199}, {%r28978}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7774, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7776, %r7774, %r7778;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7779, %r1237, %r7781, %r7776;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7783, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7785, %r7783, %r7787;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7788, %r1237, %r7790, %r7785;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7792, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7794, %r7792, %r7796;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7797, %r1237, %r7799, %r7794;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7801, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7803, %r7801, %r7805;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7806, %r1237, %r7808, %r7803;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7810, %r1240, %r7781;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7813, %r1237, %r7778, %r7810;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7817, %r1240, %r7790;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7820, %r1237, %r7787, %r7817;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7824, %r1240, %r7799;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7827, %r1237, %r7796, %r7824;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7831, %r1240, %r7808;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7834, %r1237, %r7805, %r7831;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7879, %r7883}, {%r1270, %r1264, %r1267, %r1261}, {%r7813, %r7779}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7887, %r7891}, {%r1270, %r1264, %r1267, %r1261}, {%r7820, %r7788}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7895, %r7899}, {%r1270, %r1264, %r1267, %r1261}, {%r7827, %r7797}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r7903, %r7907}, {%r1270, %r1264, %r1267, %r1261}, {%r7834, %r7806}, {%r7745, %r7745};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7878, %r7879, %r7879, %r28974;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28984, %r7883, %r7883, %r7878;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7886, %r7887, %r7887, %r28973;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28983, %r7891, %r7891, %r7886;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7894, %r7895, %r7895, %r28972;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28982, %r7899, %r7899, %r7894;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7902, %r7903, %r7903, %r28971;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28981, %r7907, %r7907, %r7902;
	// end inline asm
	add.s32 	%r28979, %r28969, 1;
	setp.ne.s32 	%p78, %r28979, 40;
	@%p78 bra 	LBB0_67;
// %bb.64:                              // %L49479
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_66;
// %bb.65:                              // %L49544
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r7910, %r28856, 193536;
	add.s32 	%r7911, %r83, %r7910;
	or.b32  	%r7912, %r7911, %r43;
	add.s32 	%r7913, %r7912, %r84;
	mul.wide.u32 	%rd393, %r7913, 4;
	add.s64 	%rd394, %rd4, %rd393;
	st.global.u32 	[%rd394], %r28984;
	add.s32 	%r7914, %r7912, %r85;
	mul.wide.u32 	%rd395, %r7914, 4;
	add.s64 	%rd396, %rd4, %rd395;
	st.global.u32 	[%rd396], %r28983;
	cvt.u64.u32 	%rd397, %r84;
	cvt.u64.u32 	%rd398, %r43;
	cvt.u64.u32 	%rd399, %r83;
	cvt.u64.u32 	%rd400, %r7910;
	add.s64 	%rd401, %rd400, %rd399;
	add.s64 	%rd402, %rd401, %rd398;
	add.s64 	%rd403, %rd402, %rd397;
	shl.b64 	%rd404, %rd403, 2;
	add.s64 	%rd405, %rd4, %rd404;
	st.global.u32 	[%rd405+387072], %r28982;
	cvt.u64.u32 	%rd406, %r85;
	add.s64 	%rd407, %rd402, %rd406;
	shl.b64 	%rd408, %rd407, 2;
	add.s64 	%rd409, %rd4, %rd408;
	st.global.u32 	[%rd409+387072], %r28981;
LBB0_66:                                // %L50057
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28979, 0;
	mov.u32 	%r28981, %r28979;
	mov.u32 	%r28982, %r28979;
	mov.u32 	%r28983, %r28979;
	mov.u32 	%r28984, %r28979;
LBB0_67:                                // %L50058
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r7916, 0;
	mov.u32 	%r28985, %r7916;
	mov.u32 	%r28986, %r7916;
	mov.u32 	%r28987, %r7916;
	mov.u32 	%r28988, %r7916;
	@%p50 bra 	LBB0_272;
	bra.uni 	LBB0_68;
LBB0_272:                               // %oksrem17674
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28985, [%rd29];
	ld.shared.u32 	%r28986, [%rd30];
	ld.shared.u32 	%r28987, [%rd31];
	ld.shared.u32 	%r28988, [%rd32];
LBB0_68:                                // %L51177
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7949, %r7952}, {%r1202, %r1199}, {%r28985}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7958, %r7961}, {%r1202, %r1199}, {%r28986}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7967, %r7970}, {%r1202, %r1199}, {%r28987}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r7976, %r7979}, {%r1202, %r1199}, {%r28988}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7945, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7947, %r7945, %r7949;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7950, %r1237, %r7952, %r7947;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7954, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7956, %r7954, %r7958;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7959, %r1237, %r7961, %r7956;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7963, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7965, %r7963, %r7967;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7968, %r1237, %r7970, %r7965;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r7972, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7974, %r7972, %r7976;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7977, %r1237, %r7979, %r7974;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7981, %r1240, %r7952;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7984, %r1237, %r7949, %r7981;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7988, %r1240, %r7961;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7991, %r1237, %r7958, %r7988;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r7995, %r1240, %r7970;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r7998, %r1237, %r7967, %r7995;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8002, %r1240, %r7979;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8005, %r1237, %r7976, %r8002;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8050, %r8054}, {%r1270, %r1264, %r1267, %r1261}, {%r7984, %r7950}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8058, %r8062}, {%r1270, %r1264, %r1267, %r1261}, {%r7991, %r7959}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8066, %r8070}, {%r1270, %r1264, %r1267, %r1261}, {%r7998, %r7968}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8074, %r8078}, {%r1270, %r1264, %r1267, %r1261}, {%r8005, %r7977}, {%r7916, %r7916};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8049, %r8050, %r8050, %r28984;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28994, %r8054, %r8054, %r8049;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8057, %r8058, %r8058, %r28983;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28993, %r8062, %r8062, %r8057;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8065, %r8066, %r8066, %r28982;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28992, %r8070, %r8070, %r8065;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8073, %r8074, %r8074, %r28981;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28991, %r8078, %r8078, %r8073;
	// end inline asm
	add.s32 	%r28989, %r28979, 1;
	setp.ne.s32 	%p81, %r28989, 40;
	@%p81 bra 	LBB0_72;
// %bb.69:                              // %L51360
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_71;
// %bb.70:                              // %L51425
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r8081, %r28856, 193536;
	add.s32 	%r8082, %r83, %r8081;
	or.b32  	%r8083, %r8082, %r43;
	add.s32 	%r8084, %r8083, %r84;
	mul.wide.u32 	%rd410, %r8084, 4;
	add.s64 	%rd411, %rd4, %rd410;
	st.global.u32 	[%rd411], %r28994;
	add.s32 	%r8085, %r8083, %r85;
	mul.wide.u32 	%rd412, %r8085, 4;
	add.s64 	%rd413, %rd4, %rd412;
	st.global.u32 	[%rd413], %r28993;
	cvt.u64.u32 	%rd414, %r84;
	cvt.u64.u32 	%rd415, %r43;
	cvt.u64.u32 	%rd416, %r83;
	cvt.u64.u32 	%rd417, %r8081;
	add.s64 	%rd418, %rd417, %rd416;
	add.s64 	%rd419, %rd418, %rd415;
	add.s64 	%rd420, %rd419, %rd414;
	shl.b64 	%rd421, %rd420, 2;
	add.s64 	%rd422, %rd4, %rd421;
	st.global.u32 	[%rd422+387072], %r28992;
	cvt.u64.u32 	%rd423, %r85;
	add.s64 	%rd424, %rd419, %rd423;
	shl.b64 	%rd425, %rd424, 2;
	add.s64 	%rd426, %rd4, %rd425;
	st.global.u32 	[%rd426+387072], %r28991;
LBB0_71:                                // %L51938
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28989, 0;
	mov.u32 	%r28991, %r28989;
	mov.u32 	%r28992, %r28989;
	mov.u32 	%r28993, %r28989;
	mov.u32 	%r28994, %r28989;
LBB0_72:                                // %L51939
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r8087, 0;
	mov.u32 	%r28995, %r8087;
	mov.u32 	%r28996, %r8087;
	mov.u32 	%r28997, %r8087;
	mov.u32 	%r28998, %r8087;
	@%p50 bra 	LBB0_273;
	bra.uni 	LBB0_73;
LBB0_273:                               // %oksrem18372
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r28995, [%rd33];
	ld.shared.u32 	%r28996, [%rd34];
	ld.shared.u32 	%r28997, [%rd35];
	ld.shared.u32 	%r28998, [%rd36];
LBB0_73:                                // %L53058
                                        //   in Loop: Header=BB0_16 Depth=1
	xor.b32  	%r141, %r28873, -2004318072;
	xor.b32  	%r143, %r28874, -2004318072;
	xor.b32  	%r145, %r28875, -2004318072;
	xor.b32  	%r147, %r28876, -2004318072;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r8120, %r8123}, {%r1202, %r1199}, {%r28995}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r8129, %r8132}, {%r1202, %r1199}, {%r28996}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r8138, %r8141}, {%r1202, %r1199}, {%r28997}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r8147, %r8150}, {%r1202, %r1199}, {%r28998}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r8116, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8118, %r8116, %r8120;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8121, %r1237, %r8123, %r8118;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r8125, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8127, %r8125, %r8129;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8130, %r1237, %r8132, %r8127;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r8134, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8136, %r8134, %r8138;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8139, %r1237, %r8141, %r8136;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r8143, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8145, %r8143, %r8147;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8148, %r1237, %r8150, %r8145;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8152, %r1240, %r8123;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8155, %r1237, %r8120, %r8152;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8159, %r1240, %r8132;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8162, %r1237, %r8129, %r8159;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8166, %r1240, %r8141;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8169, %r1237, %r8138, %r8166;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r8173, %r1240, %r8150;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8176, %r1237, %r8147, %r8173;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8221, %r8225}, {%r1270, %r1264, %r1267, %r1261}, {%r8155, %r8121}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8229, %r8233}, {%r1270, %r1264, %r1267, %r1261}, {%r8162, %r8130}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8237, %r8241}, {%r1270, %r1264, %r1267, %r1261}, {%r8169, %r8139}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r8245, %r8249}, {%r1270, %r1264, %r1267, %r1261}, {%r8176, %r8148}, {%r8087, %r8087};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8220, %r8221, %r8221, %r28994;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29004, %r8225, %r8225, %r8220;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8228, %r8229, %r8229, %r28993;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29003, %r8233, %r8233, %r8228;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8236, %r8237, %r8237, %r28992;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29002, %r8241, %r8241, %r8236;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r8244, %r8245, %r8245, %r28991;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29001, %r8249, %r8249, %r8244;
	// end inline asm
	add.s32 	%r28999, %r28989, 1;
	setp.ne.s32 	%p84, %r28999, 40;
	@%p84 bra 	LBB0_77;
// %bb.74:                              // %L53241
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_76;
// %bb.75:                              // %L53306
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r8252, %r28856, 193536;
	add.s32 	%r8253, %r83, %r8252;
	or.b32  	%r8254, %r8253, %r43;
	add.s32 	%r8255, %r8254, %r84;
	mul.wide.u32 	%rd427, %r8255, 4;
	add.s64 	%rd428, %rd4, %rd427;
	st.global.u32 	[%rd428], %r29004;
	add.s32 	%r8256, %r8254, %r85;
	mul.wide.u32 	%rd429, %r8256, 4;
	add.s64 	%rd430, %rd4, %rd429;
	st.global.u32 	[%rd430], %r29003;
	cvt.u64.u32 	%rd431, %r84;
	cvt.u64.u32 	%rd432, %r43;
	cvt.u64.u32 	%rd433, %r83;
	cvt.u64.u32 	%rd434, %r8252;
	add.s64 	%rd435, %rd434, %rd433;
	add.s64 	%rd436, %rd435, %rd432;
	add.s64 	%rd437, %rd436, %rd431;
	shl.b64 	%rd438, %rd437, 2;
	add.s64 	%rd439, %rd4, %rd438;
	st.global.u32 	[%rd439+387072], %r29002;
	cvt.u64.u32 	%rd440, %r85;
	add.s64 	%rd441, %rd436, %rd440;
	shl.b64 	%rd442, %rd441, 2;
	add.s64 	%rd443, %rd4, %rd442;
	st.global.u32 	[%rd443+387072], %r29001;
LBB0_76:                                // %L53819
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r28999, 0;
	mov.u32 	%r29001, %r28999;
	mov.u32 	%r29002, %r28999;
	mov.u32 	%r29003, %r28999;
	mov.u32 	%r29004, %r28999;
LBB0_77:                                // %L53820
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r8815, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r8826, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r8812, %r1979, %r141, %r8815, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8816, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8817, %r8815, %r8816;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8820, %r8812, %r8817;
	// end inline asm
	// begin inline asm
	lop3.b32 %r8823, %r1990, %r141, %r8826, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8827, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8828, %r8826, %r8827;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8831, %r8823, %r8828;
	// end inline asm
	// begin inline asm
	mov.b32 %r8861, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r8872, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r8858, %r1979, %r143, %r8861, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8862, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8863, %r8861, %r8862;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8866, %r8858, %r8863;
	// end inline asm
	// begin inline asm
	lop3.b32 %r8869, %r1990, %r143, %r8872, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8873, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8874, %r8872, %r8873;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8877, %r8869, %r8874;
	// end inline asm
	// begin inline asm
	mov.b32 %r8907, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r8918, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r8904, %r1979, %r145, %r8907, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8908, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8909, %r8907, %r8908;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8912, %r8904, %r8909;
	// end inline asm
	// begin inline asm
	lop3.b32 %r8915, %r1990, %r145, %r8918, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8919, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8920, %r8918, %r8919;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8923, %r8915, %r8920;
	// end inline asm
	// begin inline asm
	mov.b32 %r8953, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r8964, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r8950, %r1979, %r147, %r8953, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8954, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8955, %r8953, %r8954;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8958, %r8950, %r8955;
	// end inline asm
	// begin inline asm
	lop3.b32 %r8961, %r1990, %r147, %r8964, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r8965, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r8966, %r8964, %r8965;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r8969, %r8961, %r8966;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r8820;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9362, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r8831;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9365, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r8866;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9368, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r8877;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9371, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r8912;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9374, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r8923;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9377, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r8958;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9380, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r8969;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r9383, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r9714, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9446, %r9449}, {%r1202, %r1199}, {%r9362}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9455, %r9458}, {%r1202, %r1199}, {%r9365}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9464, %r9467}, {%r1202, %r1199}, {%r9368}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9473, %r9476}, {%r1202, %r1199}, {%r9371}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9482, %r9485}, {%r1202, %r1199}, {%r9374}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9491, %r9494}, {%r1202, %r1199}, {%r9377}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9500, %r9503}, {%r1202, %r1199}, {%r9380}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9509, %r9512}, {%r1202, %r1199}, {%r9383}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9442, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9444, %r9442, %r9446;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9447, %r1237, %r9449, %r9444;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9451, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9453, %r9451, %r9455;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9456, %r1237, %r9458, %r9453;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9460, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9462, %r9460, %r9464;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9465, %r1237, %r9467, %r9462;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9469, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9471, %r9469, %r9473;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9474, %r1237, %r9476, %r9471;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9478, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9480, %r9478, %r9482;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9483, %r1237, %r9485, %r9480;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9487, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9489, %r9487, %r9491;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9492, %r1237, %r9494, %r9489;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9496, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9498, %r9496, %r9500;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9501, %r1237, %r9503, %r9498;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9505, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9507, %r9505, %r9509;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9510, %r1237, %r9512, %r9507;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9514, %r1240, %r9449;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9517, %r1237, %r9446, %r9514;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9521, %r1240, %r9458;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9524, %r1237, %r9455, %r9521;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9528, %r1240, %r9467;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9531, %r1237, %r9464, %r9528;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9535, %r1240, %r9476;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9538, %r1237, %r9473, %r9535;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9542, %r1240, %r9485;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9545, %r1237, %r9482, %r9542;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9549, %r1240, %r9494;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9552, %r1237, %r9491, %r9549;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9556, %r1240, %r9503;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9559, %r1237, %r9500, %r9556;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9563, %r1240, %r9512;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9566, %r1237, %r9509, %r9563;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9651, %r9652}, {%r1270, %r1264, %r1267, %r1261}, {%r9517, %r9447}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9659, %r9660}, {%r1270, %r1264, %r1267, %r1261}, {%r9524, %r9456}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9667, %r9668}, {%r1270, %r1264, %r1267, %r1261}, {%r9531, %r9465}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9675, %r9676}, {%r1270, %r1264, %r1267, %r1261}, {%r9538, %r9474}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9683, %r9684}, {%r1270, %r1264, %r1267, %r1261}, {%r9545, %r9483}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9691, %r9692}, {%r1270, %r1264, %r1267, %r1261}, {%r9552, %r9492}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9699, %r9700}, {%r1270, %r1264, %r1267, %r1261}, {%r9559, %r9501}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9707, %r9708}, {%r1270, %r1264, %r1267, %r1261}, {%r9566, %r9510}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	prmt.b32 %r9650, %r9651, %r9652, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9654, %r9651, %r9652, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9658, %r9659, %r9660, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9662, %r9659, %r9660, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9666, %r9667, %r9668, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9670, %r9667, %r9668, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9674, %r9675, %r9676, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9678, %r9675, %r9676, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9682, %r9683, %r9684, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9686, %r9683, %r9684, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9690, %r9691, %r9692, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9694, %r9691, %r9692, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9698, %r9699, %r9700, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9702, %r9699, %r9700, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9706, %r9707, %r9708, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r9710, %r9707, %r9708, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r9650;
	st.shared.u32 	[%rd7+33024], %r9654;
	st.shared.u32 	[%rd8], %r9658;
	st.shared.u32 	[%rd9], %r9662;
	st.shared.u32 	[%rd13], %r9666;
	st.shared.u32 	[%rd14], %r9670;
	st.shared.u32 	[%rd15], %r9674;
	st.shared.u32 	[%rd16], %r9678;
	st.shared.u32 	[%rd17], %r9682;
	st.shared.u32 	[%rd18], %r9686;
	st.shared.u32 	[%rd19], %r9690;
	st.shared.u32 	[%rd20], %r9694;
	st.shared.u32 	[%rd21], %r9698;
	st.shared.u32 	[%rd22], %r9702;
	st.shared.u32 	[%rd23], %r9706;
	st.shared.u32 	[%rd24], %r9710;
	bar.sync 	0;
	mov.u32 	%r29005, %r9714;
	mov.u32 	%r29006, %r9714;
	mov.u32 	%r29007, %r9714;
	mov.u32 	%r29008, %r9714;
	@%p50 bra 	LBB0_274;
	bra.uni 	LBB0_78;
LBB0_274:                               // %oksrem20974
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29005, [%rd10];
	ld.shared.u32 	%r29006, [%rd11+33024];
	ld.shared.u32 	%r29007, [%rd11+128];
	ld.shared.u32 	%r29008, [%rd11+33152];
LBB0_78:                                // %L61089
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9747, %r9750}, {%r1202, %r1199}, {%r29005}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9756, %r9759}, {%r1202, %r1199}, {%r29006}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9765, %r9768}, {%r1202, %r1199}, {%r29007}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9774, %r9777}, {%r1202, %r1199}, {%r29008}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9743, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9745, %r9743, %r9747;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9748, %r1237, %r9750, %r9745;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9752, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9754, %r9752, %r9756;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9757, %r1237, %r9759, %r9754;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9761, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9763, %r9761, %r9765;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9766, %r1237, %r9768, %r9763;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9770, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9772, %r9770, %r9774;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9775, %r1237, %r9777, %r9772;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9779, %r1240, %r9750;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9782, %r1237, %r9747, %r9779;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9786, %r1240, %r9759;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9789, %r1237, %r9756, %r9786;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9793, %r1240, %r9768;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9796, %r1237, %r9765, %r9793;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9800, %r1240, %r9777;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9803, %r1237, %r9774, %r9800;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9848, %r9852}, {%r1270, %r1264, %r1267, %r1261}, {%r9782, %r9748}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9856, %r9860}, {%r1270, %r1264, %r1267, %r1261}, {%r9789, %r9757}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9864, %r9868}, {%r1270, %r1264, %r1267, %r1261}, {%r9796, %r9766}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r9872, %r9876}, {%r1270, %r1264, %r1267, %r1261}, {%r9803, %r9775}, {%r9714, %r9714};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9847, %r9848, %r9848, %r29004;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29014, %r9852, %r9852, %r9847;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9855, %r9856, %r9856, %r29003;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29013, %r9860, %r9860, %r9855;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9863, %r9864, %r9864, %r29002;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29012, %r9868, %r9868, %r9863;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9871, %r9872, %r9872, %r29001;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29011, %r9876, %r9876, %r9871;
	// end inline asm
	add.s32 	%r29009, %r28999, 1;
	setp.ne.s32 	%p87, %r29009, 40;
	@%p87 bra 	LBB0_82;
// %bb.79:                              // %L61272
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_81;
// %bb.80:                              // %L61337
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r9879, %r28856, 193536;
	add.s32 	%r9880, %r83, %r9879;
	or.b32  	%r9881, %r9880, %r43;
	add.s32 	%r9882, %r9881, %r84;
	mul.wide.u32 	%rd444, %r9882, 4;
	add.s64 	%rd445, %rd4, %rd444;
	st.global.u32 	[%rd445], %r29014;
	add.s32 	%r9883, %r9881, %r85;
	mul.wide.u32 	%rd446, %r9883, 4;
	add.s64 	%rd447, %rd4, %rd446;
	st.global.u32 	[%rd447], %r29013;
	cvt.u64.u32 	%rd448, %r84;
	cvt.u64.u32 	%rd449, %r43;
	cvt.u64.u32 	%rd450, %r83;
	cvt.u64.u32 	%rd451, %r9879;
	add.s64 	%rd452, %rd451, %rd450;
	add.s64 	%rd453, %rd452, %rd449;
	add.s64 	%rd454, %rd453, %rd448;
	shl.b64 	%rd455, %rd454, 2;
	add.s64 	%rd456, %rd4, %rd455;
	st.global.u32 	[%rd456+387072], %r29012;
	cvt.u64.u32 	%rd457, %r85;
	add.s64 	%rd458, %rd453, %rd457;
	shl.b64 	%rd459, %rd458, 2;
	add.s64 	%rd460, %rd4, %rd459;
	st.global.u32 	[%rd460+387072], %r29011;
LBB0_81:                                // %L61850
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29009, 0;
	mov.u32 	%r29011, %r29009;
	mov.u32 	%r29012, %r29009;
	mov.u32 	%r29013, %r29009;
	mov.u32 	%r29014, %r29009;
LBB0_82:                                // %L61851
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r9885, 0;
	mov.u32 	%r29015, %r9885;
	mov.u32 	%r29016, %r9885;
	mov.u32 	%r29017, %r9885;
	mov.u32 	%r29018, %r9885;
	@%p50 bra 	LBB0_275;
	bra.uni 	LBB0_83;
LBB0_275:                               // %oksrem21672
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29015, [%rd25];
	ld.shared.u32 	%r29016, [%rd26];
	ld.shared.u32 	%r29017, [%rd27];
	ld.shared.u32 	%r29018, [%rd28];
LBB0_83:                                // %L62970
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9918, %r9921}, {%r1202, %r1199}, {%r29015}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9927, %r9930}, {%r1202, %r1199}, {%r29016}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9936, %r9939}, {%r1202, %r1199}, {%r29017}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r9945, %r9948}, {%r1202, %r1199}, {%r29018}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9914, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9916, %r9914, %r9918;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9919, %r1237, %r9921, %r9916;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9923, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9925, %r9923, %r9927;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9928, %r1237, %r9930, %r9925;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9932, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9934, %r9932, %r9936;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9937, %r1237, %r9939, %r9934;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r9941, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9943, %r9941, %r9945;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9946, %r1237, %r9948, %r9943;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9950, %r1240, %r9921;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9953, %r1237, %r9918, %r9950;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9957, %r1240, %r9930;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9960, %r1237, %r9927, %r9957;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9964, %r1240, %r9939;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9967, %r1237, %r9936, %r9964;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r9971, %r1240, %r9948;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r9974, %r1237, %r9945, %r9971;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10019, %r10023}, {%r1270, %r1264, %r1267, %r1261}, {%r9953, %r9919}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10027, %r10031}, {%r1270, %r1264, %r1267, %r1261}, {%r9960, %r9928}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10035, %r10039}, {%r1270, %r1264, %r1267, %r1261}, {%r9967, %r9937}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10043, %r10047}, {%r1270, %r1264, %r1267, %r1261}, {%r9974, %r9946}, {%r9885, %r9885};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10018, %r10019, %r10019, %r29014;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29024, %r10023, %r10023, %r10018;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10026, %r10027, %r10027, %r29013;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29023, %r10031, %r10031, %r10026;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10034, %r10035, %r10035, %r29012;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29022, %r10039, %r10039, %r10034;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10042, %r10043, %r10043, %r29011;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29021, %r10047, %r10047, %r10042;
	// end inline asm
	add.s32 	%r29019, %r29009, 1;
	setp.ne.s32 	%p90, %r29019, 40;
	@%p90 bra 	LBB0_87;
// %bb.84:                              // %L63153
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_86;
// %bb.85:                              // %L63218
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r10050, %r28856, 193536;
	add.s32 	%r10051, %r83, %r10050;
	or.b32  	%r10052, %r10051, %r43;
	add.s32 	%r10053, %r10052, %r84;
	mul.wide.u32 	%rd461, %r10053, 4;
	add.s64 	%rd462, %rd4, %rd461;
	st.global.u32 	[%rd462], %r29024;
	add.s32 	%r10054, %r10052, %r85;
	mul.wide.u32 	%rd463, %r10054, 4;
	add.s64 	%rd464, %rd4, %rd463;
	st.global.u32 	[%rd464], %r29023;
	cvt.u64.u32 	%rd465, %r84;
	cvt.u64.u32 	%rd466, %r43;
	cvt.u64.u32 	%rd467, %r83;
	cvt.u64.u32 	%rd468, %r10050;
	add.s64 	%rd469, %rd468, %rd467;
	add.s64 	%rd470, %rd469, %rd466;
	add.s64 	%rd471, %rd470, %rd465;
	shl.b64 	%rd472, %rd471, 2;
	add.s64 	%rd473, %rd4, %rd472;
	st.global.u32 	[%rd473+387072], %r29022;
	cvt.u64.u32 	%rd474, %r85;
	add.s64 	%rd475, %rd470, %rd474;
	shl.b64 	%rd476, %rd475, 2;
	add.s64 	%rd477, %rd4, %rd476;
	st.global.u32 	[%rd477+387072], %r29021;
LBB0_86:                                // %L63731
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29019, 0;
	mov.u32 	%r29021, %r29019;
	mov.u32 	%r29022, %r29019;
	mov.u32 	%r29023, %r29019;
	mov.u32 	%r29024, %r29019;
LBB0_87:                                // %L63732
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r10056, 0;
	mov.u32 	%r29025, %r10056;
	mov.u32 	%r29026, %r10056;
	mov.u32 	%r29027, %r10056;
	mov.u32 	%r29028, %r10056;
	@%p50 bra 	LBB0_276;
	bra.uni 	LBB0_88;
LBB0_276:                               // %oksrem22370
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29025, [%rd29];
	ld.shared.u32 	%r29026, [%rd30];
	ld.shared.u32 	%r29027, [%rd31];
	ld.shared.u32 	%r29028, [%rd32];
LBB0_88:                                // %L64851
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10089, %r10092}, {%r1202, %r1199}, {%r29025}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10098, %r10101}, {%r1202, %r1199}, {%r29026}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10107, %r10110}, {%r1202, %r1199}, {%r29027}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10116, %r10119}, {%r1202, %r1199}, {%r29028}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10085, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10087, %r10085, %r10089;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10090, %r1237, %r10092, %r10087;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10094, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10096, %r10094, %r10098;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10099, %r1237, %r10101, %r10096;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10103, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10105, %r10103, %r10107;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10108, %r1237, %r10110, %r10105;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10112, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10114, %r10112, %r10116;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10117, %r1237, %r10119, %r10114;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10121, %r1240, %r10092;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10124, %r1237, %r10089, %r10121;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10128, %r1240, %r10101;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10131, %r1237, %r10098, %r10128;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10135, %r1240, %r10110;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10138, %r1237, %r10107, %r10135;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10142, %r1240, %r10119;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10145, %r1237, %r10116, %r10142;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10190, %r10194}, {%r1270, %r1264, %r1267, %r1261}, {%r10124, %r10090}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10198, %r10202}, {%r1270, %r1264, %r1267, %r1261}, {%r10131, %r10099}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10206, %r10210}, {%r1270, %r1264, %r1267, %r1261}, {%r10138, %r10108}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10214, %r10218}, {%r1270, %r1264, %r1267, %r1261}, {%r10145, %r10117}, {%r10056, %r10056};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10189, %r10190, %r10190, %r29024;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29034, %r10194, %r10194, %r10189;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10197, %r10198, %r10198, %r29023;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29033, %r10202, %r10202, %r10197;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10205, %r10206, %r10206, %r29022;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29032, %r10210, %r10210, %r10205;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10213, %r10214, %r10214, %r29021;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29031, %r10218, %r10218, %r10213;
	// end inline asm
	add.s32 	%r29029, %r29019, 1;
	setp.ne.s32 	%p93, %r29029, 40;
	@%p93 bra 	LBB0_92;
// %bb.89:                              // %L65034
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_91;
// %bb.90:                              // %L65099
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r10221, %r28856, 193536;
	add.s32 	%r10222, %r83, %r10221;
	or.b32  	%r10223, %r10222, %r43;
	add.s32 	%r10224, %r10223, %r84;
	mul.wide.u32 	%rd478, %r10224, 4;
	add.s64 	%rd479, %rd4, %rd478;
	st.global.u32 	[%rd479], %r29034;
	add.s32 	%r10225, %r10223, %r85;
	mul.wide.u32 	%rd480, %r10225, 4;
	add.s64 	%rd481, %rd4, %rd480;
	st.global.u32 	[%rd481], %r29033;
	cvt.u64.u32 	%rd482, %r84;
	cvt.u64.u32 	%rd483, %r43;
	cvt.u64.u32 	%rd484, %r83;
	cvt.u64.u32 	%rd485, %r10221;
	add.s64 	%rd486, %rd485, %rd484;
	add.s64 	%rd487, %rd486, %rd483;
	add.s64 	%rd488, %rd487, %rd482;
	shl.b64 	%rd489, %rd488, 2;
	add.s64 	%rd490, %rd4, %rd489;
	st.global.u32 	[%rd490+387072], %r29032;
	cvt.u64.u32 	%rd491, %r85;
	add.s64 	%rd492, %rd487, %rd491;
	shl.b64 	%rd493, %rd492, 2;
	add.s64 	%rd494, %rd4, %rd493;
	st.global.u32 	[%rd494+387072], %r29031;
LBB0_91:                                // %L65612
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29029, 0;
	mov.u32 	%r29031, %r29029;
	mov.u32 	%r29032, %r29029;
	mov.u32 	%r29033, %r29029;
	mov.u32 	%r29034, %r29029;
LBB0_92:                                // %L65613
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r10227, 0;
	mov.u32 	%r29035, %r10227;
	mov.u32 	%r29036, %r10227;
	mov.u32 	%r29037, %r10227;
	mov.u32 	%r29038, %r10227;
	@%p50 bra 	LBB0_277;
	bra.uni 	LBB0_93;
LBB0_277:                               // %oksrem23068
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29035, [%rd33];
	ld.shared.u32 	%r29036, [%rd34];
	ld.shared.u32 	%r29037, [%rd35];
	ld.shared.u32 	%r29038, [%rd36];
LBB0_93:                                // %L66732
                                        //   in Loop: Header=BB0_16 Depth=1
	xor.b32  	%r149, %r28877, -2004318072;
	xor.b32  	%r151, %r28878, -2004318072;
	xor.b32  	%r153, %r28879, -2004318072;
	xor.b32  	%r155, %r28880, -2004318072;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10260, %r10263}, {%r1202, %r1199}, {%r29035}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10269, %r10272}, {%r1202, %r1199}, {%r29036}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10278, %r10281}, {%r1202, %r1199}, {%r29037}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r10287, %r10290}, {%r1202, %r1199}, {%r29038}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10256, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10258, %r10256, %r10260;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10261, %r1237, %r10263, %r10258;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10265, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10267, %r10265, %r10269;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10270, %r1237, %r10272, %r10267;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10274, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10276, %r10274, %r10278;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10279, %r1237, %r10281, %r10276;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r10283, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10285, %r10283, %r10287;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10288, %r1237, %r10290, %r10285;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10292, %r1240, %r10263;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10295, %r1237, %r10260, %r10292;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10299, %r1240, %r10272;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10302, %r1237, %r10269, %r10299;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10306, %r1240, %r10281;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10309, %r1237, %r10278, %r10306;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r10313, %r1240, %r10290;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10316, %r1237, %r10287, %r10313;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10361, %r10365}, {%r1270, %r1264, %r1267, %r1261}, {%r10295, %r10261}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10369, %r10373}, {%r1270, %r1264, %r1267, %r1261}, {%r10302, %r10270}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10377, %r10381}, {%r1270, %r1264, %r1267, %r1261}, {%r10309, %r10279}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r10385, %r10389}, {%r1270, %r1264, %r1267, %r1261}, {%r10316, %r10288}, {%r10227, %r10227};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10360, %r10361, %r10361, %r29034;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29044, %r10365, %r10365, %r10360;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10368, %r10369, %r10369, %r29033;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29043, %r10373, %r10373, %r10368;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10376, %r10377, %r10377, %r29032;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29042, %r10381, %r10381, %r10376;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r10384, %r10385, %r10385, %r29031;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29041, %r10389, %r10389, %r10384;
	// end inline asm
	add.s32 	%r29039, %r29029, 1;
	setp.ne.s32 	%p96, %r29039, 40;
	@%p96 bra 	LBB0_97;
// %bb.94:                              // %L66915
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_96;
// %bb.95:                              // %L66980
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r10392, %r28856, 193536;
	add.s32 	%r10393, %r83, %r10392;
	or.b32  	%r10394, %r10393, %r43;
	add.s32 	%r10395, %r10394, %r84;
	mul.wide.u32 	%rd495, %r10395, 4;
	add.s64 	%rd496, %rd4, %rd495;
	st.global.u32 	[%rd496], %r29044;
	add.s32 	%r10396, %r10394, %r85;
	mul.wide.u32 	%rd497, %r10396, 4;
	add.s64 	%rd498, %rd4, %rd497;
	st.global.u32 	[%rd498], %r29043;
	cvt.u64.u32 	%rd499, %r84;
	cvt.u64.u32 	%rd500, %r43;
	cvt.u64.u32 	%rd501, %r83;
	cvt.u64.u32 	%rd502, %r10392;
	add.s64 	%rd503, %rd502, %rd501;
	add.s64 	%rd504, %rd503, %rd500;
	add.s64 	%rd505, %rd504, %rd499;
	shl.b64 	%rd506, %rd505, 2;
	add.s64 	%rd507, %rd4, %rd506;
	st.global.u32 	[%rd507+387072], %r29042;
	cvt.u64.u32 	%rd508, %r85;
	add.s64 	%rd509, %rd504, %rd508;
	shl.b64 	%rd510, %rd509, 2;
	add.s64 	%rd511, %rd4, %rd510;
	st.global.u32 	[%rd511+387072], %r29041;
LBB0_96:                                // %L67493
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29039, 0;
	mov.u32 	%r29041, %r29039;
	mov.u32 	%r29042, %r29039;
	mov.u32 	%r29043, %r29039;
	mov.u32 	%r29044, %r29039;
LBB0_97:                                // %L67494
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r11139, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r11150, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r11136, %r1979, %r149, %r11139, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11140, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11141, %r11139, %r11140;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11144, %r11136, %r11141;
	// end inline asm
	// begin inline asm
	lop3.b32 %r11147, %r1990, %r149, %r11150, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11151, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11152, %r11150, %r11151;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11155, %r11147, %r11152;
	// end inline asm
	// begin inline asm
	mov.b32 %r11185, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r11196, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r11182, %r1979, %r151, %r11185, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11186, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11187, %r11185, %r11186;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11190, %r11182, %r11187;
	// end inline asm
	// begin inline asm
	lop3.b32 %r11193, %r1990, %r151, %r11196, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11197, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11198, %r11196, %r11197;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11201, %r11193, %r11198;
	// end inline asm
	// begin inline asm
	mov.b32 %r11231, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r11242, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r11228, %r1979, %r153, %r11231, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11232, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11233, %r11231, %r11232;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11236, %r11228, %r11233;
	// end inline asm
	// begin inline asm
	lop3.b32 %r11239, %r1990, %r153, %r11242, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11243, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11244, %r11242, %r11243;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11247, %r11239, %r11244;
	// end inline asm
	// begin inline asm
	mov.b32 %r11277, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r11288, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r11274, %r1979, %r155, %r11277, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11278, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11279, %r11277, %r11278;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11282, %r11274, %r11279;
	// end inline asm
	// begin inline asm
	lop3.b32 %r11285, %r1990, %r155, %r11288, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r11289, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r11290, %r11288, %r11289;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r11293, %r11285, %r11290;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r11144;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11502, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r11155;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11505, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r11190;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11508, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r11201;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11511, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r11236;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11514, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r11247;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11517, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r11282;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11520, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r11293;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r11523, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r11854, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11586, %r11589}, {%r1202, %r1199}, {%r11502}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11595, %r11598}, {%r1202, %r1199}, {%r11505}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11604, %r11607}, {%r1202, %r1199}, {%r11508}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11613, %r11616}, {%r1202, %r1199}, {%r11511}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11622, %r11625}, {%r1202, %r1199}, {%r11514}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11631, %r11634}, {%r1202, %r1199}, {%r11517}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11640, %r11643}, {%r1202, %r1199}, {%r11520}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11649, %r11652}, {%r1202, %r1199}, {%r11523}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11582, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11584, %r11582, %r11586;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11587, %r1237, %r11589, %r11584;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11591, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11593, %r11591, %r11595;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11596, %r1237, %r11598, %r11593;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11600, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11602, %r11600, %r11604;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11605, %r1237, %r11607, %r11602;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11609, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11611, %r11609, %r11613;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11614, %r1237, %r11616, %r11611;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11618, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11620, %r11618, %r11622;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11623, %r1237, %r11625, %r11620;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11627, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11629, %r11627, %r11631;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11632, %r1237, %r11634, %r11629;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11636, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11638, %r11636, %r11640;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11641, %r1237, %r11643, %r11638;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11645, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11647, %r11645, %r11649;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11650, %r1237, %r11652, %r11647;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11654, %r1240, %r11589;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11657, %r1237, %r11586, %r11654;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11661, %r1240, %r11598;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11664, %r1237, %r11595, %r11661;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11668, %r1240, %r11607;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11671, %r1237, %r11604, %r11668;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11675, %r1240, %r11616;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11678, %r1237, %r11613, %r11675;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11682, %r1240, %r11625;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11685, %r1237, %r11622, %r11682;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11689, %r1240, %r11634;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11692, %r1237, %r11631, %r11689;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11696, %r1240, %r11643;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11699, %r1237, %r11640, %r11696;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11703, %r1240, %r11652;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11706, %r1237, %r11649, %r11703;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11791, %r11792}, {%r1270, %r1264, %r1267, %r1261}, {%r11657, %r11587}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11799, %r11800}, {%r1270, %r1264, %r1267, %r1261}, {%r11664, %r11596}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11807, %r11808}, {%r1270, %r1264, %r1267, %r1261}, {%r11671, %r11605}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11815, %r11816}, {%r1270, %r1264, %r1267, %r1261}, {%r11678, %r11614}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11823, %r11824}, {%r1270, %r1264, %r1267, %r1261}, {%r11685, %r11623}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11831, %r11832}, {%r1270, %r1264, %r1267, %r1261}, {%r11692, %r11632}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11839, %r11840}, {%r1270, %r1264, %r1267, %r1261}, {%r11699, %r11641}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r11847, %r11848}, {%r1270, %r1264, %r1267, %r1261}, {%r11706, %r11650}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	prmt.b32 %r11790, %r11791, %r11792, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11794, %r11791, %r11792, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11798, %r11799, %r11800, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11802, %r11799, %r11800, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11806, %r11807, %r11808, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11810, %r11807, %r11808, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11814, %r11815, %r11816, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11818, %r11815, %r11816, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11822, %r11823, %r11824, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11826, %r11823, %r11824, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11830, %r11831, %r11832, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11834, %r11831, %r11832, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11838, %r11839, %r11840, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11842, %r11839, %r11840, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11846, %r11847, %r11848, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r11850, %r11847, %r11848, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r11790;
	st.shared.u32 	[%rd7+33024], %r11794;
	st.shared.u32 	[%rd8], %r11798;
	st.shared.u32 	[%rd9], %r11802;
	shr.s16 	%rs1620, %rs3779, 15;
	shr.u16 	%rs1621, %rs1620, 14;
	add.s16 	%rs1622, %rs3779, %rs1621;
	and.b16  	%rs1623, %rs1622, -4;
	sub.s16 	%rs1624, %rs3779, %rs1623;
	mul.wide.s16 	%r421, %rs1624, 64;
	add.s32 	%r11855, %r71, %r421;
	add.s32 	%r11856, %r11855, %r28848;
	add.s32 	%r11857, %r11856, %r72;
	add.s32 	%r11858, %r11857, %r28849;
	add.s32 	%r11859, %r11858, %r73;
	add.s32 	%r11860, %r11859, %r74;
	add.s32 	%r11861, %r11860, %r75;
	mul.wide.s32 	%rd512, %r11861, 4;
	add.s64 	%rd514, %rd52, %rd512;
	st.shared.u32 	[%rd514], %r11806;
	add.s32 	%r422, %r421, 8256;
	add.s32 	%r11862, %r422, %r71;
	add.s32 	%r11863, %r11862, %r28848;
	add.s32 	%r11864, %r11863, %r72;
	add.s32 	%r11865, %r11864, %r28849;
	add.s32 	%r11866, %r11865, %r73;
	add.s32 	%r11867, %r11866, %r74;
	add.s32 	%r11868, %r11867, %r75;
	mul.wide.u32 	%rd515, %r11868, 4;
	add.s64 	%rd516, %rd52, %rd515;
	st.shared.u32 	[%rd516], %r11810;
	add.s32 	%r11869, %r11855, %r76;
	add.s32 	%r11870, %r11869, %r72;
	add.s32 	%r11871, %r11870, %r28849;
	add.s32 	%r11872, %r11871, %r73;
	add.s32 	%r11873, %r11872, %r74;
	add.s32 	%r11874, %r11873, %r75;
	mul.wide.s32 	%rd517, %r11874, 4;
	add.s64 	%rd518, %rd52, %rd517;
	st.shared.u32 	[%rd518], %r11814;
	add.s32 	%r11875, %r11862, %r76;
	add.s32 	%r11876, %r11875, %r72;
	add.s32 	%r11877, %r11876, %r28849;
	add.s32 	%r11878, %r11877, %r73;
	add.s32 	%r11879, %r11878, %r74;
	add.s32 	%r11880, %r11879, %r75;
	mul.wide.s32 	%rd519, %r11880, 4;
	add.s64 	%rd520, %rd52, %rd519;
	st.shared.u32 	[%rd520], %r11818;
	shr.s16 	%rs1626, %rs3780, 15;
	shr.u16 	%rs1627, %rs1626, 14;
	add.s16 	%rs1628, %rs3780, %rs1627;
	and.b16  	%rs1629, %rs1628, -4;
	sub.s16 	%rs1630, %rs3780, %rs1629;
	mul.wide.s16 	%r423, %rs1630, 64;
	add.s32 	%r11881, %r71, %r423;
	add.s32 	%r11882, %r11881, %r28848;
	add.s32 	%r11883, %r11882, %r72;
	add.s32 	%r11884, %r11883, %r28849;
	add.s32 	%r11885, %r11884, %r73;
	add.s32 	%r11886, %r11885, %r74;
	add.s32 	%r11887, %r11886, %r75;
	mul.wide.s32 	%rd521, %r11887, 4;
	add.s64 	%rd522, %rd52, %rd521;
	st.shared.u32 	[%rd522], %r11822;
	add.s32 	%r424, %r423, 8256;
	add.s32 	%r11888, %r424, %r71;
	add.s32 	%r11889, %r11888, %r28848;
	add.s32 	%r11890, %r11889, %r72;
	add.s32 	%r11891, %r11890, %r28849;
	add.s32 	%r11892, %r11891, %r73;
	add.s32 	%r11893, %r11892, %r74;
	add.s32 	%r11894, %r11893, %r75;
	mul.wide.u32 	%rd523, %r11894, 4;
	add.s64 	%rd524, %rd52, %rd523;
	st.shared.u32 	[%rd524], %r11826;
	add.s32 	%r11895, %r11881, %r76;
	add.s32 	%r11896, %r11895, %r72;
	add.s32 	%r11897, %r11896, %r28849;
	add.s32 	%r11898, %r11897, %r73;
	add.s32 	%r11899, %r11898, %r74;
	add.s32 	%r11900, %r11899, %r75;
	mul.wide.s32 	%rd525, %r11900, 4;
	add.s64 	%rd526, %rd52, %rd525;
	st.shared.u32 	[%rd526], %r11830;
	add.s32 	%r11901, %r11888, %r76;
	add.s32 	%r11902, %r11901, %r72;
	add.s32 	%r11903, %r11902, %r28849;
	add.s32 	%r11904, %r11903, %r73;
	add.s32 	%r11905, %r11904, %r74;
	add.s32 	%r11906, %r11905, %r75;
	mul.wide.s32 	%rd527, %r11906, 4;
	add.s64 	%rd528, %rd52, %rd527;
	st.shared.u32 	[%rd528], %r11834;
	shr.s16 	%rs1632, %rs3781, 15;
	shr.u16 	%rs1633, %rs1632, 14;
	add.s16 	%rs1634, %rs3781, %rs1633;
	and.b16  	%rs1635, %rs1634, -4;
	sub.s16 	%rs1636, %rs3781, %rs1635;
	mul.wide.s16 	%r425, %rs1636, 64;
	add.s32 	%r11907, %r71, %r425;
	add.s32 	%r11908, %r11907, %r28848;
	add.s32 	%r11909, %r11908, %r72;
	add.s32 	%r11910, %r11909, %r28849;
	add.s32 	%r11911, %r11910, %r73;
	add.s32 	%r11912, %r11911, %r74;
	add.s32 	%r11913, %r11912, %r75;
	mul.wide.s32 	%rd529, %r11913, 4;
	add.s64 	%rd530, %rd52, %rd529;
	st.shared.u32 	[%rd530], %r11838;
	add.s32 	%r426, %r425, 8256;
	add.s32 	%r11914, %r426, %r71;
	add.s32 	%r11915, %r11914, %r28848;
	add.s32 	%r11916, %r11915, %r72;
	add.s32 	%r11917, %r11916, %r28849;
	add.s32 	%r11918, %r11917, %r73;
	add.s32 	%r11919, %r11918, %r74;
	add.s32 	%r11920, %r11919, %r75;
	mul.wide.u32 	%rd531, %r11920, 4;
	add.s64 	%rd532, %rd52, %rd531;
	st.shared.u32 	[%rd532], %r11842;
	add.s32 	%r11921, %r11907, %r76;
	add.s32 	%r11922, %r11921, %r72;
	add.s32 	%r11923, %r11922, %r28849;
	add.s32 	%r11924, %r11923, %r73;
	add.s32 	%r11925, %r11924, %r74;
	add.s32 	%r11926, %r11925, %r75;
	mul.wide.s32 	%rd533, %r11926, 4;
	add.s64 	%rd534, %rd52, %rd533;
	st.shared.u32 	[%rd534], %r11846;
	add.s32 	%r11927, %r11914, %r76;
	add.s32 	%r11928, %r11927, %r72;
	add.s32 	%r11929, %r11928, %r28849;
	add.s32 	%r11930, %r11929, %r73;
	add.s32 	%r11931, %r11930, %r74;
	add.s32 	%r11932, %r11931, %r75;
	mul.wide.s32 	%rd535, %r11932, 4;
	add.s64 	%rd536, %rd52, %rd535;
	st.shared.u32 	[%rd536], %r11850;
	bar.sync 	0;
	mov.u32 	%r29045, %r11854;
	mov.u32 	%r29046, %r11854;
	mov.u32 	%r29047, %r11854;
	mov.u32 	%r29048, %r11854;
	@%p50 bra 	LBB0_278;
	bra.uni 	LBB0_98;
LBB0_278:                               // %oksrem25670
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29045, [%rd10];
	ld.shared.u32 	%r29046, [%rd11+33024];
	ld.shared.u32 	%r29047, [%rd11+128];
	ld.shared.u32 	%r29048, [%rd11+33152];
LBB0_98:                                // %L74763
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11965, %r11968}, {%r1202, %r1199}, {%r29045}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11974, %r11977}, {%r1202, %r1199}, {%r29046}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11983, %r11986}, {%r1202, %r1199}, {%r29047}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r11992, %r11995}, {%r1202, %r1199}, {%r29048}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11961, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11963, %r11961, %r11965;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11966, %r1237, %r11968, %r11963;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11970, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11972, %r11970, %r11974;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11975, %r1237, %r11977, %r11972;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11979, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11981, %r11979, %r11983;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11984, %r1237, %r11986, %r11981;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r11988, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11990, %r11988, %r11992;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r11993, %r1237, %r11995, %r11990;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r11997, %r1240, %r11968;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12000, %r1237, %r11965, %r11997;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12004, %r1240, %r11977;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12007, %r1237, %r11974, %r12004;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12011, %r1240, %r11986;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12014, %r1237, %r11983, %r12011;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12018, %r1240, %r11995;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12021, %r1237, %r11992, %r12018;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12066, %r12070}, {%r1270, %r1264, %r1267, %r1261}, {%r12000, %r11966}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12074, %r12078}, {%r1270, %r1264, %r1267, %r1261}, {%r12007, %r11975}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12082, %r12086}, {%r1270, %r1264, %r1267, %r1261}, {%r12014, %r11984}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12090, %r12094}, {%r1270, %r1264, %r1267, %r1261}, {%r12021, %r11993}, {%r11854, %r11854};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12065, %r12066, %r12066, %r29044;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29054, %r12070, %r12070, %r12065;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12073, %r12074, %r12074, %r29043;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29053, %r12078, %r12078, %r12073;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12081, %r12082, %r12082, %r29042;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29052, %r12086, %r12086, %r12081;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12089, %r12090, %r12090, %r29041;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29051, %r12094, %r12094, %r12089;
	// end inline asm
	add.s32 	%r29049, %r29039, 1;
	setp.ne.s32 	%p99, %r29049, 40;
	@%p99 bra 	LBB0_102;
// %bb.99:                              // %L74946
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_101;
// %bb.100:                             // %L75011
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r12097, %r28856, 193536;
	add.s32 	%r12098, %r83, %r12097;
	or.b32  	%r12099, %r12098, %r43;
	add.s32 	%r12100, %r12099, %r84;
	mul.wide.u32 	%rd537, %r12100, 4;
	add.s64 	%rd538, %rd4, %rd537;
	st.global.u32 	[%rd538], %r29054;
	add.s32 	%r12101, %r12099, %r85;
	mul.wide.u32 	%rd539, %r12101, 4;
	add.s64 	%rd540, %rd4, %rd539;
	st.global.u32 	[%rd540], %r29053;
	cvt.u64.u32 	%rd541, %r84;
	cvt.u64.u32 	%rd542, %r43;
	cvt.u64.u32 	%rd543, %r83;
	cvt.u64.u32 	%rd544, %r12097;
	add.s64 	%rd545, %rd544, %rd543;
	add.s64 	%rd546, %rd545, %rd542;
	add.s64 	%rd547, %rd546, %rd541;
	shl.b64 	%rd548, %rd547, 2;
	add.s64 	%rd549, %rd4, %rd548;
	st.global.u32 	[%rd549+387072], %r29052;
	cvt.u64.u32 	%rd550, %r85;
	add.s64 	%rd551, %rd546, %rd550;
	shl.b64 	%rd552, %rd551, 2;
	add.s64 	%rd553, %rd4, %rd552;
	st.global.u32 	[%rd553+387072], %r29051;
LBB0_101:                               // %L75524
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29049, 0;
	mov.u32 	%r29051, %r29049;
	mov.u32 	%r29052, %r29049;
	mov.u32 	%r29053, %r29049;
	mov.u32 	%r29054, %r29049;
LBB0_102:                               // %L75525
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r12103, 0;
	mov.u32 	%r29055, %r12103;
	mov.u32 	%r29056, %r12103;
	mov.u32 	%r29057, %r12103;
	mov.u32 	%r29058, %r12103;
	@%p50 bra 	LBB0_279;
	bra.uni 	LBB0_103;
LBB0_279:                               // %oksrem26368
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r12104, %r77, %r421;
	add.s32 	%r12105, %r12104, %r78;
	add.s32 	%r12106, %r12105, %r79;
	add.s32 	%r12107, %r12106, %r28851;
	add.s32 	%r12108, %r12107, %r80;
	add.s32 	%r12109, %r12108, %r81;
	add.s32 	%r12110, %r12109, %r82;
	mul.wide.s32 	%rd554, %r12110, 4;
	add.s64 	%rd556, %rd52, %rd554;
	ld.shared.u32 	%r29055, [%rd556];
	or.b32  	%r12111, %r422, %r77;
	add.s32 	%r12112, %r12111, %r78;
	add.s32 	%r12113, %r12112, %r79;
	add.s32 	%r12114, %r12113, %r28851;
	add.s32 	%r12115, %r12114, %r80;
	add.s32 	%r12116, %r12115, %r81;
	add.s32 	%r12117, %r12116, %r82;
	mul.wide.u32 	%rd557, %r12117, 4;
	add.s64 	%rd558, %rd52, %rd557;
	ld.shared.u32 	%r29056, [%rd558];
	or.b32  	%r12118, %r12104, 32;
	add.s32 	%r12119, %r12118, %r78;
	add.s32 	%r12120, %r12119, %r79;
	add.s32 	%r12121, %r12120, %r28851;
	add.s32 	%r12122, %r12121, %r80;
	add.s32 	%r12123, %r12122, %r81;
	add.s32 	%r12124, %r12123, %r82;
	mul.wide.s32 	%rd559, %r12124, 4;
	add.s64 	%rd560, %rd52, %rd559;
	ld.shared.u32 	%r29057, [%rd560];
	add.s32 	%r12125, %r421, 8288;
	or.b32  	%r12126, %r12125, %r77;
	add.s32 	%r12127, %r12126, %r78;
	add.s32 	%r12128, %r12127, %r79;
	add.s32 	%r12129, %r12128, %r28851;
	add.s32 	%r12130, %r12129, %r80;
	add.s32 	%r12131, %r12130, %r81;
	add.s32 	%r12132, %r12131, %r82;
	mul.wide.u32 	%rd561, %r12132, 4;
	add.s64 	%rd562, %rd52, %rd561;
	ld.shared.u32 	%r29058, [%rd562];
LBB0_103:                               // %L76644
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12165, %r12168}, {%r1202, %r1199}, {%r29055}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12174, %r12177}, {%r1202, %r1199}, {%r29056}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12183, %r12186}, {%r1202, %r1199}, {%r29057}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12192, %r12195}, {%r1202, %r1199}, {%r29058}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12161, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12163, %r12161, %r12165;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12166, %r1237, %r12168, %r12163;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12170, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12172, %r12170, %r12174;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12175, %r1237, %r12177, %r12172;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12179, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12181, %r12179, %r12183;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12184, %r1237, %r12186, %r12181;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12188, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12190, %r12188, %r12192;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12193, %r1237, %r12195, %r12190;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12197, %r1240, %r12168;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12200, %r1237, %r12165, %r12197;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12204, %r1240, %r12177;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12207, %r1237, %r12174, %r12204;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12211, %r1240, %r12186;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12214, %r1237, %r12183, %r12211;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12218, %r1240, %r12195;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12221, %r1237, %r12192, %r12218;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12266, %r12270}, {%r1270, %r1264, %r1267, %r1261}, {%r12200, %r12166}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12274, %r12278}, {%r1270, %r1264, %r1267, %r1261}, {%r12207, %r12175}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12282, %r12286}, {%r1270, %r1264, %r1267, %r1261}, {%r12214, %r12184}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12290, %r12294}, {%r1270, %r1264, %r1267, %r1261}, {%r12221, %r12193}, {%r12103, %r12103};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12265, %r12266, %r12266, %r29054;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29064, %r12270, %r12270, %r12265;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12273, %r12274, %r12274, %r29053;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29063, %r12278, %r12278, %r12273;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12281, %r12282, %r12282, %r29052;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29062, %r12286, %r12286, %r12281;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12289, %r12290, %r12290, %r29051;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29061, %r12294, %r12294, %r12289;
	// end inline asm
	add.s32 	%r29059, %r29049, 1;
	setp.ne.s32 	%p102, %r29059, 40;
	@%p102 bra 	LBB0_107;
// %bb.104:                             // %L76827
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_106;
// %bb.105:                             // %L76892
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r12297, %r28856, 193536;
	add.s32 	%r12298, %r83, %r12297;
	or.b32  	%r12299, %r12298, %r43;
	add.s32 	%r12300, %r12299, %r84;
	mul.wide.u32 	%rd563, %r12300, 4;
	add.s64 	%rd564, %rd4, %rd563;
	st.global.u32 	[%rd564], %r29064;
	add.s32 	%r12301, %r12299, %r85;
	mul.wide.u32 	%rd565, %r12301, 4;
	add.s64 	%rd566, %rd4, %rd565;
	st.global.u32 	[%rd566], %r29063;
	cvt.u64.u32 	%rd567, %r84;
	cvt.u64.u32 	%rd568, %r43;
	cvt.u64.u32 	%rd569, %r83;
	cvt.u64.u32 	%rd570, %r12297;
	add.s64 	%rd571, %rd570, %rd569;
	add.s64 	%rd572, %rd571, %rd568;
	add.s64 	%rd573, %rd572, %rd567;
	shl.b64 	%rd574, %rd573, 2;
	add.s64 	%rd575, %rd4, %rd574;
	st.global.u32 	[%rd575+387072], %r29062;
	cvt.u64.u32 	%rd576, %r85;
	add.s64 	%rd577, %rd572, %rd576;
	shl.b64 	%rd578, %rd577, 2;
	add.s64 	%rd579, %rd4, %rd578;
	st.global.u32 	[%rd579+387072], %r29061;
LBB0_106:                               // %L77405
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29059, 0;
	mov.u32 	%r29061, %r29059;
	mov.u32 	%r29062, %r29059;
	mov.u32 	%r29063, %r29059;
	mov.u32 	%r29064, %r29059;
LBB0_107:                               // %L77406
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r12303, 0;
	mov.u32 	%r29065, %r12303;
	mov.u32 	%r29066, %r12303;
	mov.u32 	%r29067, %r12303;
	mov.u32 	%r29068, %r12303;
	@%p50 bra 	LBB0_280;
	bra.uni 	LBB0_108;
LBB0_280:                               // %oksrem27066
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r12304, %r77, %r423;
	add.s32 	%r12305, %r12304, %r78;
	add.s32 	%r12306, %r12305, %r79;
	add.s32 	%r12307, %r12306, %r28851;
	add.s32 	%r12308, %r12307, %r80;
	add.s32 	%r12309, %r12308, %r81;
	add.s32 	%r12310, %r12309, %r82;
	mul.wide.s32 	%rd580, %r12310, 4;
	add.s64 	%rd582, %rd52, %rd580;
	ld.shared.u32 	%r29065, [%rd582];
	or.b32  	%r12311, %r424, %r77;
	add.s32 	%r12312, %r12311, %r78;
	add.s32 	%r12313, %r12312, %r79;
	add.s32 	%r12314, %r12313, %r28851;
	add.s32 	%r12315, %r12314, %r80;
	add.s32 	%r12316, %r12315, %r81;
	add.s32 	%r12317, %r12316, %r82;
	mul.wide.u32 	%rd583, %r12317, 4;
	add.s64 	%rd584, %rd52, %rd583;
	ld.shared.u32 	%r29066, [%rd584];
	or.b32  	%r12318, %r12304, 32;
	add.s32 	%r12319, %r12318, %r78;
	add.s32 	%r12320, %r12319, %r79;
	add.s32 	%r12321, %r12320, %r28851;
	add.s32 	%r12322, %r12321, %r80;
	add.s32 	%r12323, %r12322, %r81;
	add.s32 	%r12324, %r12323, %r82;
	mul.wide.s32 	%rd585, %r12324, 4;
	add.s64 	%rd586, %rd52, %rd585;
	ld.shared.u32 	%r29067, [%rd586];
	add.s32 	%r12325, %r423, 8288;
	or.b32  	%r12326, %r12325, %r77;
	add.s32 	%r12327, %r12326, %r78;
	add.s32 	%r12328, %r12327, %r79;
	add.s32 	%r12329, %r12328, %r28851;
	add.s32 	%r12330, %r12329, %r80;
	add.s32 	%r12331, %r12330, %r81;
	add.s32 	%r12332, %r12331, %r82;
	mul.wide.u32 	%rd587, %r12332, 4;
	add.s64 	%rd588, %rd52, %rd587;
	ld.shared.u32 	%r29068, [%rd588];
LBB0_108:                               // %L78525
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12365, %r12368}, {%r1202, %r1199}, {%r29065}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12374, %r12377}, {%r1202, %r1199}, {%r29066}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12383, %r12386}, {%r1202, %r1199}, {%r29067}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12392, %r12395}, {%r1202, %r1199}, {%r29068}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12361, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12363, %r12361, %r12365;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12366, %r1237, %r12368, %r12363;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12370, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12372, %r12370, %r12374;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12375, %r1237, %r12377, %r12372;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12379, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12381, %r12379, %r12383;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12384, %r1237, %r12386, %r12381;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12388, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12390, %r12388, %r12392;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12393, %r1237, %r12395, %r12390;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12397, %r1240, %r12368;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12400, %r1237, %r12365, %r12397;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12404, %r1240, %r12377;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12407, %r1237, %r12374, %r12404;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12411, %r1240, %r12386;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12414, %r1237, %r12383, %r12411;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12418, %r1240, %r12395;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12421, %r1237, %r12392, %r12418;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12466, %r12470}, {%r1270, %r1264, %r1267, %r1261}, {%r12400, %r12366}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12474, %r12478}, {%r1270, %r1264, %r1267, %r1261}, {%r12407, %r12375}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12482, %r12486}, {%r1270, %r1264, %r1267, %r1261}, {%r12414, %r12384}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12490, %r12494}, {%r1270, %r1264, %r1267, %r1261}, {%r12421, %r12393}, {%r12303, %r12303};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12465, %r12466, %r12466, %r29064;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29074, %r12470, %r12470, %r12465;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12473, %r12474, %r12474, %r29063;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29073, %r12478, %r12478, %r12473;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12481, %r12482, %r12482, %r29062;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29072, %r12486, %r12486, %r12481;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12489, %r12490, %r12490, %r29061;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29071, %r12494, %r12494, %r12489;
	// end inline asm
	add.s32 	%r29069, %r29059, 1;
	setp.ne.s32 	%p105, %r29069, 40;
	@%p105 bra 	LBB0_112;
// %bb.109:                             // %L78708
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_111;
// %bb.110:                             // %L78773
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r12497, %r28856, 193536;
	add.s32 	%r12498, %r83, %r12497;
	or.b32  	%r12499, %r12498, %r43;
	add.s32 	%r12500, %r12499, %r84;
	mul.wide.u32 	%rd589, %r12500, 4;
	add.s64 	%rd590, %rd4, %rd589;
	st.global.u32 	[%rd590], %r29074;
	add.s32 	%r12501, %r12499, %r85;
	mul.wide.u32 	%rd591, %r12501, 4;
	add.s64 	%rd592, %rd4, %rd591;
	st.global.u32 	[%rd592], %r29073;
	cvt.u64.u32 	%rd593, %r84;
	cvt.u64.u32 	%rd594, %r43;
	cvt.u64.u32 	%rd595, %r83;
	cvt.u64.u32 	%rd596, %r12497;
	add.s64 	%rd597, %rd596, %rd595;
	add.s64 	%rd598, %rd597, %rd594;
	add.s64 	%rd599, %rd598, %rd593;
	shl.b64 	%rd600, %rd599, 2;
	add.s64 	%rd601, %rd4, %rd600;
	st.global.u32 	[%rd601+387072], %r29072;
	cvt.u64.u32 	%rd602, %r85;
	add.s64 	%rd603, %rd598, %rd602;
	shl.b64 	%rd604, %rd603, 2;
	add.s64 	%rd605, %rd4, %rd604;
	st.global.u32 	[%rd605+387072], %r29071;
LBB0_111:                               // %L79286
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29069, 0;
	mov.u32 	%r29071, %r29069;
	mov.u32 	%r29072, %r29069;
	mov.u32 	%r29073, %r29069;
	mov.u32 	%r29074, %r29069;
LBB0_112:                               // %L79287
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r12503, 0;
	mov.u32 	%r29075, %r12503;
	mov.u32 	%r29076, %r12503;
	mov.u32 	%r29077, %r12503;
	mov.u32 	%r29078, %r12503;
	@%p50 bra 	LBB0_281;
	bra.uni 	LBB0_113;
LBB0_281:                               // %oksrem27764
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r12504, %r77, %r425;
	add.s32 	%r12505, %r12504, %r78;
	add.s32 	%r12506, %r12505, %r79;
	add.s32 	%r12507, %r12506, %r28851;
	add.s32 	%r12508, %r12507, %r80;
	add.s32 	%r12509, %r12508, %r81;
	add.s32 	%r12510, %r12509, %r82;
	mul.wide.s32 	%rd606, %r12510, 4;
	add.s64 	%rd608, %rd52, %rd606;
	ld.shared.u32 	%r29075, [%rd608];
	or.b32  	%r12511, %r426, %r77;
	add.s32 	%r12512, %r12511, %r78;
	add.s32 	%r12513, %r12512, %r79;
	add.s32 	%r12514, %r12513, %r28851;
	add.s32 	%r12515, %r12514, %r80;
	add.s32 	%r12516, %r12515, %r81;
	add.s32 	%r12517, %r12516, %r82;
	mul.wide.u32 	%rd609, %r12517, 4;
	add.s64 	%rd610, %rd52, %rd609;
	ld.shared.u32 	%r29076, [%rd610];
	or.b32  	%r12518, %r12504, 32;
	add.s32 	%r12519, %r12518, %r78;
	add.s32 	%r12520, %r12519, %r79;
	add.s32 	%r12521, %r12520, %r28851;
	add.s32 	%r12522, %r12521, %r80;
	add.s32 	%r12523, %r12522, %r81;
	add.s32 	%r12524, %r12523, %r82;
	mul.wide.s32 	%rd611, %r12524, 4;
	add.s64 	%rd612, %rd52, %rd611;
	ld.shared.u32 	%r29077, [%rd612];
	add.s32 	%r12525, %r425, 8288;
	or.b32  	%r12526, %r12525, %r77;
	add.s32 	%r12527, %r12526, %r78;
	add.s32 	%r12528, %r12527, %r79;
	add.s32 	%r12529, %r12528, %r28851;
	add.s32 	%r12530, %r12529, %r80;
	add.s32 	%r12531, %r12530, %r81;
	add.s32 	%r12532, %r12531, %r82;
	mul.wide.u32 	%rd613, %r12532, 4;
	add.s64 	%rd614, %rd52, %rd613;
	ld.shared.u32 	%r29078, [%rd614];
LBB0_113:                               // %L80406
                                        //   in Loop: Header=BB0_16 Depth=1
	xor.b32  	%r157, %r28881, -2004318072;
	xor.b32  	%r159, %r28882, -2004318072;
	xor.b32  	%r161, %r28883, -2004318072;
	xor.b32  	%r163, %r28884, -2004318072;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12565, %r12568}, {%r1202, %r1199}, {%r29075}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12574, %r12577}, {%r1202, %r1199}, {%r29076}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12583, %r12586}, {%r1202, %r1199}, {%r29077}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r12592, %r12595}, {%r1202, %r1199}, {%r29078}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12561, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12563, %r12561, %r12565;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12566, %r1237, %r12568, %r12563;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12570, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12572, %r12570, %r12574;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12575, %r1237, %r12577, %r12572;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12579, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12581, %r12579, %r12583;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12584, %r1237, %r12586, %r12581;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r12588, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12590, %r12588, %r12592;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12593, %r1237, %r12595, %r12590;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12597, %r1240, %r12568;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12600, %r1237, %r12565, %r12597;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12604, %r1240, %r12577;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12607, %r1237, %r12574, %r12604;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12611, %r1240, %r12586;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12614, %r1237, %r12583, %r12611;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r12618, %r1240, %r12595;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12621, %r1237, %r12592, %r12618;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12666, %r12670}, {%r1270, %r1264, %r1267, %r1261}, {%r12600, %r12566}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12674, %r12678}, {%r1270, %r1264, %r1267, %r1261}, {%r12607, %r12575}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12682, %r12686}, {%r1270, %r1264, %r1267, %r1261}, {%r12614, %r12584}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r12690, %r12694}, {%r1270, %r1264, %r1267, %r1261}, {%r12621, %r12593}, {%r12503, %r12503};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12665, %r12666, %r12666, %r29074;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29084, %r12670, %r12670, %r12665;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12673, %r12674, %r12674, %r29073;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29083, %r12678, %r12678, %r12673;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12681, %r12682, %r12682, %r29072;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29082, %r12686, %r12686, %r12681;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r12689, %r12690, %r12690, %r29071;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29081, %r12694, %r12694, %r12689;
	// end inline asm
	add.s32 	%r29079, %r29069, 1;
	setp.ne.s32 	%p108, %r29079, 40;
	@%p108 bra 	LBB0_117;
// %bb.114:                             // %L80589
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_116;
// %bb.115:                             // %L80654
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r12697, %r28856, 193536;
	add.s32 	%r12698, %r83, %r12697;
	or.b32  	%r12699, %r12698, %r43;
	add.s32 	%r12700, %r12699, %r84;
	mul.wide.u32 	%rd615, %r12700, 4;
	add.s64 	%rd616, %rd4, %rd615;
	st.global.u32 	[%rd616], %r29084;
	add.s32 	%r12701, %r12699, %r85;
	mul.wide.u32 	%rd617, %r12701, 4;
	add.s64 	%rd618, %rd4, %rd617;
	st.global.u32 	[%rd618], %r29083;
	cvt.u64.u32 	%rd619, %r84;
	cvt.u64.u32 	%rd620, %r43;
	cvt.u64.u32 	%rd621, %r83;
	cvt.u64.u32 	%rd622, %r12697;
	add.s64 	%rd623, %rd622, %rd621;
	add.s64 	%rd624, %rd623, %rd620;
	add.s64 	%rd625, %rd624, %rd619;
	shl.b64 	%rd626, %rd625, 2;
	add.s64 	%rd627, %rd4, %rd626;
	st.global.u32 	[%rd627+387072], %r29082;
	cvt.u64.u32 	%rd628, %r85;
	add.s64 	%rd629, %rd624, %rd628;
	shl.b64 	%rd630, %rd629, 2;
	add.s64 	%rd631, %rd4, %rd630;
	st.global.u32 	[%rd631+387072], %r29081;
LBB0_116:                               // %L81167
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29079, 0;
	mov.u32 	%r29081, %r29079;
	mov.u32 	%r29082, %r29079;
	mov.u32 	%r29083, %r29079;
	mov.u32 	%r29084, %r29079;
LBB0_117:                               // %L81168
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r13628, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r13639, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r13625, %r1979, %r157, %r13628, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13629, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13630, %r13628, %r13629;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13633, %r13625, %r13630;
	// end inline asm
	// begin inline asm
	lop3.b32 %r13636, %r1990, %r157, %r13639, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13640, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13641, %r13639, %r13640;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13644, %r13636, %r13641;
	// end inline asm
	// begin inline asm
	mov.b32 %r13674, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r13685, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r13671, %r1979, %r159, %r13674, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13675, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13676, %r13674, %r13675;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13679, %r13671, %r13676;
	// end inline asm
	// begin inline asm
	lop3.b32 %r13682, %r1990, %r159, %r13685, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13686, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13687, %r13685, %r13686;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13690, %r13682, %r13687;
	// end inline asm
	// begin inline asm
	mov.b32 %r13720, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r13731, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r13717, %r1979, %r161, %r13720, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13721, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13722, %r13720, %r13721;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13725, %r13717, %r13722;
	// end inline asm
	// begin inline asm
	lop3.b32 %r13728, %r1990, %r161, %r13731, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13732, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13733, %r13731, %r13732;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13736, %r13728, %r13733;
	// end inline asm
	// begin inline asm
	mov.b32 %r13766, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r13777, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r13763, %r1979, %r163, %r13766, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13767, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13768, %r13766, %r13767;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13771, %r13763, %r13768;
	// end inline asm
	// begin inline asm
	lop3.b32 %r13774, %r1990, %r163, %r13777, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r13778, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r13779, %r13777, %r13778;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r13782, %r13774, %r13779;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r13633;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13807, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r13644;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13810, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r13679;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13813, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r13690;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13816, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r13725;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13819, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r13736;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13822, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r13771;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13825, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r13782;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r13828, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r14159, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13891, %r13894}, {%r1202, %r1199}, {%r13807}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13900, %r13903}, {%r1202, %r1199}, {%r13810}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13909, %r13912}, {%r1202, %r1199}, {%r13813}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13918, %r13921}, {%r1202, %r1199}, {%r13816}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13927, %r13930}, {%r1202, %r1199}, {%r13819}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13936, %r13939}, {%r1202, %r1199}, {%r13822}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13945, %r13948}, {%r1202, %r1199}, {%r13825}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r13954, %r13957}, {%r1202, %r1199}, {%r13828}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13887, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13889, %r13887, %r13891;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13892, %r1237, %r13894, %r13889;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13896, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13898, %r13896, %r13900;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13901, %r1237, %r13903, %r13898;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13905, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13907, %r13905, %r13909;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13910, %r1237, %r13912, %r13907;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13914, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13916, %r13914, %r13918;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13919, %r1237, %r13921, %r13916;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13923, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13925, %r13923, %r13927;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13928, %r1237, %r13930, %r13925;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13932, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13934, %r13932, %r13936;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13937, %r1237, %r13939, %r13934;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13941, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13943, %r13941, %r13945;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13946, %r1237, %r13948, %r13943;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r13950, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13952, %r13950, %r13954;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13955, %r1237, %r13957, %r13952;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13959, %r1240, %r13894;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13962, %r1237, %r13891, %r13959;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13966, %r1240, %r13903;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13969, %r1237, %r13900, %r13966;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13973, %r1240, %r13912;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13976, %r1237, %r13909, %r13973;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13980, %r1240, %r13921;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13983, %r1237, %r13918, %r13980;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13987, %r1240, %r13930;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13990, %r1237, %r13927, %r13987;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r13994, %r1240, %r13939;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r13997, %r1237, %r13936, %r13994;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14001, %r1240, %r13948;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14004, %r1237, %r13945, %r14001;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14008, %r1240, %r13957;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14011, %r1237, %r13954, %r14008;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14096, %r14097}, {%r1270, %r1264, %r1267, %r1261}, {%r13962, %r13892}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14104, %r14105}, {%r1270, %r1264, %r1267, %r1261}, {%r13969, %r13901}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14112, %r14113}, {%r1270, %r1264, %r1267, %r1261}, {%r13976, %r13910}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14120, %r14121}, {%r1270, %r1264, %r1267, %r1261}, {%r13983, %r13919}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14128, %r14129}, {%r1270, %r1264, %r1267, %r1261}, {%r13990, %r13928}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14136, %r14137}, {%r1270, %r1264, %r1267, %r1261}, {%r13997, %r13937}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14144, %r14145}, {%r1270, %r1264, %r1267, %r1261}, {%r14004, %r13946}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14152, %r14153}, {%r1270, %r1264, %r1267, %r1261}, {%r14011, %r13955}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	prmt.b32 %r14095, %r14096, %r14097, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14099, %r14096, %r14097, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14103, %r14104, %r14105, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14107, %r14104, %r14105, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14111, %r14112, %r14113, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14115, %r14112, %r14113, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14119, %r14120, %r14121, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14123, %r14120, %r14121, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14127, %r14128, %r14129, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14131, %r14128, %r14129, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14135, %r14136, %r14137, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14139, %r14136, %r14137, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14143, %r14144, %r14145, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14147, %r14144, %r14145, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14151, %r14152, %r14153, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r14155, %r14152, %r14153, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r14095;
	st.shared.u32 	[%rd7+33024], %r14099;
	st.shared.u32 	[%rd8], %r14103;
	st.shared.u32 	[%rd9], %r14107;
	shr.s16 	%rs1926, %rs3782, 15;
	shr.u16 	%rs1927, %rs1926, 14;
	add.s16 	%rs1928, %rs3782, %rs1927;
	and.b16  	%rs1929, %rs1928, -4;
	sub.s16 	%rs1930, %rs3782, %rs1929;
	mul.wide.s16 	%r491, %rs1930, 64;
	add.s32 	%r14160, %r71, %r491;
	add.s32 	%r14161, %r14160, %r28848;
	add.s32 	%r14162, %r14161, %r72;
	add.s32 	%r14163, %r14162, %r28849;
	add.s32 	%r14164, %r14163, %r73;
	add.s32 	%r14165, %r14164, %r74;
	add.s32 	%r14166, %r14165, %r75;
	mul.wide.s32 	%rd632, %r14166, 4;
	add.s64 	%rd634, %rd52, %rd632;
	st.shared.u32 	[%rd634], %r14111;
	add.s32 	%r492, %r491, 8256;
	add.s32 	%r14167, %r492, %r71;
	add.s32 	%r14168, %r14167, %r28848;
	add.s32 	%r14169, %r14168, %r72;
	add.s32 	%r14170, %r14169, %r28849;
	add.s32 	%r14171, %r14170, %r73;
	add.s32 	%r14172, %r14171, %r74;
	add.s32 	%r14173, %r14172, %r75;
	mul.wide.u32 	%rd635, %r14173, 4;
	add.s64 	%rd636, %rd52, %rd635;
	st.shared.u32 	[%rd636], %r14115;
	add.s32 	%r14174, %r14160, %r76;
	add.s32 	%r14175, %r14174, %r72;
	add.s32 	%r14176, %r14175, %r28849;
	add.s32 	%r14177, %r14176, %r73;
	add.s32 	%r14178, %r14177, %r74;
	add.s32 	%r14179, %r14178, %r75;
	mul.wide.s32 	%rd637, %r14179, 4;
	add.s64 	%rd638, %rd52, %rd637;
	st.shared.u32 	[%rd638], %r14119;
	add.s32 	%r14180, %r14167, %r76;
	add.s32 	%r14181, %r14180, %r72;
	add.s32 	%r14182, %r14181, %r28849;
	add.s32 	%r14183, %r14182, %r73;
	add.s32 	%r14184, %r14183, %r74;
	add.s32 	%r14185, %r14184, %r75;
	mul.wide.s32 	%rd639, %r14185, 4;
	add.s64 	%rd640, %rd52, %rd639;
	st.shared.u32 	[%rd640], %r14123;
	shr.s16 	%rs1932, %rs3783, 15;
	shr.u16 	%rs1933, %rs1932, 14;
	add.s16 	%rs1934, %rs3783, %rs1933;
	and.b16  	%rs1935, %rs1934, -4;
	sub.s16 	%rs1936, %rs3783, %rs1935;
	mul.wide.s16 	%r493, %rs1936, 64;
	add.s32 	%r14186, %r71, %r493;
	add.s32 	%r14187, %r14186, %r28848;
	add.s32 	%r14188, %r14187, %r72;
	add.s32 	%r14189, %r14188, %r28849;
	add.s32 	%r14190, %r14189, %r73;
	add.s32 	%r14191, %r14190, %r74;
	add.s32 	%r14192, %r14191, %r75;
	mul.wide.s32 	%rd641, %r14192, 4;
	add.s64 	%rd642, %rd52, %rd641;
	st.shared.u32 	[%rd642], %r14127;
	add.s32 	%r494, %r493, 8256;
	add.s32 	%r14193, %r494, %r71;
	add.s32 	%r14194, %r14193, %r28848;
	add.s32 	%r14195, %r14194, %r72;
	add.s32 	%r14196, %r14195, %r28849;
	add.s32 	%r14197, %r14196, %r73;
	add.s32 	%r14198, %r14197, %r74;
	add.s32 	%r14199, %r14198, %r75;
	mul.wide.u32 	%rd643, %r14199, 4;
	add.s64 	%rd644, %rd52, %rd643;
	st.shared.u32 	[%rd644], %r14131;
	add.s32 	%r14200, %r14186, %r76;
	add.s32 	%r14201, %r14200, %r72;
	add.s32 	%r14202, %r14201, %r28849;
	add.s32 	%r14203, %r14202, %r73;
	add.s32 	%r14204, %r14203, %r74;
	add.s32 	%r14205, %r14204, %r75;
	mul.wide.s32 	%rd645, %r14205, 4;
	add.s64 	%rd646, %rd52, %rd645;
	st.shared.u32 	[%rd646], %r14135;
	add.s32 	%r14206, %r14193, %r76;
	add.s32 	%r14207, %r14206, %r72;
	add.s32 	%r14208, %r14207, %r28849;
	add.s32 	%r14209, %r14208, %r73;
	add.s32 	%r14210, %r14209, %r74;
	add.s32 	%r14211, %r14210, %r75;
	mul.wide.s32 	%rd647, %r14211, 4;
	add.s64 	%rd648, %rd52, %rd647;
	st.shared.u32 	[%rd648], %r14139;
	shr.s16 	%rs1937, %rs3785, 15;
	shr.u16 	%rs1938, %rs1937, 14;
	add.s16 	%rs1939, %rs3785, %rs1938;
	and.b16  	%rs1940, %rs1939, -4;
	sub.s16 	%rs1941, %rs3785, %rs1940;
	mul.wide.s16 	%r495, %rs1941, 64;
	add.s32 	%r14212, %r71, %r495;
	add.s32 	%r14213, %r14212, %r28848;
	add.s32 	%r14214, %r14213, %r72;
	add.s32 	%r14215, %r14214, %r28849;
	add.s32 	%r14216, %r14215, %r73;
	add.s32 	%r14217, %r14216, %r74;
	add.s32 	%r14218, %r14217, %r75;
	mul.wide.s32 	%rd649, %r14218, 4;
	add.s64 	%rd650, %rd52, %rd649;
	st.shared.u32 	[%rd650], %r14143;
	add.s32 	%r496, %r495, 8256;
	add.s32 	%r14219, %r496, %r71;
	add.s32 	%r14220, %r14219, %r28848;
	add.s32 	%r14221, %r14220, %r72;
	add.s32 	%r14222, %r14221, %r28849;
	add.s32 	%r14223, %r14222, %r73;
	add.s32 	%r14224, %r14223, %r74;
	add.s32 	%r14225, %r14224, %r75;
	mul.wide.u32 	%rd651, %r14225, 4;
	add.s64 	%rd652, %rd52, %rd651;
	st.shared.u32 	[%rd652], %r14147;
	add.s32 	%r14226, %r14212, %r76;
	add.s32 	%r14227, %r14226, %r72;
	add.s32 	%r14228, %r14227, %r28849;
	add.s32 	%r14229, %r14228, %r73;
	add.s32 	%r14230, %r14229, %r74;
	add.s32 	%r14231, %r14230, %r75;
	mul.wide.s32 	%rd653, %r14231, 4;
	add.s64 	%rd654, %rd52, %rd653;
	st.shared.u32 	[%rd654], %r14151;
	add.s32 	%r14232, %r14219, %r76;
	add.s32 	%r14233, %r14232, %r72;
	add.s32 	%r14234, %r14233, %r28849;
	add.s32 	%r14235, %r14234, %r73;
	add.s32 	%r14236, %r14235, %r74;
	add.s32 	%r14237, %r14236, %r75;
	mul.wide.s32 	%rd655, %r14237, 4;
	add.s64 	%rd656, %rd52, %rd655;
	st.shared.u32 	[%rd656], %r14155;
	bar.sync 	0;
	mov.u32 	%r29085, %r14159;
	mov.u32 	%r29086, %r14159;
	mov.u32 	%r29087, %r14159;
	mov.u32 	%r29088, %r14159;
	@%p50 bra 	LBB0_282;
	bra.uni 	LBB0_118;
LBB0_282:                               // %oksrem30366
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29085, [%rd10];
	ld.shared.u32 	%r29086, [%rd11+33024];
	ld.shared.u32 	%r29087, [%rd11+128];
	ld.shared.u32 	%r29088, [%rd11+33152];
LBB0_118:                               // %L88437
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14270, %r14273}, {%r1202, %r1199}, {%r29085}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14279, %r14282}, {%r1202, %r1199}, {%r29086}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14288, %r14291}, {%r1202, %r1199}, {%r29087}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14297, %r14300}, {%r1202, %r1199}, {%r29088}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14266, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14268, %r14266, %r14270;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14271, %r1237, %r14273, %r14268;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14275, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14277, %r14275, %r14279;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14280, %r1237, %r14282, %r14277;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14284, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14286, %r14284, %r14288;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14289, %r1237, %r14291, %r14286;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14293, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14295, %r14293, %r14297;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14298, %r1237, %r14300, %r14295;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14302, %r1240, %r14273;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14305, %r1237, %r14270, %r14302;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14309, %r1240, %r14282;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14312, %r1237, %r14279, %r14309;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14316, %r1240, %r14291;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14319, %r1237, %r14288, %r14316;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14323, %r1240, %r14300;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14326, %r1237, %r14297, %r14323;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14371, %r14375}, {%r1270, %r1264, %r1267, %r1261}, {%r14305, %r14271}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14379, %r14383}, {%r1270, %r1264, %r1267, %r1261}, {%r14312, %r14280}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14387, %r14391}, {%r1270, %r1264, %r1267, %r1261}, {%r14319, %r14289}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14395, %r14399}, {%r1270, %r1264, %r1267, %r1261}, {%r14326, %r14298}, {%r14159, %r14159};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14370, %r14371, %r14371, %r29084;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29094, %r14375, %r14375, %r14370;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14378, %r14379, %r14379, %r29083;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29093, %r14383, %r14383, %r14378;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14386, %r14387, %r14387, %r29082;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29092, %r14391, %r14391, %r14386;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14394, %r14395, %r14395, %r29081;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29091, %r14399, %r14399, %r14394;
	// end inline asm
	add.s32 	%r29089, %r29079, 1;
	setp.ne.s32 	%p111, %r29089, 40;
	@%p111 bra 	LBB0_122;
// %bb.119:                             // %L88620
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_121;
// %bb.120:                             // %L88685
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r14402, %r28856, 193536;
	add.s32 	%r14403, %r83, %r14402;
	or.b32  	%r14404, %r14403, %r43;
	add.s32 	%r14405, %r14404, %r84;
	mul.wide.u32 	%rd657, %r14405, 4;
	add.s64 	%rd658, %rd4, %rd657;
	st.global.u32 	[%rd658], %r29094;
	add.s32 	%r14406, %r14404, %r85;
	mul.wide.u32 	%rd659, %r14406, 4;
	add.s64 	%rd660, %rd4, %rd659;
	st.global.u32 	[%rd660], %r29093;
	cvt.u64.u32 	%rd661, %r84;
	cvt.u64.u32 	%rd662, %r43;
	cvt.u64.u32 	%rd663, %r83;
	cvt.u64.u32 	%rd664, %r14402;
	add.s64 	%rd665, %rd664, %rd663;
	add.s64 	%rd666, %rd665, %rd662;
	add.s64 	%rd667, %rd666, %rd661;
	shl.b64 	%rd668, %rd667, 2;
	add.s64 	%rd669, %rd4, %rd668;
	st.global.u32 	[%rd669+387072], %r29092;
	cvt.u64.u32 	%rd670, %r85;
	add.s64 	%rd671, %rd666, %rd670;
	shl.b64 	%rd672, %rd671, 2;
	add.s64 	%rd673, %rd4, %rd672;
	st.global.u32 	[%rd673+387072], %r29091;
LBB0_121:                               // %L89198
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29089, 0;
	mov.u32 	%r29091, %r29089;
	mov.u32 	%r29092, %r29089;
	mov.u32 	%r29093, %r29089;
	mov.u32 	%r29094, %r29089;
LBB0_122:                               // %L89199
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r14408, 0;
	mov.u32 	%r29095, %r14408;
	mov.u32 	%r29096, %r14408;
	mov.u32 	%r29097, %r14408;
	mov.u32 	%r29098, %r14408;
	@%p50 bra 	LBB0_283;
	bra.uni 	LBB0_123;
LBB0_283:                               // %oksrem31064
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r14409, %r77, %r491;
	add.s32 	%r14410, %r14409, %r78;
	add.s32 	%r14411, %r14410, %r79;
	add.s32 	%r14412, %r14411, %r28851;
	add.s32 	%r14413, %r14412, %r80;
	add.s32 	%r14414, %r14413, %r81;
	add.s32 	%r14415, %r14414, %r82;
	mul.wide.s32 	%rd674, %r14415, 4;
	add.s64 	%rd676, %rd52, %rd674;
	ld.shared.u32 	%r29095, [%rd676];
	or.b32  	%r14416, %r492, %r77;
	add.s32 	%r14417, %r14416, %r78;
	add.s32 	%r14418, %r14417, %r79;
	add.s32 	%r14419, %r14418, %r28851;
	add.s32 	%r14420, %r14419, %r80;
	add.s32 	%r14421, %r14420, %r81;
	add.s32 	%r14422, %r14421, %r82;
	mul.wide.u32 	%rd677, %r14422, 4;
	add.s64 	%rd678, %rd52, %rd677;
	ld.shared.u32 	%r29096, [%rd678];
	or.b32  	%r14423, %r14409, 32;
	add.s32 	%r14424, %r14423, %r78;
	add.s32 	%r14425, %r14424, %r79;
	add.s32 	%r14426, %r14425, %r28851;
	add.s32 	%r14427, %r14426, %r80;
	add.s32 	%r14428, %r14427, %r81;
	add.s32 	%r14429, %r14428, %r82;
	mul.wide.s32 	%rd679, %r14429, 4;
	add.s64 	%rd680, %rd52, %rd679;
	ld.shared.u32 	%r29097, [%rd680];
	add.s32 	%r14430, %r491, 8288;
	or.b32  	%r14431, %r14430, %r77;
	add.s32 	%r14432, %r14431, %r78;
	add.s32 	%r14433, %r14432, %r79;
	add.s32 	%r14434, %r14433, %r28851;
	add.s32 	%r14435, %r14434, %r80;
	add.s32 	%r14436, %r14435, %r81;
	add.s32 	%r14437, %r14436, %r82;
	mul.wide.u32 	%rd681, %r14437, 4;
	add.s64 	%rd682, %rd52, %rd681;
	ld.shared.u32 	%r29098, [%rd682];
LBB0_123:                               // %L90318
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14470, %r14473}, {%r1202, %r1199}, {%r29095}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14479, %r14482}, {%r1202, %r1199}, {%r29096}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14488, %r14491}, {%r1202, %r1199}, {%r29097}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14497, %r14500}, {%r1202, %r1199}, {%r29098}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14466, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14468, %r14466, %r14470;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14471, %r1237, %r14473, %r14468;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14475, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14477, %r14475, %r14479;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14480, %r1237, %r14482, %r14477;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14484, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14486, %r14484, %r14488;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14489, %r1237, %r14491, %r14486;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14493, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14495, %r14493, %r14497;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14498, %r1237, %r14500, %r14495;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14502, %r1240, %r14473;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14505, %r1237, %r14470, %r14502;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14509, %r1240, %r14482;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14512, %r1237, %r14479, %r14509;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14516, %r1240, %r14491;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14519, %r1237, %r14488, %r14516;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14523, %r1240, %r14500;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14526, %r1237, %r14497, %r14523;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14571, %r14575}, {%r1270, %r1264, %r1267, %r1261}, {%r14505, %r14471}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14579, %r14583}, {%r1270, %r1264, %r1267, %r1261}, {%r14512, %r14480}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14587, %r14591}, {%r1270, %r1264, %r1267, %r1261}, {%r14519, %r14489}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14595, %r14599}, {%r1270, %r1264, %r1267, %r1261}, {%r14526, %r14498}, {%r14408, %r14408};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14570, %r14571, %r14571, %r29094;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29104, %r14575, %r14575, %r14570;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14578, %r14579, %r14579, %r29093;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29103, %r14583, %r14583, %r14578;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14586, %r14587, %r14587, %r29092;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29102, %r14591, %r14591, %r14586;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14594, %r14595, %r14595, %r29091;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29101, %r14599, %r14599, %r14594;
	// end inline asm
	add.s32 	%r29099, %r29089, 1;
	setp.ne.s32 	%p114, %r29099, 40;
	@%p114 bra 	LBB0_127;
// %bb.124:                             // %L90501
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_126;
// %bb.125:                             // %L90566
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r14602, %r28856, 193536;
	add.s32 	%r14603, %r83, %r14602;
	or.b32  	%r14604, %r14603, %r43;
	add.s32 	%r14605, %r14604, %r84;
	mul.wide.u32 	%rd683, %r14605, 4;
	add.s64 	%rd684, %rd4, %rd683;
	st.global.u32 	[%rd684], %r29104;
	add.s32 	%r14606, %r14604, %r85;
	mul.wide.u32 	%rd685, %r14606, 4;
	add.s64 	%rd686, %rd4, %rd685;
	st.global.u32 	[%rd686], %r29103;
	cvt.u64.u32 	%rd687, %r84;
	cvt.u64.u32 	%rd688, %r43;
	cvt.u64.u32 	%rd689, %r83;
	cvt.u64.u32 	%rd690, %r14602;
	add.s64 	%rd691, %rd690, %rd689;
	add.s64 	%rd692, %rd691, %rd688;
	add.s64 	%rd693, %rd692, %rd687;
	shl.b64 	%rd694, %rd693, 2;
	add.s64 	%rd695, %rd4, %rd694;
	st.global.u32 	[%rd695+387072], %r29102;
	cvt.u64.u32 	%rd696, %r85;
	add.s64 	%rd697, %rd692, %rd696;
	shl.b64 	%rd698, %rd697, 2;
	add.s64 	%rd699, %rd4, %rd698;
	st.global.u32 	[%rd699+387072], %r29101;
LBB0_126:                               // %L91079
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29099, 0;
	mov.u32 	%r29101, %r29099;
	mov.u32 	%r29102, %r29099;
	mov.u32 	%r29103, %r29099;
	mov.u32 	%r29104, %r29099;
LBB0_127:                               // %L91080
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r14608, 0;
	mov.u32 	%r29105, %r14608;
	mov.u32 	%r29106, %r14608;
	mov.u32 	%r29107, %r14608;
	mov.u32 	%r29108, %r14608;
	@%p50 bra 	LBB0_284;
	bra.uni 	LBB0_128;
LBB0_284:                               // %oksrem31762
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r14609, %r77, %r493;
	add.s32 	%r14610, %r14609, %r78;
	add.s32 	%r14611, %r14610, %r79;
	add.s32 	%r14612, %r14611, %r28851;
	add.s32 	%r14613, %r14612, %r80;
	add.s32 	%r14614, %r14613, %r81;
	add.s32 	%r14615, %r14614, %r82;
	mul.wide.s32 	%rd700, %r14615, 4;
	add.s64 	%rd702, %rd52, %rd700;
	ld.shared.u32 	%r29105, [%rd702];
	or.b32  	%r14616, %r494, %r77;
	add.s32 	%r14617, %r14616, %r78;
	add.s32 	%r14618, %r14617, %r79;
	add.s32 	%r14619, %r14618, %r28851;
	add.s32 	%r14620, %r14619, %r80;
	add.s32 	%r14621, %r14620, %r81;
	add.s32 	%r14622, %r14621, %r82;
	mul.wide.u32 	%rd703, %r14622, 4;
	add.s64 	%rd704, %rd52, %rd703;
	ld.shared.u32 	%r29106, [%rd704];
	or.b32  	%r14623, %r14609, 32;
	add.s32 	%r14624, %r14623, %r78;
	add.s32 	%r14625, %r14624, %r79;
	add.s32 	%r14626, %r14625, %r28851;
	add.s32 	%r14627, %r14626, %r80;
	add.s32 	%r14628, %r14627, %r81;
	add.s32 	%r14629, %r14628, %r82;
	mul.wide.s32 	%rd705, %r14629, 4;
	add.s64 	%rd706, %rd52, %rd705;
	ld.shared.u32 	%r29107, [%rd706];
	add.s32 	%r14630, %r493, 8288;
	or.b32  	%r14631, %r14630, %r77;
	add.s32 	%r14632, %r14631, %r78;
	add.s32 	%r14633, %r14632, %r79;
	add.s32 	%r14634, %r14633, %r28851;
	add.s32 	%r14635, %r14634, %r80;
	add.s32 	%r14636, %r14635, %r81;
	add.s32 	%r14637, %r14636, %r82;
	mul.wide.u32 	%rd707, %r14637, 4;
	add.s64 	%rd708, %rd52, %rd707;
	ld.shared.u32 	%r29108, [%rd708];
LBB0_128:                               // %L92199
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14670, %r14673}, {%r1202, %r1199}, {%r29105}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14679, %r14682}, {%r1202, %r1199}, {%r29106}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14688, %r14691}, {%r1202, %r1199}, {%r29107}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14697, %r14700}, {%r1202, %r1199}, {%r29108}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14666, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14668, %r14666, %r14670;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14671, %r1237, %r14673, %r14668;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14675, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14677, %r14675, %r14679;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14680, %r1237, %r14682, %r14677;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14684, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14686, %r14684, %r14688;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14689, %r1237, %r14691, %r14686;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14693, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14695, %r14693, %r14697;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14698, %r1237, %r14700, %r14695;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14702, %r1240, %r14673;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14705, %r1237, %r14670, %r14702;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14709, %r1240, %r14682;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14712, %r1237, %r14679, %r14709;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14716, %r1240, %r14691;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14719, %r1237, %r14688, %r14716;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14723, %r1240, %r14700;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14726, %r1237, %r14697, %r14723;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14771, %r14775}, {%r1270, %r1264, %r1267, %r1261}, {%r14705, %r14671}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14779, %r14783}, {%r1270, %r1264, %r1267, %r1261}, {%r14712, %r14680}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14787, %r14791}, {%r1270, %r1264, %r1267, %r1261}, {%r14719, %r14689}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14795, %r14799}, {%r1270, %r1264, %r1267, %r1261}, {%r14726, %r14698}, {%r14608, %r14608};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14770, %r14771, %r14771, %r29104;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29114, %r14775, %r14775, %r14770;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14778, %r14779, %r14779, %r29103;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29113, %r14783, %r14783, %r14778;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14786, %r14787, %r14787, %r29102;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29112, %r14791, %r14791, %r14786;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14794, %r14795, %r14795, %r29101;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29111, %r14799, %r14799, %r14794;
	// end inline asm
	add.s32 	%r29109, %r29099, 1;
	setp.ne.s32 	%p117, %r29109, 40;
	@%p117 bra 	LBB0_132;
// %bb.129:                             // %L92382
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_131;
// %bb.130:                             // %L92447
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r14802, %r28856, 193536;
	add.s32 	%r14803, %r83, %r14802;
	or.b32  	%r14804, %r14803, %r43;
	add.s32 	%r14805, %r14804, %r84;
	mul.wide.u32 	%rd709, %r14805, 4;
	add.s64 	%rd710, %rd4, %rd709;
	st.global.u32 	[%rd710], %r29114;
	add.s32 	%r14806, %r14804, %r85;
	mul.wide.u32 	%rd711, %r14806, 4;
	add.s64 	%rd712, %rd4, %rd711;
	st.global.u32 	[%rd712], %r29113;
	cvt.u64.u32 	%rd713, %r84;
	cvt.u64.u32 	%rd714, %r43;
	cvt.u64.u32 	%rd715, %r83;
	cvt.u64.u32 	%rd716, %r14802;
	add.s64 	%rd717, %rd716, %rd715;
	add.s64 	%rd718, %rd717, %rd714;
	add.s64 	%rd719, %rd718, %rd713;
	shl.b64 	%rd720, %rd719, 2;
	add.s64 	%rd721, %rd4, %rd720;
	st.global.u32 	[%rd721+387072], %r29112;
	cvt.u64.u32 	%rd722, %r85;
	add.s64 	%rd723, %rd718, %rd722;
	shl.b64 	%rd724, %rd723, 2;
	add.s64 	%rd725, %rd4, %rd724;
	st.global.u32 	[%rd725+387072], %r29111;
LBB0_131:                               // %L92960
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29109, 0;
	mov.u32 	%r29111, %r29109;
	mov.u32 	%r29112, %r29109;
	mov.u32 	%r29113, %r29109;
	mov.u32 	%r29114, %r29109;
LBB0_132:                               // %L92961
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r14808, 0;
	mov.u32 	%r29115, %r14808;
	mov.u32 	%r29116, %r14808;
	mov.u32 	%r29117, %r14808;
	mov.u32 	%r29118, %r14808;
	@%p50 bra 	LBB0_285;
	bra.uni 	LBB0_133;
LBB0_285:                               // %oksrem32460
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r14809, %r77, %r495;
	add.s32 	%r14810, %r14809, %r78;
	add.s32 	%r14811, %r14810, %r79;
	add.s32 	%r14812, %r14811, %r28851;
	add.s32 	%r14813, %r14812, %r80;
	add.s32 	%r14814, %r14813, %r81;
	add.s32 	%r14815, %r14814, %r82;
	mul.wide.s32 	%rd726, %r14815, 4;
	add.s64 	%rd728, %rd52, %rd726;
	ld.shared.u32 	%r29115, [%rd728];
	or.b32  	%r14816, %r496, %r77;
	add.s32 	%r14817, %r14816, %r78;
	add.s32 	%r14818, %r14817, %r79;
	add.s32 	%r14819, %r14818, %r28851;
	add.s32 	%r14820, %r14819, %r80;
	add.s32 	%r14821, %r14820, %r81;
	add.s32 	%r14822, %r14821, %r82;
	mul.wide.u32 	%rd729, %r14822, 4;
	add.s64 	%rd730, %rd52, %rd729;
	ld.shared.u32 	%r29116, [%rd730];
	or.b32  	%r14823, %r14809, 32;
	add.s32 	%r14824, %r14823, %r78;
	add.s32 	%r14825, %r14824, %r79;
	add.s32 	%r14826, %r14825, %r28851;
	add.s32 	%r14827, %r14826, %r80;
	add.s32 	%r14828, %r14827, %r81;
	add.s32 	%r14829, %r14828, %r82;
	mul.wide.s32 	%rd731, %r14829, 4;
	add.s64 	%rd732, %rd52, %rd731;
	ld.shared.u32 	%r29117, [%rd732];
	add.s32 	%r14830, %r495, 8288;
	or.b32  	%r14831, %r14830, %r77;
	add.s32 	%r14832, %r14831, %r78;
	add.s32 	%r14833, %r14832, %r79;
	add.s32 	%r14834, %r14833, %r28851;
	add.s32 	%r14835, %r14834, %r80;
	add.s32 	%r14836, %r14835, %r81;
	add.s32 	%r14837, %r14836, %r82;
	mul.wide.u32 	%rd733, %r14837, 4;
	add.s64 	%rd734, %rd52, %rd733;
	ld.shared.u32 	%r29118, [%rd734];
LBB0_133:                               // %L94080
                                        //   in Loop: Header=BB0_16 Depth=1
	shr.u32 	%r15034, %r1853, 8;
	shr.u32 	%r15080, %r1899, 8;
	shr.u32 	%r15126, %r1945, 8;
	shr.u32 	%r15172, %r1991, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14870, %r14873}, {%r1202, %r1199}, {%r29115}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14879, %r14882}, {%r1202, %r1199}, {%r29116}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14888, %r14891}, {%r1202, %r1199}, {%r29117}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r14897, %r14900}, {%r1202, %r1199}, {%r29118}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14866, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14868, %r14866, %r14870;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14871, %r1237, %r14873, %r14868;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14875, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14877, %r14875, %r14879;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14880, %r1237, %r14882, %r14877;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14884, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14886, %r14884, %r14888;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14889, %r1237, %r14891, %r14886;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r14893, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14895, %r14893, %r14897;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14898, %r1237, %r14900, %r14895;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14902, %r1240, %r14873;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14905, %r1237, %r14870, %r14902;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14909, %r1240, %r14882;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14912, %r1237, %r14879, %r14909;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14916, %r1240, %r14891;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14919, %r1237, %r14888, %r14916;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r14923, %r1240, %r14900;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14926, %r1237, %r14897, %r14923;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14971, %r14975}, {%r1270, %r1264, %r1267, %r1261}, {%r14905, %r14871}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14979, %r14983}, {%r1270, %r1264, %r1267, %r1261}, {%r14912, %r14880}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14987, %r14991}, {%r1270, %r1264, %r1267, %r1261}, {%r14919, %r14889}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r14995, %r14999}, {%r1270, %r1264, %r1267, %r1261}, {%r14926, %r14898}, {%r14808, %r14808};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14970, %r14971, %r14971, %r29114;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29124, %r14975, %r14975, %r14970;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14978, %r14979, %r14979, %r29113;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29123, %r14983, %r14983, %r14978;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14986, %r14987, %r14987, %r29112;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29122, %r14991, %r14991, %r14986;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r14994, %r14995, %r14995, %r29111;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29121, %r14999, %r14999, %r14994;
	// end inline asm
	add.s32 	%r29119, %r29109, 1;
	setp.ne.s32 	%p120, %r29119, 40;
	@%p120 bra 	LBB0_137;
// %bb.134:                             // %L94263
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_136;
// %bb.135:                             // %L94328
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r15002, %r28856, 193536;
	add.s32 	%r15003, %r83, %r15002;
	or.b32  	%r15004, %r15003, %r43;
	add.s32 	%r15005, %r15004, %r84;
	mul.wide.u32 	%rd735, %r15005, 4;
	add.s64 	%rd736, %rd4, %rd735;
	st.global.u32 	[%rd736], %r29124;
	add.s32 	%r15006, %r15004, %r85;
	mul.wide.u32 	%rd737, %r15006, 4;
	add.s64 	%rd738, %rd4, %rd737;
	st.global.u32 	[%rd738], %r29123;
	cvt.u64.u32 	%rd739, %r84;
	cvt.u64.u32 	%rd740, %r43;
	cvt.u64.u32 	%rd741, %r83;
	cvt.u64.u32 	%rd742, %r15002;
	add.s64 	%rd743, %rd742, %rd741;
	add.s64 	%rd744, %rd743, %rd740;
	add.s64 	%rd745, %rd744, %rd739;
	shl.b64 	%rd746, %rd745, 2;
	add.s64 	%rd747, %rd4, %rd746;
	st.global.u32 	[%rd747+387072], %r29122;
	cvt.u64.u32 	%rd748, %r85;
	add.s64 	%rd749, %rd744, %rd748;
	shl.b64 	%rd750, %rd749, 2;
	add.s64 	%rd751, %rd4, %rd750;
	st.global.u32 	[%rd751+387072], %r29121;
LBB0_136:                               // %L94841
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29119, 0;
	mov.u32 	%r29121, %r29119;
	mov.u32 	%r29122, %r29119;
	mov.u32 	%r29123, %r29119;
	mov.u32 	%r29124, %r29119;
LBB0_137:                               // %L94842
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r15035, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r15046, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r15032, %r1979, %r15034, %r15035, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15036, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15037, %r15035, %r15036;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15040, %r15032, %r15037;
	// end inline asm
	// begin inline asm
	lop3.b32 %r15043, %r1990, %r15034, %r15046, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15047, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15048, %r15046, %r15047;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15051, %r15043, %r15048;
	// end inline asm
	// begin inline asm
	mov.b32 %r15081, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r15092, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r15078, %r1979, %r15080, %r15081, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15082, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15083, %r15081, %r15082;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15086, %r15078, %r15083;
	// end inline asm
	// begin inline asm
	lop3.b32 %r15089, %r1990, %r15080, %r15092, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15093, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15094, %r15092, %r15093;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15097, %r15089, %r15094;
	// end inline asm
	// begin inline asm
	mov.b32 %r15127, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r15138, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r15124, %r1979, %r15126, %r15127, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15128, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15129, %r15127, %r15128;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15132, %r15124, %r15129;
	// end inline asm
	// begin inline asm
	lop3.b32 %r15135, %r1990, %r15126, %r15138, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15139, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15140, %r15138, %r15139;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15143, %r15135, %r15140;
	// end inline asm
	// begin inline asm
	mov.b32 %r15173, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r15184, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r15170, %r1979, %r15172, %r15173, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15174, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15175, %r15173, %r15174;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15178, %r15170, %r15175;
	// end inline asm
	// begin inline asm
	lop3.b32 %r15181, %r1990, %r15172, %r15184, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r15185, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r15186, %r15184, %r15185;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r15189, %r15181, %r15186;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r15040;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16112, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r15051;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16115, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r15086;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16118, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r15097;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16121, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r15132;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16124, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r15143;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16127, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r15178;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16130, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r15189;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r16133, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r16464, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16196, %r16199}, {%r1202, %r1199}, {%r16112}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16205, %r16208}, {%r1202, %r1199}, {%r16115}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16214, %r16217}, {%r1202, %r1199}, {%r16118}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16223, %r16226}, {%r1202, %r1199}, {%r16121}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16232, %r16235}, {%r1202, %r1199}, {%r16124}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16241, %r16244}, {%r1202, %r1199}, {%r16127}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16250, %r16253}, {%r1202, %r1199}, {%r16130}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16259, %r16262}, {%r1202, %r1199}, {%r16133}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16192, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16194, %r16192, %r16196;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16197, %r1237, %r16199, %r16194;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16201, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16203, %r16201, %r16205;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16206, %r1237, %r16208, %r16203;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16210, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16212, %r16210, %r16214;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16215, %r1237, %r16217, %r16212;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16219, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16221, %r16219, %r16223;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16224, %r1237, %r16226, %r16221;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16228, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16230, %r16228, %r16232;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16233, %r1237, %r16235, %r16230;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16237, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16239, %r16237, %r16241;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16242, %r1237, %r16244, %r16239;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16246, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16248, %r16246, %r16250;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16251, %r1237, %r16253, %r16248;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16255, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16257, %r16255, %r16259;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16260, %r1237, %r16262, %r16257;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16264, %r1240, %r16199;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16267, %r1237, %r16196, %r16264;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16271, %r1240, %r16208;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16274, %r1237, %r16205, %r16271;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16278, %r1240, %r16217;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16281, %r1237, %r16214, %r16278;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16285, %r1240, %r16226;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16288, %r1237, %r16223, %r16285;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16292, %r1240, %r16235;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16295, %r1237, %r16232, %r16292;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16299, %r1240, %r16244;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16302, %r1237, %r16241, %r16299;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16306, %r1240, %r16253;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16309, %r1237, %r16250, %r16306;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16313, %r1240, %r16262;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16316, %r1237, %r16259, %r16313;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16401, %r16402}, {%r1270, %r1264, %r1267, %r1261}, {%r16267, %r16197}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16409, %r16410}, {%r1270, %r1264, %r1267, %r1261}, {%r16274, %r16206}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16417, %r16418}, {%r1270, %r1264, %r1267, %r1261}, {%r16281, %r16215}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16425, %r16426}, {%r1270, %r1264, %r1267, %r1261}, {%r16288, %r16224}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16433, %r16434}, {%r1270, %r1264, %r1267, %r1261}, {%r16295, %r16233}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16441, %r16442}, {%r1270, %r1264, %r1267, %r1261}, {%r16302, %r16242}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16449, %r16450}, {%r1270, %r1264, %r1267, %r1261}, {%r16309, %r16251}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16457, %r16458}, {%r1270, %r1264, %r1267, %r1261}, {%r16316, %r16260}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	prmt.b32 %r16400, %r16401, %r16402, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16404, %r16401, %r16402, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16408, %r16409, %r16410, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16412, %r16409, %r16410, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16416, %r16417, %r16418, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16420, %r16417, %r16418, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16424, %r16425, %r16426, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16428, %r16425, %r16426, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16432, %r16433, %r16434, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16436, %r16433, %r16434, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16440, %r16441, %r16442, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16444, %r16441, %r16442, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16448, %r16449, %r16450, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16452, %r16449, %r16450, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16456, %r16457, %r16458, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r16460, %r16457, %r16458, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r16400;
	st.shared.u32 	[%rd7+33024], %r16404;
	st.shared.u32 	[%rd8], %r16408;
	st.shared.u32 	[%rd9], %r16412;
	add.s16 	%rs2230, %rs3785, 2;
	shr.s16 	%rs2231, %rs2230, 15;
	shr.u16 	%rs2232, %rs2231, 14;
	add.s16 	%rs2233, %rs2230, %rs2232;
	and.b16  	%rs2234, %rs2233, -4;
	sub.s16 	%rs2235, %rs2230, %rs2234;
	mul.wide.s16 	%r561, %rs2235, 64;
	add.s32 	%r16465, %r71, %r561;
	add.s32 	%r16466, %r16465, %r28848;
	add.s32 	%r16467, %r16466, %r72;
	add.s32 	%r16468, %r16467, %r28849;
	add.s32 	%r16469, %r16468, %r73;
	add.s32 	%r16470, %r16469, %r74;
	add.s32 	%r16471, %r16470, %r75;
	mul.wide.s32 	%rd752, %r16471, 4;
	add.s64 	%rd754, %rd52, %rd752;
	st.shared.u32 	[%rd754], %r16416;
	add.s32 	%r562, %r561, 8256;
	add.s32 	%r16472, %r562, %r71;
	add.s32 	%r16473, %r16472, %r28848;
	add.s32 	%r16474, %r16473, %r72;
	add.s32 	%r16475, %r16474, %r28849;
	add.s32 	%r16476, %r16475, %r73;
	add.s32 	%r16477, %r16476, %r74;
	add.s32 	%r16478, %r16477, %r75;
	mul.wide.u32 	%rd755, %r16478, 4;
	add.s64 	%rd756, %rd52, %rd755;
	st.shared.u32 	[%rd756], %r16420;
	add.s32 	%r16479, %r16465, %r76;
	add.s32 	%r16480, %r16479, %r72;
	add.s32 	%r16481, %r16480, %r28849;
	add.s32 	%r16482, %r16481, %r73;
	add.s32 	%r16483, %r16482, %r74;
	add.s32 	%r16484, %r16483, %r75;
	mul.wide.s32 	%rd757, %r16484, 4;
	add.s64 	%rd758, %rd52, %rd757;
	st.shared.u32 	[%rd758], %r16424;
	add.s32 	%r16485, %r16472, %r76;
	add.s32 	%r16486, %r16485, %r72;
	add.s32 	%r16487, %r16486, %r28849;
	add.s32 	%r16488, %r16487, %r73;
	add.s32 	%r16489, %r16488, %r74;
	add.s32 	%r16490, %r16489, %r75;
	mul.wide.s32 	%rd759, %r16490, 4;
	add.s64 	%rd760, %rd52, %rd759;
	st.shared.u32 	[%rd760], %r16428;
	add.s16 	%rs2236, %rs3785, 3;
	shr.s16 	%rs2237, %rs2236, 15;
	shr.u16 	%rs2238, %rs2237, 14;
	add.s16 	%rs2239, %rs2236, %rs2238;
	and.b16  	%rs2240, %rs2239, -4;
	sub.s16 	%rs2241, %rs2236, %rs2240;
	mul.wide.s16 	%r563, %rs2241, 64;
	add.s32 	%r16491, %r71, %r563;
	add.s32 	%r16492, %r16491, %r28848;
	add.s32 	%r16493, %r16492, %r72;
	add.s32 	%r16494, %r16493, %r28849;
	add.s32 	%r16495, %r16494, %r73;
	add.s32 	%r16496, %r16495, %r74;
	add.s32 	%r16497, %r16496, %r75;
	mul.wide.s32 	%rd761, %r16497, 4;
	add.s64 	%rd762, %rd52, %rd761;
	st.shared.u32 	[%rd762], %r16432;
	add.s32 	%r564, %r563, 8256;
	add.s32 	%r16498, %r564, %r71;
	add.s32 	%r16499, %r16498, %r28848;
	add.s32 	%r16500, %r16499, %r72;
	add.s32 	%r16501, %r16500, %r28849;
	add.s32 	%r16502, %r16501, %r73;
	add.s32 	%r16503, %r16502, %r74;
	add.s32 	%r16504, %r16503, %r75;
	mul.wide.u32 	%rd763, %r16504, 4;
	add.s64 	%rd764, %rd52, %rd763;
	st.shared.u32 	[%rd764], %r16436;
	add.s32 	%r16505, %r16491, %r76;
	add.s32 	%r16506, %r16505, %r72;
	add.s32 	%r16507, %r16506, %r28849;
	add.s32 	%r16508, %r16507, %r73;
	add.s32 	%r16509, %r16508, %r74;
	add.s32 	%r16510, %r16509, %r75;
	mul.wide.s32 	%rd765, %r16510, 4;
	add.s64 	%rd766, %rd52, %rd765;
	st.shared.u32 	[%rd766], %r16440;
	add.s32 	%r16511, %r16498, %r76;
	add.s32 	%r16512, %r16511, %r72;
	add.s32 	%r16513, %r16512, %r28849;
	add.s32 	%r16514, %r16513, %r73;
	add.s32 	%r16515, %r16514, %r74;
	add.s32 	%r16516, %r16515, %r75;
	mul.wide.s32 	%rd767, %r16516, 4;
	add.s64 	%rd768, %rd52, %rd767;
	st.shared.u32 	[%rd768], %r16444;
	add.s16 	%rs2242, %rs3785, 4;
	shr.s16 	%rs2243, %rs2242, 15;
	shr.u16 	%rs2244, %rs2243, 14;
	add.s16 	%rs2245, %rs2242, %rs2244;
	and.b16  	%rs2246, %rs2245, -4;
	sub.s16 	%rs2247, %rs2242, %rs2246;
	mul.wide.s16 	%r565, %rs2247, 64;
	add.s32 	%r16517, %r71, %r565;
	add.s32 	%r16518, %r16517, %r28848;
	add.s32 	%r16519, %r16518, %r72;
	add.s32 	%r16520, %r16519, %r28849;
	add.s32 	%r16521, %r16520, %r73;
	add.s32 	%r16522, %r16521, %r74;
	add.s32 	%r16523, %r16522, %r75;
	mul.wide.s32 	%rd769, %r16523, 4;
	add.s64 	%rd770, %rd52, %rd769;
	st.shared.u32 	[%rd770], %r16448;
	add.s32 	%r566, %r565, 8256;
	add.s32 	%r16524, %r566, %r71;
	add.s32 	%r16525, %r16524, %r28848;
	add.s32 	%r16526, %r16525, %r72;
	add.s32 	%r16527, %r16526, %r28849;
	add.s32 	%r16528, %r16527, %r73;
	add.s32 	%r16529, %r16528, %r74;
	add.s32 	%r16530, %r16529, %r75;
	mul.wide.u32 	%rd771, %r16530, 4;
	add.s64 	%rd772, %rd52, %rd771;
	st.shared.u32 	[%rd772], %r16452;
	add.s32 	%r16531, %r16517, %r76;
	add.s32 	%r16532, %r16531, %r72;
	add.s32 	%r16533, %r16532, %r28849;
	add.s32 	%r16534, %r16533, %r73;
	add.s32 	%r16535, %r16534, %r74;
	add.s32 	%r16536, %r16535, %r75;
	mul.wide.s32 	%rd773, %r16536, 4;
	add.s64 	%rd774, %rd52, %rd773;
	st.shared.u32 	[%rd774], %r16456;
	add.s32 	%r16537, %r16524, %r76;
	add.s32 	%r16538, %r16537, %r72;
	add.s32 	%r16539, %r16538, %r28849;
	add.s32 	%r16540, %r16539, %r73;
	add.s32 	%r16541, %r16540, %r74;
	add.s32 	%r16542, %r16541, %r75;
	mul.wide.s32 	%rd775, %r16542, 4;
	add.s64 	%rd776, %rd52, %rd775;
	st.shared.u32 	[%rd776], %r16460;
	bar.sync 	0;
	mov.u32 	%r29125, %r16464;
	mov.u32 	%r29126, %r16464;
	mov.u32 	%r29127, %r16464;
	mov.u32 	%r29128, %r16464;
	@%p50 bra 	LBB0_286;
	bra.uni 	LBB0_138;
LBB0_286:                               // %oksrem35062
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29125, [%rd10];
	ld.shared.u32 	%r29126, [%rd11+33024];
	ld.shared.u32 	%r29127, [%rd11+128];
	ld.shared.u32 	%r29128, [%rd11+33152];
LBB0_138:                               // %L102111
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16575, %r16578}, {%r1202, %r1199}, {%r29125}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16584, %r16587}, {%r1202, %r1199}, {%r29126}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16593, %r16596}, {%r1202, %r1199}, {%r29127}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16602, %r16605}, {%r1202, %r1199}, {%r29128}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16571, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16573, %r16571, %r16575;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16576, %r1237, %r16578, %r16573;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16580, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16582, %r16580, %r16584;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16585, %r1237, %r16587, %r16582;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16589, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16591, %r16589, %r16593;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16594, %r1237, %r16596, %r16591;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16598, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16600, %r16598, %r16602;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16603, %r1237, %r16605, %r16600;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16607, %r1240, %r16578;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16610, %r1237, %r16575, %r16607;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16614, %r1240, %r16587;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16617, %r1237, %r16584, %r16614;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16621, %r1240, %r16596;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16624, %r1237, %r16593, %r16621;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16628, %r1240, %r16605;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16631, %r1237, %r16602, %r16628;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16676, %r16680}, {%r1270, %r1264, %r1267, %r1261}, {%r16610, %r16576}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16684, %r16688}, {%r1270, %r1264, %r1267, %r1261}, {%r16617, %r16585}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16692, %r16696}, {%r1270, %r1264, %r1267, %r1261}, {%r16624, %r16594}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16700, %r16704}, {%r1270, %r1264, %r1267, %r1261}, {%r16631, %r16603}, {%r16464, %r16464};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16675, %r16676, %r16676, %r29124;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29134, %r16680, %r16680, %r16675;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16683, %r16684, %r16684, %r29123;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29133, %r16688, %r16688, %r16683;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16691, %r16692, %r16692, %r29122;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29132, %r16696, %r16696, %r16691;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16699, %r16700, %r16700, %r29121;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29131, %r16704, %r16704, %r16699;
	// end inline asm
	add.s32 	%r29129, %r29119, 1;
	setp.ne.s32 	%p123, %r29129, 40;
	@%p123 bra 	LBB0_142;
// %bb.139:                             // %L102294
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_141;
// %bb.140:                             // %L102359
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r16707, %r28856, 193536;
	add.s32 	%r16708, %r83, %r16707;
	or.b32  	%r16709, %r16708, %r43;
	add.s32 	%r16710, %r16709, %r84;
	mul.wide.u32 	%rd777, %r16710, 4;
	add.s64 	%rd778, %rd4, %rd777;
	st.global.u32 	[%rd778], %r29134;
	add.s32 	%r16711, %r16709, %r85;
	mul.wide.u32 	%rd779, %r16711, 4;
	add.s64 	%rd780, %rd4, %rd779;
	st.global.u32 	[%rd780], %r29133;
	cvt.u64.u32 	%rd781, %r84;
	cvt.u64.u32 	%rd782, %r43;
	cvt.u64.u32 	%rd783, %r83;
	cvt.u64.u32 	%rd784, %r16707;
	add.s64 	%rd785, %rd784, %rd783;
	add.s64 	%rd786, %rd785, %rd782;
	add.s64 	%rd787, %rd786, %rd781;
	shl.b64 	%rd788, %rd787, 2;
	add.s64 	%rd789, %rd4, %rd788;
	st.global.u32 	[%rd789+387072], %r29132;
	cvt.u64.u32 	%rd790, %r85;
	add.s64 	%rd791, %rd786, %rd790;
	shl.b64 	%rd792, %rd791, 2;
	add.s64 	%rd793, %rd4, %rd792;
	st.global.u32 	[%rd793+387072], %r29131;
LBB0_141:                               // %L102872
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29129, 0;
	mov.u32 	%r29131, %r29129;
	mov.u32 	%r29132, %r29129;
	mov.u32 	%r29133, %r29129;
	mov.u32 	%r29134, %r29129;
LBB0_142:                               // %L102873
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r16713, 0;
	mov.u32 	%r29135, %r16713;
	mov.u32 	%r29136, %r16713;
	mov.u32 	%r29137, %r16713;
	mov.u32 	%r29138, %r16713;
	@%p50 bra 	LBB0_287;
	bra.uni 	LBB0_143;
LBB0_287:                               // %oksrem35760
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r16714, %r77, %r561;
	add.s32 	%r16715, %r16714, %r78;
	add.s32 	%r16716, %r16715, %r79;
	add.s32 	%r16717, %r16716, %r28851;
	add.s32 	%r16718, %r16717, %r80;
	add.s32 	%r16719, %r16718, %r81;
	add.s32 	%r16720, %r16719, %r82;
	mul.wide.s32 	%rd794, %r16720, 4;
	add.s64 	%rd796, %rd52, %rd794;
	ld.shared.u32 	%r29135, [%rd796];
	or.b32  	%r16721, %r562, %r77;
	add.s32 	%r16722, %r16721, %r78;
	add.s32 	%r16723, %r16722, %r79;
	add.s32 	%r16724, %r16723, %r28851;
	add.s32 	%r16725, %r16724, %r80;
	add.s32 	%r16726, %r16725, %r81;
	add.s32 	%r16727, %r16726, %r82;
	mul.wide.u32 	%rd797, %r16727, 4;
	add.s64 	%rd798, %rd52, %rd797;
	ld.shared.u32 	%r29136, [%rd798];
	or.b32  	%r16728, %r16714, 32;
	add.s32 	%r16729, %r16728, %r78;
	add.s32 	%r16730, %r16729, %r79;
	add.s32 	%r16731, %r16730, %r28851;
	add.s32 	%r16732, %r16731, %r80;
	add.s32 	%r16733, %r16732, %r81;
	add.s32 	%r16734, %r16733, %r82;
	mul.wide.s32 	%rd799, %r16734, 4;
	add.s64 	%rd800, %rd52, %rd799;
	ld.shared.u32 	%r29137, [%rd800];
	add.s32 	%r16735, %r561, 8288;
	or.b32  	%r16736, %r16735, %r77;
	add.s32 	%r16737, %r16736, %r78;
	add.s32 	%r16738, %r16737, %r79;
	add.s32 	%r16739, %r16738, %r28851;
	add.s32 	%r16740, %r16739, %r80;
	add.s32 	%r16741, %r16740, %r81;
	add.s32 	%r16742, %r16741, %r82;
	mul.wide.u32 	%rd801, %r16742, 4;
	add.s64 	%rd802, %rd52, %rd801;
	ld.shared.u32 	%r29138, [%rd802];
LBB0_143:                               // %L103992
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16775, %r16778}, {%r1202, %r1199}, {%r29135}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16784, %r16787}, {%r1202, %r1199}, {%r29136}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16793, %r16796}, {%r1202, %r1199}, {%r29137}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16802, %r16805}, {%r1202, %r1199}, {%r29138}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16771, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16773, %r16771, %r16775;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16776, %r1237, %r16778, %r16773;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16780, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16782, %r16780, %r16784;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16785, %r1237, %r16787, %r16782;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16789, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16791, %r16789, %r16793;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16794, %r1237, %r16796, %r16791;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16798, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16800, %r16798, %r16802;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16803, %r1237, %r16805, %r16800;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16807, %r1240, %r16778;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16810, %r1237, %r16775, %r16807;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16814, %r1240, %r16787;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16817, %r1237, %r16784, %r16814;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16821, %r1240, %r16796;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16824, %r1237, %r16793, %r16821;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16828, %r1240, %r16805;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16831, %r1237, %r16802, %r16828;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16876, %r16880}, {%r1270, %r1264, %r1267, %r1261}, {%r16810, %r16776}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16884, %r16888}, {%r1270, %r1264, %r1267, %r1261}, {%r16817, %r16785}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16892, %r16896}, {%r1270, %r1264, %r1267, %r1261}, {%r16824, %r16794}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r16900, %r16904}, {%r1270, %r1264, %r1267, %r1261}, {%r16831, %r16803}, {%r16713, %r16713};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16875, %r16876, %r16876, %r29134;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29144, %r16880, %r16880, %r16875;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16883, %r16884, %r16884, %r29133;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29143, %r16888, %r16888, %r16883;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16891, %r16892, %r16892, %r29132;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29142, %r16896, %r16896, %r16891;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16899, %r16900, %r16900, %r29131;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29141, %r16904, %r16904, %r16899;
	// end inline asm
	add.s32 	%r29139, %r29129, 1;
	setp.ne.s32 	%p126, %r29139, 40;
	@%p126 bra 	LBB0_147;
// %bb.144:                             // %L104175
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_146;
// %bb.145:                             // %L104240
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r16907, %r28856, 193536;
	add.s32 	%r16908, %r83, %r16907;
	or.b32  	%r16909, %r16908, %r43;
	add.s32 	%r16910, %r16909, %r84;
	mul.wide.u32 	%rd803, %r16910, 4;
	add.s64 	%rd804, %rd4, %rd803;
	st.global.u32 	[%rd804], %r29144;
	add.s32 	%r16911, %r16909, %r85;
	mul.wide.u32 	%rd805, %r16911, 4;
	add.s64 	%rd806, %rd4, %rd805;
	st.global.u32 	[%rd806], %r29143;
	cvt.u64.u32 	%rd807, %r84;
	cvt.u64.u32 	%rd808, %r43;
	cvt.u64.u32 	%rd809, %r83;
	cvt.u64.u32 	%rd810, %r16907;
	add.s64 	%rd811, %rd810, %rd809;
	add.s64 	%rd812, %rd811, %rd808;
	add.s64 	%rd813, %rd812, %rd807;
	shl.b64 	%rd814, %rd813, 2;
	add.s64 	%rd815, %rd4, %rd814;
	st.global.u32 	[%rd815+387072], %r29142;
	cvt.u64.u32 	%rd816, %r85;
	add.s64 	%rd817, %rd812, %rd816;
	shl.b64 	%rd818, %rd817, 2;
	add.s64 	%rd819, %rd4, %rd818;
	st.global.u32 	[%rd819+387072], %r29141;
LBB0_146:                               // %L104753
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29139, 0;
	mov.u32 	%r29141, %r29139;
	mov.u32 	%r29142, %r29139;
	mov.u32 	%r29143, %r29139;
	mov.u32 	%r29144, %r29139;
LBB0_147:                               // %L104754
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r16913, 0;
	mov.u32 	%r29145, %r16913;
	mov.u32 	%r29146, %r16913;
	mov.u32 	%r29147, %r16913;
	mov.u32 	%r29148, %r16913;
	@%p50 bra 	LBB0_288;
	bra.uni 	LBB0_148;
LBB0_288:                               // %oksrem36458
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r16914, %r77, %r563;
	add.s32 	%r16915, %r16914, %r78;
	add.s32 	%r16916, %r16915, %r79;
	add.s32 	%r16917, %r16916, %r28851;
	add.s32 	%r16918, %r16917, %r80;
	add.s32 	%r16919, %r16918, %r81;
	add.s32 	%r16920, %r16919, %r82;
	mul.wide.s32 	%rd820, %r16920, 4;
	add.s64 	%rd822, %rd52, %rd820;
	ld.shared.u32 	%r29145, [%rd822];
	or.b32  	%r16921, %r564, %r77;
	add.s32 	%r16922, %r16921, %r78;
	add.s32 	%r16923, %r16922, %r79;
	add.s32 	%r16924, %r16923, %r28851;
	add.s32 	%r16925, %r16924, %r80;
	add.s32 	%r16926, %r16925, %r81;
	add.s32 	%r16927, %r16926, %r82;
	mul.wide.u32 	%rd823, %r16927, 4;
	add.s64 	%rd824, %rd52, %rd823;
	ld.shared.u32 	%r29146, [%rd824];
	or.b32  	%r16928, %r16914, 32;
	add.s32 	%r16929, %r16928, %r78;
	add.s32 	%r16930, %r16929, %r79;
	add.s32 	%r16931, %r16930, %r28851;
	add.s32 	%r16932, %r16931, %r80;
	add.s32 	%r16933, %r16932, %r81;
	add.s32 	%r16934, %r16933, %r82;
	mul.wide.s32 	%rd825, %r16934, 4;
	add.s64 	%rd826, %rd52, %rd825;
	ld.shared.u32 	%r29147, [%rd826];
	add.s32 	%r16935, %r563, 8288;
	or.b32  	%r16936, %r16935, %r77;
	add.s32 	%r16937, %r16936, %r78;
	add.s32 	%r16938, %r16937, %r79;
	add.s32 	%r16939, %r16938, %r28851;
	add.s32 	%r16940, %r16939, %r80;
	add.s32 	%r16941, %r16940, %r81;
	add.s32 	%r16942, %r16941, %r82;
	mul.wide.u32 	%rd827, %r16942, 4;
	add.s64 	%rd828, %rd52, %rd827;
	ld.shared.u32 	%r29148, [%rd828];
LBB0_148:                               // %L105873
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16975, %r16978}, {%r1202, %r1199}, {%r29145}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16984, %r16987}, {%r1202, %r1199}, {%r29146}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r16993, %r16996}, {%r1202, %r1199}, {%r29147}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r17002, %r17005}, {%r1202, %r1199}, {%r29148}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16971, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16973, %r16971, %r16975;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16976, %r1237, %r16978, %r16973;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16980, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16982, %r16980, %r16984;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16985, %r1237, %r16987, %r16982;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16989, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r16991, %r16989, %r16993;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r16994, %r1237, %r16996, %r16991;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r16998, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17000, %r16998, %r17002;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17003, %r1237, %r17005, %r17000;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17007, %r1240, %r16978;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17010, %r1237, %r16975, %r17007;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17014, %r1240, %r16987;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17017, %r1237, %r16984, %r17014;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17021, %r1240, %r16996;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17024, %r1237, %r16993, %r17021;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17028, %r1240, %r17005;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17031, %r1237, %r17002, %r17028;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17076, %r17080}, {%r1270, %r1264, %r1267, %r1261}, {%r17010, %r16976}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17084, %r17088}, {%r1270, %r1264, %r1267, %r1261}, {%r17017, %r16985}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17092, %r17096}, {%r1270, %r1264, %r1267, %r1261}, {%r17024, %r16994}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17100, %r17104}, {%r1270, %r1264, %r1267, %r1261}, {%r17031, %r17003}, {%r16913, %r16913};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17075, %r17076, %r17076, %r29144;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29154, %r17080, %r17080, %r17075;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17083, %r17084, %r17084, %r29143;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29153, %r17088, %r17088, %r17083;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17091, %r17092, %r17092, %r29142;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29152, %r17096, %r17096, %r17091;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17099, %r17100, %r17100, %r29141;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29151, %r17104, %r17104, %r17099;
	// end inline asm
	add.s32 	%r29149, %r29139, 1;
	setp.ne.s32 	%p129, %r29149, 40;
	@%p129 bra 	LBB0_152;
// %bb.149:                             // %L106056
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_151;
// %bb.150:                             // %L106121
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r17107, %r28856, 193536;
	add.s32 	%r17108, %r83, %r17107;
	or.b32  	%r17109, %r17108, %r43;
	add.s32 	%r17110, %r17109, %r84;
	mul.wide.u32 	%rd829, %r17110, 4;
	add.s64 	%rd830, %rd4, %rd829;
	st.global.u32 	[%rd830], %r29154;
	add.s32 	%r17111, %r17109, %r85;
	mul.wide.u32 	%rd831, %r17111, 4;
	add.s64 	%rd832, %rd4, %rd831;
	st.global.u32 	[%rd832], %r29153;
	cvt.u64.u32 	%rd833, %r84;
	cvt.u64.u32 	%rd834, %r43;
	cvt.u64.u32 	%rd835, %r83;
	cvt.u64.u32 	%rd836, %r17107;
	add.s64 	%rd837, %rd836, %rd835;
	add.s64 	%rd838, %rd837, %rd834;
	add.s64 	%rd839, %rd838, %rd833;
	shl.b64 	%rd840, %rd839, 2;
	add.s64 	%rd841, %rd4, %rd840;
	st.global.u32 	[%rd841+387072], %r29152;
	cvt.u64.u32 	%rd842, %r85;
	add.s64 	%rd843, %rd838, %rd842;
	shl.b64 	%rd844, %rd843, 2;
	add.s64 	%rd845, %rd4, %rd844;
	st.global.u32 	[%rd845+387072], %r29151;
LBB0_151:                               // %L106634
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29149, 0;
	mov.u32 	%r29151, %r29149;
	mov.u32 	%r29152, %r29149;
	mov.u32 	%r29153, %r29149;
	mov.u32 	%r29154, %r29149;
LBB0_152:                               // %L106635
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r17113, 0;
	mov.u32 	%r29155, %r17113;
	mov.u32 	%r29156, %r17113;
	mov.u32 	%r29157, %r17113;
	mov.u32 	%r29158, %r17113;
	@%p50 bra 	LBB0_289;
	bra.uni 	LBB0_153;
LBB0_289:                               // %oksrem37156
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r17114, %r77, %r565;
	add.s32 	%r17115, %r17114, %r78;
	add.s32 	%r17116, %r17115, %r79;
	add.s32 	%r17117, %r17116, %r28851;
	add.s32 	%r17118, %r17117, %r80;
	add.s32 	%r17119, %r17118, %r81;
	add.s32 	%r17120, %r17119, %r82;
	mul.wide.s32 	%rd846, %r17120, 4;
	add.s64 	%rd848, %rd52, %rd846;
	ld.shared.u32 	%r29155, [%rd848];
	or.b32  	%r17121, %r566, %r77;
	add.s32 	%r17122, %r17121, %r78;
	add.s32 	%r17123, %r17122, %r79;
	add.s32 	%r17124, %r17123, %r28851;
	add.s32 	%r17125, %r17124, %r80;
	add.s32 	%r17126, %r17125, %r81;
	add.s32 	%r17127, %r17126, %r82;
	mul.wide.u32 	%rd849, %r17127, 4;
	add.s64 	%rd850, %rd52, %rd849;
	ld.shared.u32 	%r29156, [%rd850];
	or.b32  	%r17128, %r17114, 32;
	add.s32 	%r17129, %r17128, %r78;
	add.s32 	%r17130, %r17129, %r79;
	add.s32 	%r17131, %r17130, %r28851;
	add.s32 	%r17132, %r17131, %r80;
	add.s32 	%r17133, %r17132, %r81;
	add.s32 	%r17134, %r17133, %r82;
	mul.wide.s32 	%rd851, %r17134, 4;
	add.s64 	%rd852, %rd52, %rd851;
	ld.shared.u32 	%r29157, [%rd852];
	add.s32 	%r17135, %r565, 8288;
	or.b32  	%r17136, %r17135, %r77;
	add.s32 	%r17137, %r17136, %r78;
	add.s32 	%r17138, %r17137, %r79;
	add.s32 	%r17139, %r17138, %r28851;
	add.s32 	%r17140, %r17139, %r80;
	add.s32 	%r17141, %r17140, %r81;
	add.s32 	%r17142, %r17141, %r82;
	mul.wide.u32 	%rd853, %r17142, 4;
	add.s64 	%rd854, %rd52, %rd853;
	ld.shared.u32 	%r29158, [%rd854];
LBB0_153:                               // %L107754
                                        //   in Loop: Header=BB0_16 Depth=1
	shr.u32 	%r17523, %r125, 8;
	shr.u32 	%r17569, %r127, 8;
	shr.u32 	%r17615, %r129, 8;
	shr.u32 	%r17661, %r131, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r17175, %r17178}, {%r1202, %r1199}, {%r29155}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r17184, %r17187}, {%r1202, %r1199}, {%r29156}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r17193, %r17196}, {%r1202, %r1199}, {%r29157}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r17202, %r17205}, {%r1202, %r1199}, {%r29158}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r17171, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17173, %r17171, %r17175;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17176, %r1237, %r17178, %r17173;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r17180, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17182, %r17180, %r17184;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17185, %r1237, %r17187, %r17182;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r17189, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17191, %r17189, %r17193;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17194, %r1237, %r17196, %r17191;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r17198, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17200, %r17198, %r17202;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17203, %r1237, %r17205, %r17200;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17207, %r1240, %r17178;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17210, %r1237, %r17175, %r17207;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17214, %r1240, %r17187;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17217, %r1237, %r17184, %r17214;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17221, %r1240, %r17196;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17224, %r1237, %r17193, %r17221;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r17228, %r1240, %r17205;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17231, %r1237, %r17202, %r17228;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17276, %r17280}, {%r1270, %r1264, %r1267, %r1261}, {%r17210, %r17176}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17284, %r17288}, {%r1270, %r1264, %r1267, %r1261}, {%r17217, %r17185}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17292, %r17296}, {%r1270, %r1264, %r1267, %r1261}, {%r17224, %r17194}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r17300, %r17304}, {%r1270, %r1264, %r1267, %r1261}, {%r17231, %r17203}, {%r17113, %r17113};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17275, %r17276, %r17276, %r29154;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29164, %r17280, %r17280, %r17275;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17283, %r17284, %r17284, %r29153;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29163, %r17288, %r17288, %r17283;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17291, %r17292, %r17292, %r29152;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29162, %r17296, %r17296, %r17291;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r17299, %r17300, %r17300, %r29151;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29161, %r17304, %r17304, %r17299;
	// end inline asm
	add.s32 	%r29159, %r29149, 1;
	setp.ne.s32 	%p132, %r29159, 40;
	@%p132 bra 	LBB0_157;
// %bb.154:                             // %L107937
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_156;
// %bb.155:                             // %L108002
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r17307, %r28856, 193536;
	add.s32 	%r17308, %r83, %r17307;
	or.b32  	%r17309, %r17308, %r43;
	add.s32 	%r17310, %r17309, %r84;
	mul.wide.u32 	%rd855, %r17310, 4;
	add.s64 	%rd856, %rd4, %rd855;
	st.global.u32 	[%rd856], %r29164;
	add.s32 	%r17311, %r17309, %r85;
	mul.wide.u32 	%rd857, %r17311, 4;
	add.s64 	%rd858, %rd4, %rd857;
	st.global.u32 	[%rd858], %r29163;
	cvt.u64.u32 	%rd859, %r84;
	cvt.u64.u32 	%rd860, %r43;
	cvt.u64.u32 	%rd861, %r83;
	cvt.u64.u32 	%rd862, %r17307;
	add.s64 	%rd863, %rd862, %rd861;
	add.s64 	%rd864, %rd863, %rd860;
	add.s64 	%rd865, %rd864, %rd859;
	shl.b64 	%rd866, %rd865, 2;
	add.s64 	%rd867, %rd4, %rd866;
	st.global.u32 	[%rd867+387072], %r29162;
	cvt.u64.u32 	%rd868, %r85;
	add.s64 	%rd869, %rd864, %rd868;
	shl.b64 	%rd870, %rd869, 2;
	add.s64 	%rd871, %rd4, %rd870;
	st.global.u32 	[%rd871+387072], %r29161;
LBB0_156:                               // %L108515
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29159, 0;
	mov.u32 	%r29161, %r29159;
	mov.u32 	%r29162, %r29159;
	mov.u32 	%r29163, %r29159;
	mov.u32 	%r29164, %r29159;
LBB0_157:                               // %L108516
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r17524, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r17535, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r17521, %r1979, %r17523, %r17524, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17525, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17526, %r17524, %r17525;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17529, %r17521, %r17526;
	// end inline asm
	// begin inline asm
	lop3.b32 %r17532, %r1990, %r17523, %r17535, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17536, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17537, %r17535, %r17536;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17540, %r17532, %r17537;
	// end inline asm
	// begin inline asm
	mov.b32 %r17570, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r17581, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r17567, %r1979, %r17569, %r17570, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17571, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17572, %r17570, %r17571;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17575, %r17567, %r17572;
	// end inline asm
	// begin inline asm
	lop3.b32 %r17578, %r1990, %r17569, %r17581, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17582, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17583, %r17581, %r17582;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17586, %r17578, %r17583;
	// end inline asm
	// begin inline asm
	mov.b32 %r17616, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r17627, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r17613, %r1979, %r17615, %r17616, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17617, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17618, %r17616, %r17617;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17621, %r17613, %r17618;
	// end inline asm
	// begin inline asm
	lop3.b32 %r17624, %r1990, %r17615, %r17627, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17628, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17629, %r17627, %r17628;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17632, %r17624, %r17629;
	// end inline asm
	// begin inline asm
	mov.b32 %r17662, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r17673, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r17659, %r1979, %r17661, %r17662, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17663, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17664, %r17662, %r17663;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17667, %r17659, %r17664;
	// end inline asm
	// begin inline asm
	lop3.b32 %r17670, %r1990, %r17661, %r17673, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r17674, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r17675, %r17673, %r17674;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r17678, %r17670, %r17675;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r17529;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18417, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r17540;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18420, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r17575;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18423, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r17586;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18426, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r17621;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18429, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r17632;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18432, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r17667;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18435, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r17678;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r18438, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r18769, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18501, %r18504}, {%r1202, %r1199}, {%r18417}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18510, %r18513}, {%r1202, %r1199}, {%r18420}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18519, %r18522}, {%r1202, %r1199}, {%r18423}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18528, %r18531}, {%r1202, %r1199}, {%r18426}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18537, %r18540}, {%r1202, %r1199}, {%r18429}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18546, %r18549}, {%r1202, %r1199}, {%r18432}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18555, %r18558}, {%r1202, %r1199}, {%r18435}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18564, %r18567}, {%r1202, %r1199}, {%r18438}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18497, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18499, %r18497, %r18501;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18502, %r1237, %r18504, %r18499;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18506, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18508, %r18506, %r18510;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18511, %r1237, %r18513, %r18508;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18515, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18517, %r18515, %r18519;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18520, %r1237, %r18522, %r18517;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18524, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18526, %r18524, %r18528;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18529, %r1237, %r18531, %r18526;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18533, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18535, %r18533, %r18537;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18538, %r1237, %r18540, %r18535;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18542, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18544, %r18542, %r18546;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18547, %r1237, %r18549, %r18544;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18551, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18553, %r18551, %r18555;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18556, %r1237, %r18558, %r18553;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18560, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18562, %r18560, %r18564;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18565, %r1237, %r18567, %r18562;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18569, %r1240, %r18504;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18572, %r1237, %r18501, %r18569;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18576, %r1240, %r18513;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18579, %r1237, %r18510, %r18576;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18583, %r1240, %r18522;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18586, %r1237, %r18519, %r18583;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18590, %r1240, %r18531;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18593, %r1237, %r18528, %r18590;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18597, %r1240, %r18540;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18600, %r1237, %r18537, %r18597;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18604, %r1240, %r18549;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18607, %r1237, %r18546, %r18604;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18611, %r1240, %r18558;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18614, %r1237, %r18555, %r18611;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18618, %r1240, %r18567;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18621, %r1237, %r18564, %r18618;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18706, %r18707}, {%r1270, %r1264, %r1267, %r1261}, {%r18572, %r18502}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18714, %r18715}, {%r1270, %r1264, %r1267, %r1261}, {%r18579, %r18511}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18722, %r18723}, {%r1270, %r1264, %r1267, %r1261}, {%r18586, %r18520}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18730, %r18731}, {%r1270, %r1264, %r1267, %r1261}, {%r18593, %r18529}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18738, %r18739}, {%r1270, %r1264, %r1267, %r1261}, {%r18600, %r18538}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18746, %r18747}, {%r1270, %r1264, %r1267, %r1261}, {%r18607, %r18547}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18754, %r18755}, {%r1270, %r1264, %r1267, %r1261}, {%r18614, %r18556}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18762, %r18763}, {%r1270, %r1264, %r1267, %r1261}, {%r18621, %r18565}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	prmt.b32 %r18705, %r18706, %r18707, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18709, %r18706, %r18707, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18713, %r18714, %r18715, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18717, %r18714, %r18715, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18721, %r18722, %r18723, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18725, %r18722, %r18723, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18729, %r18730, %r18731, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18733, %r18730, %r18731, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18737, %r18738, %r18739, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18741, %r18738, %r18739, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18745, %r18746, %r18747, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18749, %r18746, %r18747, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18753, %r18754, %r18755, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18757, %r18754, %r18755, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18761, %r18762, %r18763, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r18765, %r18762, %r18763, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r18705;
	st.shared.u32 	[%rd7+33024], %r18709;
	st.shared.u32 	[%rd8], %r18713;
	st.shared.u32 	[%rd9], %r18717;
	add.s16 	%rs2536, %rs3785, 6;
	shr.s16 	%rs2537, %rs2536, 15;
	shr.u16 	%rs2538, %rs2537, 14;
	add.s16 	%rs2539, %rs2536, %rs2538;
	and.b16  	%rs2540, %rs2539, -4;
	sub.s16 	%rs2541, %rs2536, %rs2540;
	mul.wide.s16 	%r631, %rs2541, 64;
	add.s32 	%r18770, %r71, %r631;
	add.s32 	%r18771, %r18770, %r28848;
	add.s32 	%r18772, %r18771, %r72;
	add.s32 	%r18773, %r18772, %r28849;
	add.s32 	%r18774, %r18773, %r73;
	add.s32 	%r18775, %r18774, %r74;
	add.s32 	%r18776, %r18775, %r75;
	mul.wide.s32 	%rd872, %r18776, 4;
	add.s64 	%rd874, %rd52, %rd872;
	st.shared.u32 	[%rd874], %r18721;
	add.s32 	%r632, %r631, 8256;
	add.s32 	%r18777, %r632, %r71;
	add.s32 	%r18778, %r18777, %r28848;
	add.s32 	%r18779, %r18778, %r72;
	add.s32 	%r18780, %r18779, %r28849;
	add.s32 	%r18781, %r18780, %r73;
	add.s32 	%r18782, %r18781, %r74;
	add.s32 	%r18783, %r18782, %r75;
	mul.wide.u32 	%rd875, %r18783, 4;
	add.s64 	%rd876, %rd52, %rd875;
	st.shared.u32 	[%rd876], %r18725;
	add.s32 	%r18784, %r18770, %r76;
	add.s32 	%r18785, %r18784, %r72;
	add.s32 	%r18786, %r18785, %r28849;
	add.s32 	%r18787, %r18786, %r73;
	add.s32 	%r18788, %r18787, %r74;
	add.s32 	%r18789, %r18788, %r75;
	mul.wide.s32 	%rd877, %r18789, 4;
	add.s64 	%rd878, %rd52, %rd877;
	st.shared.u32 	[%rd878], %r18729;
	add.s32 	%r18790, %r18777, %r76;
	add.s32 	%r18791, %r18790, %r72;
	add.s32 	%r18792, %r18791, %r28849;
	add.s32 	%r18793, %r18792, %r73;
	add.s32 	%r18794, %r18793, %r74;
	add.s32 	%r18795, %r18794, %r75;
	mul.wide.s32 	%rd879, %r18795, 4;
	add.s64 	%rd880, %rd52, %rd879;
	st.shared.u32 	[%rd880], %r18733;
	add.s16 	%rs2542, %rs3785, 7;
	shr.s16 	%rs2543, %rs2542, 15;
	shr.u16 	%rs2544, %rs2543, 14;
	add.s16 	%rs2545, %rs2542, %rs2544;
	and.b16  	%rs2546, %rs2545, -4;
	sub.s16 	%rs2547, %rs2542, %rs2546;
	mul.wide.s16 	%r633, %rs2547, 64;
	add.s32 	%r18796, %r71, %r633;
	add.s32 	%r18797, %r18796, %r28848;
	add.s32 	%r18798, %r18797, %r72;
	add.s32 	%r18799, %r18798, %r28849;
	add.s32 	%r18800, %r18799, %r73;
	add.s32 	%r18801, %r18800, %r74;
	add.s32 	%r18802, %r18801, %r75;
	mul.wide.s32 	%rd881, %r18802, 4;
	add.s64 	%rd882, %rd52, %rd881;
	st.shared.u32 	[%rd882], %r18737;
	add.s32 	%r634, %r633, 8256;
	add.s32 	%r18803, %r634, %r71;
	add.s32 	%r18804, %r18803, %r28848;
	add.s32 	%r18805, %r18804, %r72;
	add.s32 	%r18806, %r18805, %r28849;
	add.s32 	%r18807, %r18806, %r73;
	add.s32 	%r18808, %r18807, %r74;
	add.s32 	%r18809, %r18808, %r75;
	mul.wide.u32 	%rd883, %r18809, 4;
	add.s64 	%rd884, %rd52, %rd883;
	st.shared.u32 	[%rd884], %r18741;
	add.s32 	%r18810, %r18796, %r76;
	add.s32 	%r18811, %r18810, %r72;
	add.s32 	%r18812, %r18811, %r28849;
	add.s32 	%r18813, %r18812, %r73;
	add.s32 	%r18814, %r18813, %r74;
	add.s32 	%r18815, %r18814, %r75;
	mul.wide.s32 	%rd885, %r18815, 4;
	add.s64 	%rd886, %rd52, %rd885;
	st.shared.u32 	[%rd886], %r18745;
	add.s32 	%r18816, %r18803, %r76;
	add.s32 	%r18817, %r18816, %r72;
	add.s32 	%r18818, %r18817, %r28849;
	add.s32 	%r18819, %r18818, %r73;
	add.s32 	%r18820, %r18819, %r74;
	add.s32 	%r18821, %r18820, %r75;
	mul.wide.s32 	%rd887, %r18821, 4;
	add.s64 	%rd888, %rd52, %rd887;
	st.shared.u32 	[%rd888], %r18749;
	add.s16 	%rs2548, %rs3785, 8;
	shr.s16 	%rs2549, %rs2548, 15;
	shr.u16 	%rs2550, %rs2549, 14;
	add.s16 	%rs2551, %rs2548, %rs2550;
	and.b16  	%rs2552, %rs2551, -4;
	sub.s16 	%rs2553, %rs2548, %rs2552;
	mul.wide.s16 	%r635, %rs2553, 64;
	add.s32 	%r18822, %r71, %r635;
	add.s32 	%r18823, %r18822, %r28848;
	add.s32 	%r18824, %r18823, %r72;
	add.s32 	%r18825, %r18824, %r28849;
	add.s32 	%r18826, %r18825, %r73;
	add.s32 	%r18827, %r18826, %r74;
	add.s32 	%r18828, %r18827, %r75;
	mul.wide.s32 	%rd889, %r18828, 4;
	add.s64 	%rd890, %rd52, %rd889;
	st.shared.u32 	[%rd890], %r18753;
	add.s32 	%r636, %r635, 8256;
	add.s32 	%r18829, %r636, %r71;
	add.s32 	%r18830, %r18829, %r28848;
	add.s32 	%r18831, %r18830, %r72;
	add.s32 	%r18832, %r18831, %r28849;
	add.s32 	%r18833, %r18832, %r73;
	add.s32 	%r18834, %r18833, %r74;
	add.s32 	%r18835, %r18834, %r75;
	mul.wide.u32 	%rd891, %r18835, 4;
	add.s64 	%rd892, %rd52, %rd891;
	st.shared.u32 	[%rd892], %r18757;
	add.s32 	%r18836, %r18822, %r76;
	add.s32 	%r18837, %r18836, %r72;
	add.s32 	%r18838, %r18837, %r28849;
	add.s32 	%r18839, %r18838, %r73;
	add.s32 	%r18840, %r18839, %r74;
	add.s32 	%r18841, %r18840, %r75;
	mul.wide.s32 	%rd893, %r18841, 4;
	add.s64 	%rd894, %rd52, %rd893;
	st.shared.u32 	[%rd894], %r18761;
	add.s32 	%r18842, %r18829, %r76;
	add.s32 	%r18843, %r18842, %r72;
	add.s32 	%r18844, %r18843, %r28849;
	add.s32 	%r18845, %r18844, %r73;
	add.s32 	%r18846, %r18845, %r74;
	add.s32 	%r18847, %r18846, %r75;
	mul.wide.s32 	%rd895, %r18847, 4;
	add.s64 	%rd896, %rd52, %rd895;
	st.shared.u32 	[%rd896], %r18765;
	bar.sync 	0;
	mov.u32 	%r29165, %r18769;
	mov.u32 	%r29166, %r18769;
	mov.u32 	%r29167, %r18769;
	mov.u32 	%r29168, %r18769;
	@%p50 bra 	LBB0_290;
	bra.uni 	LBB0_158;
LBB0_290:                               // %oksrem39758
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29165, [%rd10];
	ld.shared.u32 	%r29166, [%rd11+33024];
	ld.shared.u32 	%r29167, [%rd11+128];
	ld.shared.u32 	%r29168, [%rd11+33152];
LBB0_158:                               // %L115785
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18880, %r18883}, {%r1202, %r1199}, {%r29165}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18889, %r18892}, {%r1202, %r1199}, {%r29166}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18898, %r18901}, {%r1202, %r1199}, {%r29167}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r18907, %r18910}, {%r1202, %r1199}, {%r29168}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18876, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18878, %r18876, %r18880;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18881, %r1237, %r18883, %r18878;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18885, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18887, %r18885, %r18889;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18890, %r1237, %r18892, %r18887;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18894, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18896, %r18894, %r18898;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18899, %r1237, %r18901, %r18896;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r18903, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18905, %r18903, %r18907;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18908, %r1237, %r18910, %r18905;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18912, %r1240, %r18883;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18915, %r1237, %r18880, %r18912;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18919, %r1240, %r18892;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18922, %r1237, %r18889, %r18919;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18926, %r1240, %r18901;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18929, %r1237, %r18898, %r18926;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r18933, %r1240, %r18910;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18936, %r1237, %r18907, %r18933;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18981, %r18985}, {%r1270, %r1264, %r1267, %r1261}, {%r18915, %r18881}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18989, %r18993}, {%r1270, %r1264, %r1267, %r1261}, {%r18922, %r18890}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r18997, %r19001}, {%r1270, %r1264, %r1267, %r1261}, {%r18929, %r18899}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19005, %r19009}, {%r1270, %r1264, %r1267, %r1261}, {%r18936, %r18908}, {%r18769, %r18769};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18980, %r18981, %r18981, %r29164;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29174, %r18985, %r18985, %r18980;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18988, %r18989, %r18989, %r29163;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29173, %r18993, %r18993, %r18988;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r18996, %r18997, %r18997, %r29162;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29172, %r19001, %r19001, %r18996;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19004, %r19005, %r19005, %r29161;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29171, %r19009, %r19009, %r19004;
	// end inline asm
	add.s32 	%r29169, %r29159, 1;
	setp.ne.s32 	%p135, %r29169, 40;
	@%p135 bra 	LBB0_162;
// %bb.159:                             // %L115968
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_161;
// %bb.160:                             // %L116033
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r19012, %r28856, 193536;
	add.s32 	%r19013, %r83, %r19012;
	or.b32  	%r19014, %r19013, %r43;
	add.s32 	%r19015, %r19014, %r84;
	mul.wide.u32 	%rd897, %r19015, 4;
	add.s64 	%rd898, %rd4, %rd897;
	st.global.u32 	[%rd898], %r29174;
	add.s32 	%r19016, %r19014, %r85;
	mul.wide.u32 	%rd899, %r19016, 4;
	add.s64 	%rd900, %rd4, %rd899;
	st.global.u32 	[%rd900], %r29173;
	cvt.u64.u32 	%rd901, %r84;
	cvt.u64.u32 	%rd902, %r43;
	cvt.u64.u32 	%rd903, %r83;
	cvt.u64.u32 	%rd904, %r19012;
	add.s64 	%rd905, %rd904, %rd903;
	add.s64 	%rd906, %rd905, %rd902;
	add.s64 	%rd907, %rd906, %rd901;
	shl.b64 	%rd908, %rd907, 2;
	add.s64 	%rd909, %rd4, %rd908;
	st.global.u32 	[%rd909+387072], %r29172;
	cvt.u64.u32 	%rd910, %r85;
	add.s64 	%rd911, %rd906, %rd910;
	shl.b64 	%rd912, %rd911, 2;
	add.s64 	%rd913, %rd4, %rd912;
	st.global.u32 	[%rd913+387072], %r29171;
LBB0_161:                               // %L116546
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29169, 0;
	mov.u32 	%r29171, %r29169;
	mov.u32 	%r29172, %r29169;
	mov.u32 	%r29173, %r29169;
	mov.u32 	%r29174, %r29169;
LBB0_162:                               // %L116547
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r19018, 0;
	mov.u32 	%r29175, %r19018;
	mov.u32 	%r29176, %r19018;
	mov.u32 	%r29177, %r19018;
	mov.u32 	%r29178, %r19018;
	@%p50 bra 	LBB0_291;
	bra.uni 	LBB0_163;
LBB0_291:                               // %oksrem40456
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r19019, %r77, %r631;
	add.s32 	%r19020, %r19019, %r78;
	add.s32 	%r19021, %r19020, %r79;
	add.s32 	%r19022, %r19021, %r28851;
	add.s32 	%r19023, %r19022, %r80;
	add.s32 	%r19024, %r19023, %r81;
	add.s32 	%r19025, %r19024, %r82;
	mul.wide.s32 	%rd914, %r19025, 4;
	add.s64 	%rd916, %rd52, %rd914;
	ld.shared.u32 	%r29175, [%rd916];
	or.b32  	%r19026, %r632, %r77;
	add.s32 	%r19027, %r19026, %r78;
	add.s32 	%r19028, %r19027, %r79;
	add.s32 	%r19029, %r19028, %r28851;
	add.s32 	%r19030, %r19029, %r80;
	add.s32 	%r19031, %r19030, %r81;
	add.s32 	%r19032, %r19031, %r82;
	mul.wide.u32 	%rd917, %r19032, 4;
	add.s64 	%rd918, %rd52, %rd917;
	ld.shared.u32 	%r29176, [%rd918];
	or.b32  	%r19033, %r19019, 32;
	add.s32 	%r19034, %r19033, %r78;
	add.s32 	%r19035, %r19034, %r79;
	add.s32 	%r19036, %r19035, %r28851;
	add.s32 	%r19037, %r19036, %r80;
	add.s32 	%r19038, %r19037, %r81;
	add.s32 	%r19039, %r19038, %r82;
	mul.wide.s32 	%rd919, %r19039, 4;
	add.s64 	%rd920, %rd52, %rd919;
	ld.shared.u32 	%r29177, [%rd920];
	add.s32 	%r19040, %r631, 8288;
	or.b32  	%r19041, %r19040, %r77;
	add.s32 	%r19042, %r19041, %r78;
	add.s32 	%r19043, %r19042, %r79;
	add.s32 	%r19044, %r19043, %r28851;
	add.s32 	%r19045, %r19044, %r80;
	add.s32 	%r19046, %r19045, %r81;
	add.s32 	%r19047, %r19046, %r82;
	mul.wide.u32 	%rd921, %r19047, 4;
	add.s64 	%rd922, %rd52, %rd921;
	ld.shared.u32 	%r29178, [%rd922];
LBB0_163:                               // %L117666
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19080, %r19083}, {%r1202, %r1199}, {%r29175}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19089, %r19092}, {%r1202, %r1199}, {%r29176}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19098, %r19101}, {%r1202, %r1199}, {%r29177}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19107, %r19110}, {%r1202, %r1199}, {%r29178}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19076, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19078, %r19076, %r19080;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19081, %r1237, %r19083, %r19078;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19085, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19087, %r19085, %r19089;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19090, %r1237, %r19092, %r19087;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19094, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19096, %r19094, %r19098;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19099, %r1237, %r19101, %r19096;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19103, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19105, %r19103, %r19107;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19108, %r1237, %r19110, %r19105;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19112, %r1240, %r19083;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19115, %r1237, %r19080, %r19112;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19119, %r1240, %r19092;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19122, %r1237, %r19089, %r19119;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19126, %r1240, %r19101;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19129, %r1237, %r19098, %r19126;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19133, %r1240, %r19110;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19136, %r1237, %r19107, %r19133;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19181, %r19185}, {%r1270, %r1264, %r1267, %r1261}, {%r19115, %r19081}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19189, %r19193}, {%r1270, %r1264, %r1267, %r1261}, {%r19122, %r19090}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19197, %r19201}, {%r1270, %r1264, %r1267, %r1261}, {%r19129, %r19099}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19205, %r19209}, {%r1270, %r1264, %r1267, %r1261}, {%r19136, %r19108}, {%r19018, %r19018};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19180, %r19181, %r19181, %r29174;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29184, %r19185, %r19185, %r19180;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19188, %r19189, %r19189, %r29173;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29183, %r19193, %r19193, %r19188;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19196, %r19197, %r19197, %r29172;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29182, %r19201, %r19201, %r19196;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19204, %r19205, %r19205, %r29171;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29181, %r19209, %r19209, %r19204;
	// end inline asm
	add.s32 	%r29179, %r29169, 1;
	setp.ne.s32 	%p138, %r29179, 40;
	@%p138 bra 	LBB0_167;
// %bb.164:                             // %L117849
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_166;
// %bb.165:                             // %L117914
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r19212, %r28856, 193536;
	add.s32 	%r19213, %r83, %r19212;
	or.b32  	%r19214, %r19213, %r43;
	add.s32 	%r19215, %r19214, %r84;
	mul.wide.u32 	%rd923, %r19215, 4;
	add.s64 	%rd924, %rd4, %rd923;
	st.global.u32 	[%rd924], %r29184;
	add.s32 	%r19216, %r19214, %r85;
	mul.wide.u32 	%rd925, %r19216, 4;
	add.s64 	%rd926, %rd4, %rd925;
	st.global.u32 	[%rd926], %r29183;
	cvt.u64.u32 	%rd927, %r84;
	cvt.u64.u32 	%rd928, %r43;
	cvt.u64.u32 	%rd929, %r83;
	cvt.u64.u32 	%rd930, %r19212;
	add.s64 	%rd931, %rd930, %rd929;
	add.s64 	%rd932, %rd931, %rd928;
	add.s64 	%rd933, %rd932, %rd927;
	shl.b64 	%rd934, %rd933, 2;
	add.s64 	%rd935, %rd4, %rd934;
	st.global.u32 	[%rd935+387072], %r29182;
	cvt.u64.u32 	%rd936, %r85;
	add.s64 	%rd937, %rd932, %rd936;
	shl.b64 	%rd938, %rd937, 2;
	add.s64 	%rd939, %rd4, %rd938;
	st.global.u32 	[%rd939+387072], %r29181;
LBB0_166:                               // %L118427
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29179, 0;
	mov.u32 	%r29181, %r29179;
	mov.u32 	%r29182, %r29179;
	mov.u32 	%r29183, %r29179;
	mov.u32 	%r29184, %r29179;
LBB0_167:                               // %L118428
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r19218, 0;
	mov.u32 	%r29185, %r19218;
	mov.u32 	%r29186, %r19218;
	mov.u32 	%r29187, %r19218;
	mov.u32 	%r29188, %r19218;
	@%p50 bra 	LBB0_292;
	bra.uni 	LBB0_168;
LBB0_292:                               // %oksrem41154
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r19219, %r77, %r633;
	add.s32 	%r19220, %r19219, %r78;
	add.s32 	%r19221, %r19220, %r79;
	add.s32 	%r19222, %r19221, %r28851;
	add.s32 	%r19223, %r19222, %r80;
	add.s32 	%r19224, %r19223, %r81;
	add.s32 	%r19225, %r19224, %r82;
	mul.wide.s32 	%rd940, %r19225, 4;
	add.s64 	%rd942, %rd52, %rd940;
	ld.shared.u32 	%r29185, [%rd942];
	or.b32  	%r19226, %r634, %r77;
	add.s32 	%r19227, %r19226, %r78;
	add.s32 	%r19228, %r19227, %r79;
	add.s32 	%r19229, %r19228, %r28851;
	add.s32 	%r19230, %r19229, %r80;
	add.s32 	%r19231, %r19230, %r81;
	add.s32 	%r19232, %r19231, %r82;
	mul.wide.u32 	%rd943, %r19232, 4;
	add.s64 	%rd944, %rd52, %rd943;
	ld.shared.u32 	%r29186, [%rd944];
	or.b32  	%r19233, %r19219, 32;
	add.s32 	%r19234, %r19233, %r78;
	add.s32 	%r19235, %r19234, %r79;
	add.s32 	%r19236, %r19235, %r28851;
	add.s32 	%r19237, %r19236, %r80;
	add.s32 	%r19238, %r19237, %r81;
	add.s32 	%r19239, %r19238, %r82;
	mul.wide.s32 	%rd945, %r19239, 4;
	add.s64 	%rd946, %rd52, %rd945;
	ld.shared.u32 	%r29187, [%rd946];
	add.s32 	%r19240, %r633, 8288;
	or.b32  	%r19241, %r19240, %r77;
	add.s32 	%r19242, %r19241, %r78;
	add.s32 	%r19243, %r19242, %r79;
	add.s32 	%r19244, %r19243, %r28851;
	add.s32 	%r19245, %r19244, %r80;
	add.s32 	%r19246, %r19245, %r81;
	add.s32 	%r19247, %r19246, %r82;
	mul.wide.u32 	%rd947, %r19247, 4;
	add.s64 	%rd948, %rd52, %rd947;
	ld.shared.u32 	%r29188, [%rd948];
LBB0_168:                               // %L119547
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19280, %r19283}, {%r1202, %r1199}, {%r29185}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19289, %r19292}, {%r1202, %r1199}, {%r29186}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19298, %r19301}, {%r1202, %r1199}, {%r29187}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19307, %r19310}, {%r1202, %r1199}, {%r29188}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19276, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19278, %r19276, %r19280;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19281, %r1237, %r19283, %r19278;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19285, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19287, %r19285, %r19289;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19290, %r1237, %r19292, %r19287;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19294, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19296, %r19294, %r19298;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19299, %r1237, %r19301, %r19296;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19303, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19305, %r19303, %r19307;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19308, %r1237, %r19310, %r19305;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19312, %r1240, %r19283;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19315, %r1237, %r19280, %r19312;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19319, %r1240, %r19292;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19322, %r1237, %r19289, %r19319;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19326, %r1240, %r19301;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19329, %r1237, %r19298, %r19326;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19333, %r1240, %r19310;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19336, %r1237, %r19307, %r19333;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19381, %r19385}, {%r1270, %r1264, %r1267, %r1261}, {%r19315, %r19281}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19389, %r19393}, {%r1270, %r1264, %r1267, %r1261}, {%r19322, %r19290}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19397, %r19401}, {%r1270, %r1264, %r1267, %r1261}, {%r19329, %r19299}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19405, %r19409}, {%r1270, %r1264, %r1267, %r1261}, {%r19336, %r19308}, {%r19218, %r19218};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19380, %r19381, %r19381, %r29184;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29194, %r19385, %r19385, %r19380;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19388, %r19389, %r19389, %r29183;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29193, %r19393, %r19393, %r19388;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19396, %r19397, %r19397, %r29182;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29192, %r19401, %r19401, %r19396;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19404, %r19405, %r19405, %r29181;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29191, %r19409, %r19409, %r19404;
	// end inline asm
	add.s32 	%r29189, %r29179, 1;
	setp.ne.s32 	%p141, %r29189, 40;
	@%p141 bra 	LBB0_172;
// %bb.169:                             // %L119730
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_171;
// %bb.170:                             // %L119795
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r19412, %r28856, 193536;
	add.s32 	%r19413, %r83, %r19412;
	or.b32  	%r19414, %r19413, %r43;
	add.s32 	%r19415, %r19414, %r84;
	mul.wide.u32 	%rd949, %r19415, 4;
	add.s64 	%rd950, %rd4, %rd949;
	st.global.u32 	[%rd950], %r29194;
	add.s32 	%r19416, %r19414, %r85;
	mul.wide.u32 	%rd951, %r19416, 4;
	add.s64 	%rd952, %rd4, %rd951;
	st.global.u32 	[%rd952], %r29193;
	cvt.u64.u32 	%rd953, %r84;
	cvt.u64.u32 	%rd954, %r43;
	cvt.u64.u32 	%rd955, %r83;
	cvt.u64.u32 	%rd956, %r19412;
	add.s64 	%rd957, %rd956, %rd955;
	add.s64 	%rd958, %rd957, %rd954;
	add.s64 	%rd959, %rd958, %rd953;
	shl.b64 	%rd960, %rd959, 2;
	add.s64 	%rd961, %rd4, %rd960;
	st.global.u32 	[%rd961+387072], %r29192;
	cvt.u64.u32 	%rd962, %r85;
	add.s64 	%rd963, %rd958, %rd962;
	shl.b64 	%rd964, %rd963, 2;
	add.s64 	%rd965, %rd4, %rd964;
	st.global.u32 	[%rd965+387072], %r29191;
LBB0_171:                               // %L120308
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29189, 0;
	mov.u32 	%r29191, %r29189;
	mov.u32 	%r29192, %r29189;
	mov.u32 	%r29193, %r29189;
	mov.u32 	%r29194, %r29189;
LBB0_172:                               // %L120309
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r19418, 0;
	mov.u32 	%r29195, %r19418;
	mov.u32 	%r29196, %r19418;
	mov.u32 	%r29197, %r19418;
	mov.u32 	%r29198, %r19418;
	@%p50 bra 	LBB0_293;
	bra.uni 	LBB0_173;
LBB0_293:                               // %oksrem41852
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r19419, %r77, %r635;
	add.s32 	%r19420, %r19419, %r78;
	add.s32 	%r19421, %r19420, %r79;
	add.s32 	%r19422, %r19421, %r28851;
	add.s32 	%r19423, %r19422, %r80;
	add.s32 	%r19424, %r19423, %r81;
	add.s32 	%r19425, %r19424, %r82;
	mul.wide.s32 	%rd966, %r19425, 4;
	add.s64 	%rd968, %rd52, %rd966;
	ld.shared.u32 	%r29195, [%rd968];
	or.b32  	%r19426, %r636, %r77;
	add.s32 	%r19427, %r19426, %r78;
	add.s32 	%r19428, %r19427, %r79;
	add.s32 	%r19429, %r19428, %r28851;
	add.s32 	%r19430, %r19429, %r80;
	add.s32 	%r19431, %r19430, %r81;
	add.s32 	%r19432, %r19431, %r82;
	mul.wide.u32 	%rd969, %r19432, 4;
	add.s64 	%rd970, %rd52, %rd969;
	ld.shared.u32 	%r29196, [%rd970];
	or.b32  	%r19433, %r19419, 32;
	add.s32 	%r19434, %r19433, %r78;
	add.s32 	%r19435, %r19434, %r79;
	add.s32 	%r19436, %r19435, %r28851;
	add.s32 	%r19437, %r19436, %r80;
	add.s32 	%r19438, %r19437, %r81;
	add.s32 	%r19439, %r19438, %r82;
	mul.wide.s32 	%rd971, %r19439, 4;
	add.s64 	%rd972, %rd52, %rd971;
	ld.shared.u32 	%r29197, [%rd972];
	add.s32 	%r19440, %r635, 8288;
	or.b32  	%r19441, %r19440, %r77;
	add.s32 	%r19442, %r19441, %r78;
	add.s32 	%r19443, %r19442, %r79;
	add.s32 	%r19444, %r19443, %r28851;
	add.s32 	%r19445, %r19444, %r80;
	add.s32 	%r19446, %r19445, %r81;
	add.s32 	%r19447, %r19446, %r82;
	mul.wide.u32 	%rd973, %r19447, 4;
	add.s64 	%rd974, %rd52, %rd973;
	ld.shared.u32 	%r29198, [%rd974];
LBB0_173:                               // %L121428
                                        //   in Loop: Header=BB0_16 Depth=1
	shr.u32 	%r20012, %r133, 8;
	shr.u32 	%r20058, %r135, 8;
	shr.u32 	%r20104, %r137, 8;
	shr.u32 	%r20150, %r139, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19480, %r19483}, {%r1202, %r1199}, {%r29195}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19489, %r19492}, {%r1202, %r1199}, {%r29196}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19498, %r19501}, {%r1202, %r1199}, {%r29197}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r19507, %r19510}, {%r1202, %r1199}, {%r29198}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19476, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19478, %r19476, %r19480;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19481, %r1237, %r19483, %r19478;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19485, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19487, %r19485, %r19489;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19490, %r1237, %r19492, %r19487;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19494, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19496, %r19494, %r19498;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19499, %r1237, %r19501, %r19496;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r19503, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19505, %r19503, %r19507;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19508, %r1237, %r19510, %r19505;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19512, %r1240, %r19483;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19515, %r1237, %r19480, %r19512;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19519, %r1240, %r19492;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19522, %r1237, %r19489, %r19519;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19526, %r1240, %r19501;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19529, %r1237, %r19498, %r19526;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r19533, %r1240, %r19510;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19536, %r1237, %r19507, %r19533;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19581, %r19585}, {%r1270, %r1264, %r1267, %r1261}, {%r19515, %r19481}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19589, %r19593}, {%r1270, %r1264, %r1267, %r1261}, {%r19522, %r19490}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19597, %r19601}, {%r1270, %r1264, %r1267, %r1261}, {%r19529, %r19499}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r19605, %r19609}, {%r1270, %r1264, %r1267, %r1261}, {%r19536, %r19508}, {%r19418, %r19418};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19580, %r19581, %r19581, %r29194;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29204, %r19585, %r19585, %r19580;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19588, %r19589, %r19589, %r29193;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29203, %r19593, %r19593, %r19588;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19596, %r19597, %r19597, %r29192;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29202, %r19601, %r19601, %r19596;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r19604, %r19605, %r19605, %r29191;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29201, %r19609, %r19609, %r19604;
	// end inline asm
	add.s32 	%r29199, %r29189, 1;
	setp.ne.s32 	%p144, %r29199, 40;
	@%p144 bra 	LBB0_177;
// %bb.174:                             // %L121611
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_176;
// %bb.175:                             // %L121676
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r19612, %r28856, 193536;
	add.s32 	%r19613, %r83, %r19612;
	or.b32  	%r19614, %r19613, %r43;
	add.s32 	%r19615, %r19614, %r84;
	mul.wide.u32 	%rd975, %r19615, 4;
	add.s64 	%rd976, %rd4, %rd975;
	st.global.u32 	[%rd976], %r29204;
	add.s32 	%r19616, %r19614, %r85;
	mul.wide.u32 	%rd977, %r19616, 4;
	add.s64 	%rd978, %rd4, %rd977;
	st.global.u32 	[%rd978], %r29203;
	cvt.u64.u32 	%rd979, %r84;
	cvt.u64.u32 	%rd980, %r43;
	cvt.u64.u32 	%rd981, %r83;
	cvt.u64.u32 	%rd982, %r19612;
	add.s64 	%rd983, %rd982, %rd981;
	add.s64 	%rd984, %rd983, %rd980;
	add.s64 	%rd985, %rd984, %rd979;
	shl.b64 	%rd986, %rd985, 2;
	add.s64 	%rd987, %rd4, %rd986;
	st.global.u32 	[%rd987+387072], %r29202;
	cvt.u64.u32 	%rd988, %r85;
	add.s64 	%rd989, %rd984, %rd988;
	shl.b64 	%rd990, %rd989, 2;
	add.s64 	%rd991, %rd4, %rd990;
	st.global.u32 	[%rd991+387072], %r29201;
LBB0_176:                               // %L122189
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29199, 0;
	mov.u32 	%r29201, %r29199;
	mov.u32 	%r29202, %r29199;
	mov.u32 	%r29203, %r29199;
	mov.u32 	%r29204, %r29199;
LBB0_177:                               // %L122190
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r20013, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r20024, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r20010, %r1979, %r20012, %r20013, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20014, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20015, %r20013, %r20014;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20018, %r20010, %r20015;
	// end inline asm
	// begin inline asm
	lop3.b32 %r20021, %r1990, %r20012, %r20024, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20025, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20026, %r20024, %r20025;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20029, %r20021, %r20026;
	// end inline asm
	// begin inline asm
	mov.b32 %r20059, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r20070, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r20056, %r1979, %r20058, %r20059, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20060, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20061, %r20059, %r20060;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20064, %r20056, %r20061;
	// end inline asm
	// begin inline asm
	lop3.b32 %r20067, %r1990, %r20058, %r20070, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20071, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20072, %r20070, %r20071;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20075, %r20067, %r20072;
	// end inline asm
	// begin inline asm
	mov.b32 %r20105, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r20116, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r20102, %r1979, %r20104, %r20105, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20106, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20107, %r20105, %r20106;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20110, %r20102, %r20107;
	// end inline asm
	// begin inline asm
	lop3.b32 %r20113, %r1990, %r20104, %r20116, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20117, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20118, %r20116, %r20117;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20121, %r20113, %r20118;
	// end inline asm
	// begin inline asm
	mov.b32 %r20151, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r20162, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r20148, %r1979, %r20150, %r20151, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20152, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20153, %r20151, %r20152;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20156, %r20148, %r20153;
	// end inline asm
	// begin inline asm
	lop3.b32 %r20159, %r1990, %r20150, %r20162, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r20163, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r20164, %r20162, %r20163;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r20167, %r20159, %r20164;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r20018;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20722, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r20029;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20725, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r20064;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20728, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r20075;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20731, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r20110;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20734, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r20121;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20737, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r20156;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20740, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r20167;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r20743, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r21074, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20806, %r20809}, {%r1202, %r1199}, {%r20722}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20815, %r20818}, {%r1202, %r1199}, {%r20725}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20824, %r20827}, {%r1202, %r1199}, {%r20728}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20833, %r20836}, {%r1202, %r1199}, {%r20731}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20842, %r20845}, {%r1202, %r1199}, {%r20734}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20851, %r20854}, {%r1202, %r1199}, {%r20737}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20860, %r20863}, {%r1202, %r1199}, {%r20740}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r20869, %r20872}, {%r1202, %r1199}, {%r20743}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20802, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20804, %r20802, %r20806;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20807, %r1237, %r20809, %r20804;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20811, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20813, %r20811, %r20815;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20816, %r1237, %r20818, %r20813;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20820, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20822, %r20820, %r20824;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20825, %r1237, %r20827, %r20822;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20829, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20831, %r20829, %r20833;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20834, %r1237, %r20836, %r20831;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20838, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20840, %r20838, %r20842;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20843, %r1237, %r20845, %r20840;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20847, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20849, %r20847, %r20851;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20852, %r1237, %r20854, %r20849;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20856, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20858, %r20856, %r20860;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20861, %r1237, %r20863, %r20858;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r20865, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20867, %r20865, %r20869;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20870, %r1237, %r20872, %r20867;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20874, %r1240, %r20809;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20877, %r1237, %r20806, %r20874;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20881, %r1240, %r20818;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20884, %r1237, %r20815, %r20881;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20888, %r1240, %r20827;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20891, %r1237, %r20824, %r20888;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20895, %r1240, %r20836;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20898, %r1237, %r20833, %r20895;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20902, %r1240, %r20845;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20905, %r1237, %r20842, %r20902;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20909, %r1240, %r20854;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20912, %r1237, %r20851, %r20909;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20916, %r1240, %r20863;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20919, %r1237, %r20860, %r20916;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r20923, %r1240, %r20872;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r20926, %r1237, %r20869, %r20923;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21011, %r21012}, {%r1270, %r1264, %r1267, %r1261}, {%r20877, %r20807}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21019, %r21020}, {%r1270, %r1264, %r1267, %r1261}, {%r20884, %r20816}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21027, %r21028}, {%r1270, %r1264, %r1267, %r1261}, {%r20891, %r20825}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21035, %r21036}, {%r1270, %r1264, %r1267, %r1261}, {%r20898, %r20834}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21043, %r21044}, {%r1270, %r1264, %r1267, %r1261}, {%r20905, %r20843}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21051, %r21052}, {%r1270, %r1264, %r1267, %r1261}, {%r20912, %r20852}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21059, %r21060}, {%r1270, %r1264, %r1267, %r1261}, {%r20919, %r20861}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21067, %r21068}, {%r1270, %r1264, %r1267, %r1261}, {%r20926, %r20870}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	prmt.b32 %r21010, %r21011, %r21012, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21014, %r21011, %r21012, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21018, %r21019, %r21020, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21022, %r21019, %r21020, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21026, %r21027, %r21028, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21030, %r21027, %r21028, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21034, %r21035, %r21036, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21038, %r21035, %r21036, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21042, %r21043, %r21044, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21046, %r21043, %r21044, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21050, %r21051, %r21052, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21054, %r21051, %r21052, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21058, %r21059, %r21060, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21062, %r21059, %r21060, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21066, %r21067, %r21068, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r21070, %r21067, %r21068, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r21010;
	st.shared.u32 	[%rd7+33024], %r21014;
	st.shared.u32 	[%rd8], %r21018;
	st.shared.u32 	[%rd9], %r21022;
	add.s16 	%rs2842, %rs3785, 10;
	shr.s16 	%rs2843, %rs2842, 15;
	shr.u16 	%rs2844, %rs2843, 14;
	add.s16 	%rs2845, %rs2842, %rs2844;
	and.b16  	%rs2846, %rs2845, -4;
	sub.s16 	%rs2847, %rs2842, %rs2846;
	mul.wide.s16 	%r701, %rs2847, 64;
	add.s32 	%r21075, %r71, %r701;
	add.s32 	%r21076, %r21075, %r28848;
	add.s32 	%r21077, %r21076, %r72;
	add.s32 	%r21078, %r21077, %r28849;
	add.s32 	%r21079, %r21078, %r73;
	add.s32 	%r21080, %r21079, %r74;
	add.s32 	%r21081, %r21080, %r75;
	mul.wide.s32 	%rd992, %r21081, 4;
	add.s64 	%rd994, %rd52, %rd992;
	st.shared.u32 	[%rd994], %r21026;
	add.s32 	%r702, %r701, 8256;
	add.s32 	%r21082, %r702, %r71;
	add.s32 	%r21083, %r21082, %r28848;
	add.s32 	%r21084, %r21083, %r72;
	add.s32 	%r21085, %r21084, %r28849;
	add.s32 	%r21086, %r21085, %r73;
	add.s32 	%r21087, %r21086, %r74;
	add.s32 	%r21088, %r21087, %r75;
	mul.wide.u32 	%rd995, %r21088, 4;
	add.s64 	%rd996, %rd52, %rd995;
	st.shared.u32 	[%rd996], %r21030;
	add.s32 	%r21089, %r21075, %r76;
	add.s32 	%r21090, %r21089, %r72;
	add.s32 	%r21091, %r21090, %r28849;
	add.s32 	%r21092, %r21091, %r73;
	add.s32 	%r21093, %r21092, %r74;
	add.s32 	%r21094, %r21093, %r75;
	mul.wide.s32 	%rd997, %r21094, 4;
	add.s64 	%rd998, %rd52, %rd997;
	st.shared.u32 	[%rd998], %r21034;
	add.s32 	%r21095, %r21082, %r76;
	add.s32 	%r21096, %r21095, %r72;
	add.s32 	%r21097, %r21096, %r28849;
	add.s32 	%r21098, %r21097, %r73;
	add.s32 	%r21099, %r21098, %r74;
	add.s32 	%r21100, %r21099, %r75;
	mul.wide.s32 	%rd999, %r21100, 4;
	add.s64 	%rd1000, %rd52, %rd999;
	st.shared.u32 	[%rd1000], %r21038;
	add.s16 	%rs2848, %rs3785, 11;
	shr.s16 	%rs2849, %rs2848, 15;
	shr.u16 	%rs2850, %rs2849, 14;
	add.s16 	%rs2851, %rs2848, %rs2850;
	and.b16  	%rs2852, %rs2851, -4;
	sub.s16 	%rs2853, %rs2848, %rs2852;
	mul.wide.s16 	%r703, %rs2853, 64;
	add.s32 	%r21101, %r71, %r703;
	add.s32 	%r21102, %r21101, %r28848;
	add.s32 	%r21103, %r21102, %r72;
	add.s32 	%r21104, %r21103, %r28849;
	add.s32 	%r21105, %r21104, %r73;
	add.s32 	%r21106, %r21105, %r74;
	add.s32 	%r21107, %r21106, %r75;
	mul.wide.s32 	%rd1001, %r21107, 4;
	add.s64 	%rd1002, %rd52, %rd1001;
	st.shared.u32 	[%rd1002], %r21042;
	add.s32 	%r704, %r703, 8256;
	add.s32 	%r21108, %r704, %r71;
	add.s32 	%r21109, %r21108, %r28848;
	add.s32 	%r21110, %r21109, %r72;
	add.s32 	%r21111, %r21110, %r28849;
	add.s32 	%r21112, %r21111, %r73;
	add.s32 	%r21113, %r21112, %r74;
	add.s32 	%r21114, %r21113, %r75;
	mul.wide.u32 	%rd1003, %r21114, 4;
	add.s64 	%rd1004, %rd52, %rd1003;
	st.shared.u32 	[%rd1004], %r21046;
	add.s32 	%r21115, %r21101, %r76;
	add.s32 	%r21116, %r21115, %r72;
	add.s32 	%r21117, %r21116, %r28849;
	add.s32 	%r21118, %r21117, %r73;
	add.s32 	%r21119, %r21118, %r74;
	add.s32 	%r21120, %r21119, %r75;
	mul.wide.s32 	%rd1005, %r21120, 4;
	add.s64 	%rd1006, %rd52, %rd1005;
	st.shared.u32 	[%rd1006], %r21050;
	add.s32 	%r21121, %r21108, %r76;
	add.s32 	%r21122, %r21121, %r72;
	add.s32 	%r21123, %r21122, %r28849;
	add.s32 	%r21124, %r21123, %r73;
	add.s32 	%r21125, %r21124, %r74;
	add.s32 	%r21126, %r21125, %r75;
	mul.wide.s32 	%rd1007, %r21126, 4;
	add.s64 	%rd1008, %rd52, %rd1007;
	st.shared.u32 	[%rd1008], %r21054;
	add.s16 	%rs2854, %rs3785, 12;
	shr.s16 	%rs2855, %rs2854, 15;
	shr.u16 	%rs2856, %rs2855, 14;
	add.s16 	%rs2857, %rs2854, %rs2856;
	and.b16  	%rs2858, %rs2857, -4;
	sub.s16 	%rs2859, %rs2854, %rs2858;
	mul.wide.s16 	%r705, %rs2859, 64;
	add.s32 	%r21127, %r71, %r705;
	add.s32 	%r21128, %r21127, %r28848;
	add.s32 	%r21129, %r21128, %r72;
	add.s32 	%r21130, %r21129, %r28849;
	add.s32 	%r21131, %r21130, %r73;
	add.s32 	%r21132, %r21131, %r74;
	add.s32 	%r21133, %r21132, %r75;
	mul.wide.s32 	%rd1009, %r21133, 4;
	add.s64 	%rd1010, %rd52, %rd1009;
	st.shared.u32 	[%rd1010], %r21058;
	add.s32 	%r706, %r705, 8256;
	add.s32 	%r21134, %r706, %r71;
	add.s32 	%r21135, %r21134, %r28848;
	add.s32 	%r21136, %r21135, %r72;
	add.s32 	%r21137, %r21136, %r28849;
	add.s32 	%r21138, %r21137, %r73;
	add.s32 	%r21139, %r21138, %r74;
	add.s32 	%r21140, %r21139, %r75;
	mul.wide.u32 	%rd1011, %r21140, 4;
	add.s64 	%rd1012, %rd52, %rd1011;
	st.shared.u32 	[%rd1012], %r21062;
	add.s32 	%r21141, %r21127, %r76;
	add.s32 	%r21142, %r21141, %r72;
	add.s32 	%r21143, %r21142, %r28849;
	add.s32 	%r21144, %r21143, %r73;
	add.s32 	%r21145, %r21144, %r74;
	add.s32 	%r21146, %r21145, %r75;
	mul.wide.s32 	%rd1013, %r21146, 4;
	add.s64 	%rd1014, %rd52, %rd1013;
	st.shared.u32 	[%rd1014], %r21066;
	add.s32 	%r21147, %r21134, %r76;
	add.s32 	%r21148, %r21147, %r72;
	add.s32 	%r21149, %r21148, %r28849;
	add.s32 	%r21150, %r21149, %r73;
	add.s32 	%r21151, %r21150, %r74;
	add.s32 	%r21152, %r21151, %r75;
	mul.wide.s32 	%rd1015, %r21152, 4;
	add.s64 	%rd1016, %rd52, %rd1015;
	st.shared.u32 	[%rd1016], %r21070;
	bar.sync 	0;
	mov.u32 	%r29205, %r21074;
	mov.u32 	%r29206, %r21074;
	mov.u32 	%r29207, %r21074;
	mov.u32 	%r29208, %r21074;
	@%p50 bra 	LBB0_294;
	bra.uni 	LBB0_178;
LBB0_294:                               // %oksrem44454
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29205, [%rd10];
	ld.shared.u32 	%r29206, [%rd11+33024];
	ld.shared.u32 	%r29207, [%rd11+128];
	ld.shared.u32 	%r29208, [%rd11+33152];
LBB0_178:                               // %L129459
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21185, %r21188}, {%r1202, %r1199}, {%r29205}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21194, %r21197}, {%r1202, %r1199}, {%r29206}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21203, %r21206}, {%r1202, %r1199}, {%r29207}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21212, %r21215}, {%r1202, %r1199}, {%r29208}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21181, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21183, %r21181, %r21185;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21186, %r1237, %r21188, %r21183;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21190, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21192, %r21190, %r21194;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21195, %r1237, %r21197, %r21192;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21199, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21201, %r21199, %r21203;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21204, %r1237, %r21206, %r21201;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21208, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21210, %r21208, %r21212;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21213, %r1237, %r21215, %r21210;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21217, %r1240, %r21188;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21220, %r1237, %r21185, %r21217;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21224, %r1240, %r21197;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21227, %r1237, %r21194, %r21224;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21231, %r1240, %r21206;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21234, %r1237, %r21203, %r21231;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21238, %r1240, %r21215;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21241, %r1237, %r21212, %r21238;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21286, %r21290}, {%r1270, %r1264, %r1267, %r1261}, {%r21220, %r21186}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21294, %r21298}, {%r1270, %r1264, %r1267, %r1261}, {%r21227, %r21195}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21302, %r21306}, {%r1270, %r1264, %r1267, %r1261}, {%r21234, %r21204}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21310, %r21314}, {%r1270, %r1264, %r1267, %r1261}, {%r21241, %r21213}, {%r21074, %r21074};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21285, %r21286, %r21286, %r29204;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29214, %r21290, %r21290, %r21285;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21293, %r21294, %r21294, %r29203;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29213, %r21298, %r21298, %r21293;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21301, %r21302, %r21302, %r29202;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29212, %r21306, %r21306, %r21301;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21309, %r21310, %r21310, %r29201;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29211, %r21314, %r21314, %r21309;
	// end inline asm
	add.s32 	%r29209, %r29199, 1;
	setp.ne.s32 	%p147, %r29209, 40;
	@%p147 bra 	LBB0_182;
// %bb.179:                             // %L129642
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_181;
// %bb.180:                             // %L129707
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r21317, %r28856, 193536;
	add.s32 	%r21318, %r83, %r21317;
	or.b32  	%r21319, %r21318, %r43;
	add.s32 	%r21320, %r21319, %r84;
	mul.wide.u32 	%rd1017, %r21320, 4;
	add.s64 	%rd1018, %rd4, %rd1017;
	st.global.u32 	[%rd1018], %r29214;
	add.s32 	%r21321, %r21319, %r85;
	mul.wide.u32 	%rd1019, %r21321, 4;
	add.s64 	%rd1020, %rd4, %rd1019;
	st.global.u32 	[%rd1020], %r29213;
	cvt.u64.u32 	%rd1021, %r84;
	cvt.u64.u32 	%rd1022, %r43;
	cvt.u64.u32 	%rd1023, %r83;
	cvt.u64.u32 	%rd1024, %r21317;
	add.s64 	%rd1025, %rd1024, %rd1023;
	add.s64 	%rd1026, %rd1025, %rd1022;
	add.s64 	%rd1027, %rd1026, %rd1021;
	shl.b64 	%rd1028, %rd1027, 2;
	add.s64 	%rd1029, %rd4, %rd1028;
	st.global.u32 	[%rd1029+387072], %r29212;
	cvt.u64.u32 	%rd1030, %r85;
	add.s64 	%rd1031, %rd1026, %rd1030;
	shl.b64 	%rd1032, %rd1031, 2;
	add.s64 	%rd1033, %rd4, %rd1032;
	st.global.u32 	[%rd1033+387072], %r29211;
LBB0_181:                               // %L130220
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29209, 0;
	mov.u32 	%r29211, %r29209;
	mov.u32 	%r29212, %r29209;
	mov.u32 	%r29213, %r29209;
	mov.u32 	%r29214, %r29209;
LBB0_182:                               // %L130221
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r21323, 0;
	mov.u32 	%r29215, %r21323;
	mov.u32 	%r29216, %r21323;
	mov.u32 	%r29217, %r21323;
	mov.u32 	%r29218, %r21323;
	@%p50 bra 	LBB0_295;
	bra.uni 	LBB0_183;
LBB0_295:                               // %oksrem45152
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r21324, %r77, %r701;
	add.s32 	%r21325, %r21324, %r78;
	add.s32 	%r21326, %r21325, %r79;
	add.s32 	%r21327, %r21326, %r28851;
	add.s32 	%r21328, %r21327, %r80;
	add.s32 	%r21329, %r21328, %r81;
	add.s32 	%r21330, %r21329, %r82;
	mul.wide.s32 	%rd1034, %r21330, 4;
	add.s64 	%rd1036, %rd52, %rd1034;
	ld.shared.u32 	%r29215, [%rd1036];
	or.b32  	%r21331, %r702, %r77;
	add.s32 	%r21332, %r21331, %r78;
	add.s32 	%r21333, %r21332, %r79;
	add.s32 	%r21334, %r21333, %r28851;
	add.s32 	%r21335, %r21334, %r80;
	add.s32 	%r21336, %r21335, %r81;
	add.s32 	%r21337, %r21336, %r82;
	mul.wide.u32 	%rd1037, %r21337, 4;
	add.s64 	%rd1038, %rd52, %rd1037;
	ld.shared.u32 	%r29216, [%rd1038];
	or.b32  	%r21338, %r21324, 32;
	add.s32 	%r21339, %r21338, %r78;
	add.s32 	%r21340, %r21339, %r79;
	add.s32 	%r21341, %r21340, %r28851;
	add.s32 	%r21342, %r21341, %r80;
	add.s32 	%r21343, %r21342, %r81;
	add.s32 	%r21344, %r21343, %r82;
	mul.wide.s32 	%rd1039, %r21344, 4;
	add.s64 	%rd1040, %rd52, %rd1039;
	ld.shared.u32 	%r29217, [%rd1040];
	add.s32 	%r21345, %r701, 8288;
	or.b32  	%r21346, %r21345, %r77;
	add.s32 	%r21347, %r21346, %r78;
	add.s32 	%r21348, %r21347, %r79;
	add.s32 	%r21349, %r21348, %r28851;
	add.s32 	%r21350, %r21349, %r80;
	add.s32 	%r21351, %r21350, %r81;
	add.s32 	%r21352, %r21351, %r82;
	mul.wide.u32 	%rd1041, %r21352, 4;
	add.s64 	%rd1042, %rd52, %rd1041;
	ld.shared.u32 	%r29218, [%rd1042];
LBB0_183:                               // %L131340
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21385, %r21388}, {%r1202, %r1199}, {%r29215}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21394, %r21397}, {%r1202, %r1199}, {%r29216}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21403, %r21406}, {%r1202, %r1199}, {%r29217}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21412, %r21415}, {%r1202, %r1199}, {%r29218}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21381, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21383, %r21381, %r21385;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21386, %r1237, %r21388, %r21383;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21390, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21392, %r21390, %r21394;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21395, %r1237, %r21397, %r21392;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21399, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21401, %r21399, %r21403;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21404, %r1237, %r21406, %r21401;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21408, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21410, %r21408, %r21412;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21413, %r1237, %r21415, %r21410;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21417, %r1240, %r21388;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21420, %r1237, %r21385, %r21417;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21424, %r1240, %r21397;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21427, %r1237, %r21394, %r21424;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21431, %r1240, %r21406;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21434, %r1237, %r21403, %r21431;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21438, %r1240, %r21415;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21441, %r1237, %r21412, %r21438;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21486, %r21490}, {%r1270, %r1264, %r1267, %r1261}, {%r21420, %r21386}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21494, %r21498}, {%r1270, %r1264, %r1267, %r1261}, {%r21427, %r21395}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21502, %r21506}, {%r1270, %r1264, %r1267, %r1261}, {%r21434, %r21404}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21510, %r21514}, {%r1270, %r1264, %r1267, %r1261}, {%r21441, %r21413}, {%r21323, %r21323};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21485, %r21486, %r21486, %r29214;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29224, %r21490, %r21490, %r21485;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21493, %r21494, %r21494, %r29213;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29223, %r21498, %r21498, %r21493;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21501, %r21502, %r21502, %r29212;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29222, %r21506, %r21506, %r21501;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21509, %r21510, %r21510, %r29211;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29221, %r21514, %r21514, %r21509;
	// end inline asm
	add.s32 	%r29219, %r29209, 1;
	setp.ne.s32 	%p150, %r29219, 40;
	@%p150 bra 	LBB0_187;
// %bb.184:                             // %L131523
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_186;
// %bb.185:                             // %L131588
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r21517, %r28856, 193536;
	add.s32 	%r21518, %r83, %r21517;
	or.b32  	%r21519, %r21518, %r43;
	add.s32 	%r21520, %r21519, %r84;
	mul.wide.u32 	%rd1043, %r21520, 4;
	add.s64 	%rd1044, %rd4, %rd1043;
	st.global.u32 	[%rd1044], %r29224;
	add.s32 	%r21521, %r21519, %r85;
	mul.wide.u32 	%rd1045, %r21521, 4;
	add.s64 	%rd1046, %rd4, %rd1045;
	st.global.u32 	[%rd1046], %r29223;
	cvt.u64.u32 	%rd1047, %r84;
	cvt.u64.u32 	%rd1048, %r43;
	cvt.u64.u32 	%rd1049, %r83;
	cvt.u64.u32 	%rd1050, %r21517;
	add.s64 	%rd1051, %rd1050, %rd1049;
	add.s64 	%rd1052, %rd1051, %rd1048;
	add.s64 	%rd1053, %rd1052, %rd1047;
	shl.b64 	%rd1054, %rd1053, 2;
	add.s64 	%rd1055, %rd4, %rd1054;
	st.global.u32 	[%rd1055+387072], %r29222;
	cvt.u64.u32 	%rd1056, %r85;
	add.s64 	%rd1057, %rd1052, %rd1056;
	shl.b64 	%rd1058, %rd1057, 2;
	add.s64 	%rd1059, %rd4, %rd1058;
	st.global.u32 	[%rd1059+387072], %r29221;
LBB0_186:                               // %L132101
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29219, 0;
	mov.u32 	%r29221, %r29219;
	mov.u32 	%r29222, %r29219;
	mov.u32 	%r29223, %r29219;
	mov.u32 	%r29224, %r29219;
LBB0_187:                               // %L132102
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r21523, 0;
	mov.u32 	%r29225, %r21523;
	mov.u32 	%r29226, %r21523;
	mov.u32 	%r29227, %r21523;
	mov.u32 	%r29228, %r21523;
	@%p50 bra 	LBB0_296;
	bra.uni 	LBB0_188;
LBB0_296:                               // %oksrem45850
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r21524, %r77, %r703;
	add.s32 	%r21525, %r21524, %r78;
	add.s32 	%r21526, %r21525, %r79;
	add.s32 	%r21527, %r21526, %r28851;
	add.s32 	%r21528, %r21527, %r80;
	add.s32 	%r21529, %r21528, %r81;
	add.s32 	%r21530, %r21529, %r82;
	mul.wide.s32 	%rd1060, %r21530, 4;
	add.s64 	%rd1062, %rd52, %rd1060;
	ld.shared.u32 	%r29225, [%rd1062];
	or.b32  	%r21531, %r704, %r77;
	add.s32 	%r21532, %r21531, %r78;
	add.s32 	%r21533, %r21532, %r79;
	add.s32 	%r21534, %r21533, %r28851;
	add.s32 	%r21535, %r21534, %r80;
	add.s32 	%r21536, %r21535, %r81;
	add.s32 	%r21537, %r21536, %r82;
	mul.wide.u32 	%rd1063, %r21537, 4;
	add.s64 	%rd1064, %rd52, %rd1063;
	ld.shared.u32 	%r29226, [%rd1064];
	or.b32  	%r21538, %r21524, 32;
	add.s32 	%r21539, %r21538, %r78;
	add.s32 	%r21540, %r21539, %r79;
	add.s32 	%r21541, %r21540, %r28851;
	add.s32 	%r21542, %r21541, %r80;
	add.s32 	%r21543, %r21542, %r81;
	add.s32 	%r21544, %r21543, %r82;
	mul.wide.s32 	%rd1065, %r21544, 4;
	add.s64 	%rd1066, %rd52, %rd1065;
	ld.shared.u32 	%r29227, [%rd1066];
	add.s32 	%r21545, %r703, 8288;
	or.b32  	%r21546, %r21545, %r77;
	add.s32 	%r21547, %r21546, %r78;
	add.s32 	%r21548, %r21547, %r79;
	add.s32 	%r21549, %r21548, %r28851;
	add.s32 	%r21550, %r21549, %r80;
	add.s32 	%r21551, %r21550, %r81;
	add.s32 	%r21552, %r21551, %r82;
	mul.wide.u32 	%rd1067, %r21552, 4;
	add.s64 	%rd1068, %rd52, %rd1067;
	ld.shared.u32 	%r29228, [%rd1068];
LBB0_188:                               // %L133221
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21585, %r21588}, {%r1202, %r1199}, {%r29225}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21594, %r21597}, {%r1202, %r1199}, {%r29226}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21603, %r21606}, {%r1202, %r1199}, {%r29227}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21612, %r21615}, {%r1202, %r1199}, {%r29228}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21581, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21583, %r21581, %r21585;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21586, %r1237, %r21588, %r21583;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21590, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21592, %r21590, %r21594;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21595, %r1237, %r21597, %r21592;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21599, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21601, %r21599, %r21603;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21604, %r1237, %r21606, %r21601;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21608, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21610, %r21608, %r21612;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21613, %r1237, %r21615, %r21610;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21617, %r1240, %r21588;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21620, %r1237, %r21585, %r21617;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21624, %r1240, %r21597;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21627, %r1237, %r21594, %r21624;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21631, %r1240, %r21606;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21634, %r1237, %r21603, %r21631;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21638, %r1240, %r21615;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21641, %r1237, %r21612, %r21638;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21686, %r21690}, {%r1270, %r1264, %r1267, %r1261}, {%r21620, %r21586}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21694, %r21698}, {%r1270, %r1264, %r1267, %r1261}, {%r21627, %r21595}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21702, %r21706}, {%r1270, %r1264, %r1267, %r1261}, {%r21634, %r21604}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21710, %r21714}, {%r1270, %r1264, %r1267, %r1261}, {%r21641, %r21613}, {%r21523, %r21523};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21685, %r21686, %r21686, %r29224;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29234, %r21690, %r21690, %r21685;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21693, %r21694, %r21694, %r29223;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29233, %r21698, %r21698, %r21693;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21701, %r21702, %r21702, %r29222;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29232, %r21706, %r21706, %r21701;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21709, %r21710, %r21710, %r29221;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29231, %r21714, %r21714, %r21709;
	// end inline asm
	add.s32 	%r29229, %r29219, 1;
	setp.ne.s32 	%p153, %r29229, 40;
	@%p153 bra 	LBB0_192;
// %bb.189:                             // %L133404
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_191;
// %bb.190:                             // %L133469
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r21717, %r28856, 193536;
	add.s32 	%r21718, %r83, %r21717;
	or.b32  	%r21719, %r21718, %r43;
	add.s32 	%r21720, %r21719, %r84;
	mul.wide.u32 	%rd1069, %r21720, 4;
	add.s64 	%rd1070, %rd4, %rd1069;
	st.global.u32 	[%rd1070], %r29234;
	add.s32 	%r21721, %r21719, %r85;
	mul.wide.u32 	%rd1071, %r21721, 4;
	add.s64 	%rd1072, %rd4, %rd1071;
	st.global.u32 	[%rd1072], %r29233;
	cvt.u64.u32 	%rd1073, %r84;
	cvt.u64.u32 	%rd1074, %r43;
	cvt.u64.u32 	%rd1075, %r83;
	cvt.u64.u32 	%rd1076, %r21717;
	add.s64 	%rd1077, %rd1076, %rd1075;
	add.s64 	%rd1078, %rd1077, %rd1074;
	add.s64 	%rd1079, %rd1078, %rd1073;
	shl.b64 	%rd1080, %rd1079, 2;
	add.s64 	%rd1081, %rd4, %rd1080;
	st.global.u32 	[%rd1081+387072], %r29232;
	cvt.u64.u32 	%rd1082, %r85;
	add.s64 	%rd1083, %rd1078, %rd1082;
	shl.b64 	%rd1084, %rd1083, 2;
	add.s64 	%rd1085, %rd4, %rd1084;
	st.global.u32 	[%rd1085+387072], %r29231;
LBB0_191:                               // %L133982
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29229, 0;
	mov.u32 	%r29231, %r29229;
	mov.u32 	%r29232, %r29229;
	mov.u32 	%r29233, %r29229;
	mov.u32 	%r29234, %r29229;
LBB0_192:                               // %L133983
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r21723, 0;
	mov.u32 	%r29235, %r21723;
	mov.u32 	%r29236, %r21723;
	mov.u32 	%r29237, %r21723;
	mov.u32 	%r29238, %r21723;
	@%p50 bra 	LBB0_297;
	bra.uni 	LBB0_193;
LBB0_297:                               // %oksrem46548
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r21724, %r77, %r705;
	add.s32 	%r21725, %r21724, %r78;
	add.s32 	%r21726, %r21725, %r79;
	add.s32 	%r21727, %r21726, %r28851;
	add.s32 	%r21728, %r21727, %r80;
	add.s32 	%r21729, %r21728, %r81;
	add.s32 	%r21730, %r21729, %r82;
	mul.wide.s32 	%rd1086, %r21730, 4;
	add.s64 	%rd1088, %rd52, %rd1086;
	ld.shared.u32 	%r29235, [%rd1088];
	or.b32  	%r21731, %r706, %r77;
	add.s32 	%r21732, %r21731, %r78;
	add.s32 	%r21733, %r21732, %r79;
	add.s32 	%r21734, %r21733, %r28851;
	add.s32 	%r21735, %r21734, %r80;
	add.s32 	%r21736, %r21735, %r81;
	add.s32 	%r21737, %r21736, %r82;
	mul.wide.u32 	%rd1089, %r21737, 4;
	add.s64 	%rd1090, %rd52, %rd1089;
	ld.shared.u32 	%r29236, [%rd1090];
	or.b32  	%r21738, %r21724, 32;
	add.s32 	%r21739, %r21738, %r78;
	add.s32 	%r21740, %r21739, %r79;
	add.s32 	%r21741, %r21740, %r28851;
	add.s32 	%r21742, %r21741, %r80;
	add.s32 	%r21743, %r21742, %r81;
	add.s32 	%r21744, %r21743, %r82;
	mul.wide.s32 	%rd1091, %r21744, 4;
	add.s64 	%rd1092, %rd52, %rd1091;
	ld.shared.u32 	%r29237, [%rd1092];
	add.s32 	%r21745, %r705, 8288;
	or.b32  	%r21746, %r21745, %r77;
	add.s32 	%r21747, %r21746, %r78;
	add.s32 	%r21748, %r21747, %r79;
	add.s32 	%r21749, %r21748, %r28851;
	add.s32 	%r21750, %r21749, %r80;
	add.s32 	%r21751, %r21750, %r81;
	add.s32 	%r21752, %r21751, %r82;
	mul.wide.u32 	%rd1093, %r21752, 4;
	add.s64 	%rd1094, %rd52, %rd1093;
	ld.shared.u32 	%r29238, [%rd1094];
LBB0_193:                               // %L135102
                                        //   in Loop: Header=BB0_16 Depth=1
	shr.u32 	%r22501, %r141, 8;
	shr.u32 	%r22547, %r143, 8;
	shr.u32 	%r22593, %r145, 8;
	shr.u32 	%r22639, %r147, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21785, %r21788}, {%r1202, %r1199}, {%r29235}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21794, %r21797}, {%r1202, %r1199}, {%r29236}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21803, %r21806}, {%r1202, %r1199}, {%r29237}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r21812, %r21815}, {%r1202, %r1199}, {%r29238}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21781, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21783, %r21781, %r21785;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21786, %r1237, %r21788, %r21783;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21790, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21792, %r21790, %r21794;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21795, %r1237, %r21797, %r21792;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21799, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21801, %r21799, %r21803;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21804, %r1237, %r21806, %r21801;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r21808, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21810, %r21808, %r21812;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21813, %r1237, %r21815, %r21810;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21817, %r1240, %r21788;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21820, %r1237, %r21785, %r21817;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21824, %r1240, %r21797;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21827, %r1237, %r21794, %r21824;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21831, %r1240, %r21806;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21834, %r1237, %r21803, %r21831;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r21838, %r1240, %r21815;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21841, %r1237, %r21812, %r21838;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21886, %r21890}, {%r1270, %r1264, %r1267, %r1261}, {%r21820, %r21786}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21894, %r21898}, {%r1270, %r1264, %r1267, %r1261}, {%r21827, %r21795}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21902, %r21906}, {%r1270, %r1264, %r1267, %r1261}, {%r21834, %r21804}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r21910, %r21914}, {%r1270, %r1264, %r1267, %r1261}, {%r21841, %r21813}, {%r21723, %r21723};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21885, %r21886, %r21886, %r29234;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29244, %r21890, %r21890, %r21885;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21893, %r21894, %r21894, %r29233;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29243, %r21898, %r21898, %r21893;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21901, %r21902, %r21902, %r29232;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29242, %r21906, %r21906, %r21901;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r21909, %r21910, %r21910, %r29231;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29241, %r21914, %r21914, %r21909;
	// end inline asm
	add.s32 	%r29239, %r29229, 1;
	setp.ne.s32 	%p156, %r29239, 40;
	@%p156 bra 	LBB0_197;
// %bb.194:                             // %L135285
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_196;
// %bb.195:                             // %L135350
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r21917, %r28856, 193536;
	add.s32 	%r21918, %r83, %r21917;
	or.b32  	%r21919, %r21918, %r43;
	add.s32 	%r21920, %r21919, %r84;
	mul.wide.u32 	%rd1095, %r21920, 4;
	add.s64 	%rd1096, %rd4, %rd1095;
	st.global.u32 	[%rd1096], %r29244;
	add.s32 	%r21921, %r21919, %r85;
	mul.wide.u32 	%rd1097, %r21921, 4;
	add.s64 	%rd1098, %rd4, %rd1097;
	st.global.u32 	[%rd1098], %r29243;
	cvt.u64.u32 	%rd1099, %r84;
	cvt.u64.u32 	%rd1100, %r43;
	cvt.u64.u32 	%rd1101, %r83;
	cvt.u64.u32 	%rd1102, %r21917;
	add.s64 	%rd1103, %rd1102, %rd1101;
	add.s64 	%rd1104, %rd1103, %rd1100;
	add.s64 	%rd1105, %rd1104, %rd1099;
	shl.b64 	%rd1106, %rd1105, 2;
	add.s64 	%rd1107, %rd4, %rd1106;
	st.global.u32 	[%rd1107+387072], %r29242;
	cvt.u64.u32 	%rd1108, %r85;
	add.s64 	%rd1109, %rd1104, %rd1108;
	shl.b64 	%rd1110, %rd1109, 2;
	add.s64 	%rd1111, %rd4, %rd1110;
	st.global.u32 	[%rd1111+387072], %r29241;
LBB0_196:                               // %L135863
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29239, 0;
	mov.u32 	%r29241, %r29239;
	mov.u32 	%r29242, %r29239;
	mov.u32 	%r29243, %r29239;
	mov.u32 	%r29244, %r29239;
LBB0_197:                               // %L135864
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r22502, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r22513, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r22499, %r1979, %r22501, %r22502, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22503, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22504, %r22502, %r22503;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22507, %r22499, %r22504;
	// end inline asm
	// begin inline asm
	lop3.b32 %r22510, %r1990, %r22501, %r22513, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22514, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22515, %r22513, %r22514;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22518, %r22510, %r22515;
	// end inline asm
	// begin inline asm
	mov.b32 %r22548, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r22559, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r22545, %r1979, %r22547, %r22548, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22549, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22550, %r22548, %r22549;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22553, %r22545, %r22550;
	// end inline asm
	// begin inline asm
	lop3.b32 %r22556, %r1990, %r22547, %r22559, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22560, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22561, %r22559, %r22560;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22564, %r22556, %r22561;
	// end inline asm
	// begin inline asm
	mov.b32 %r22594, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r22605, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r22591, %r1979, %r22593, %r22594, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22595, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22596, %r22594, %r22595;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22599, %r22591, %r22596;
	// end inline asm
	// begin inline asm
	lop3.b32 %r22602, %r1990, %r22593, %r22605, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22606, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22607, %r22605, %r22606;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22610, %r22602, %r22607;
	// end inline asm
	// begin inline asm
	mov.b32 %r22640, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r22651, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r22637, %r1979, %r22639, %r22640, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22641, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22642, %r22640, %r22641;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22645, %r22637, %r22642;
	// end inline asm
	// begin inline asm
	lop3.b32 %r22648, %r1990, %r22639, %r22651, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r22652, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r22653, %r22651, %r22652;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r22656, %r22648, %r22653;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r22507;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23027, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r22518;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23030, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r22553;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23033, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r22564;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23036, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r22599;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23039, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r22610;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23042, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r22645;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23045, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r22656;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r23048, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r23379, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23111, %r23114}, {%r1202, %r1199}, {%r23027}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23120, %r23123}, {%r1202, %r1199}, {%r23030}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23129, %r23132}, {%r1202, %r1199}, {%r23033}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23138, %r23141}, {%r1202, %r1199}, {%r23036}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23147, %r23150}, {%r1202, %r1199}, {%r23039}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23156, %r23159}, {%r1202, %r1199}, {%r23042}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23165, %r23168}, {%r1202, %r1199}, {%r23045}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23174, %r23177}, {%r1202, %r1199}, {%r23048}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23107, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23109, %r23107, %r23111;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23112, %r1237, %r23114, %r23109;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23116, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23118, %r23116, %r23120;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23121, %r1237, %r23123, %r23118;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23125, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23127, %r23125, %r23129;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23130, %r1237, %r23132, %r23127;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23134, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23136, %r23134, %r23138;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23139, %r1237, %r23141, %r23136;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23143, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23145, %r23143, %r23147;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23148, %r1237, %r23150, %r23145;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23152, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23154, %r23152, %r23156;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23157, %r1237, %r23159, %r23154;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23161, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23163, %r23161, %r23165;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23166, %r1237, %r23168, %r23163;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23170, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23172, %r23170, %r23174;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23175, %r1237, %r23177, %r23172;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23179, %r1240, %r23114;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23182, %r1237, %r23111, %r23179;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23186, %r1240, %r23123;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23189, %r1237, %r23120, %r23186;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23193, %r1240, %r23132;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23196, %r1237, %r23129, %r23193;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23200, %r1240, %r23141;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23203, %r1237, %r23138, %r23200;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23207, %r1240, %r23150;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23210, %r1237, %r23147, %r23207;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23214, %r1240, %r23159;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23217, %r1237, %r23156, %r23214;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23221, %r1240, %r23168;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23224, %r1237, %r23165, %r23221;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23228, %r1240, %r23177;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23231, %r1237, %r23174, %r23228;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23316, %r23317}, {%r1270, %r1264, %r1267, %r1261}, {%r23182, %r23112}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23324, %r23325}, {%r1270, %r1264, %r1267, %r1261}, {%r23189, %r23121}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23332, %r23333}, {%r1270, %r1264, %r1267, %r1261}, {%r23196, %r23130}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23340, %r23341}, {%r1270, %r1264, %r1267, %r1261}, {%r23203, %r23139}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23348, %r23349}, {%r1270, %r1264, %r1267, %r1261}, {%r23210, %r23148}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23356, %r23357}, {%r1270, %r1264, %r1267, %r1261}, {%r23217, %r23157}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23364, %r23365}, {%r1270, %r1264, %r1267, %r1261}, {%r23224, %r23166}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23372, %r23373}, {%r1270, %r1264, %r1267, %r1261}, {%r23231, %r23175}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	prmt.b32 %r23315, %r23316, %r23317, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23319, %r23316, %r23317, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23323, %r23324, %r23325, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23327, %r23324, %r23325, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23331, %r23332, %r23333, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23335, %r23332, %r23333, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23339, %r23340, %r23341, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23343, %r23340, %r23341, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23347, %r23348, %r23349, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23351, %r23348, %r23349, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23355, %r23356, %r23357, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23359, %r23356, %r23357, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23363, %r23364, %r23365, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23367, %r23364, %r23365, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23371, %r23372, %r23373, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r23375, %r23372, %r23373, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r23315;
	st.shared.u32 	[%rd7+33024], %r23319;
	st.shared.u32 	[%rd8], %r23323;
	st.shared.u32 	[%rd9], %r23327;
	add.s16 	%rs3148, %rs3785, 14;
	shr.s16 	%rs3149, %rs3148, 15;
	shr.u16 	%rs3150, %rs3149, 14;
	add.s16 	%rs3151, %rs3148, %rs3150;
	and.b16  	%rs3152, %rs3151, -4;
	sub.s16 	%rs3153, %rs3148, %rs3152;
	mul.wide.s16 	%r771, %rs3153, 64;
	add.s32 	%r23380, %r71, %r771;
	add.s32 	%r23381, %r23380, %r28848;
	add.s32 	%r23382, %r23381, %r72;
	add.s32 	%r23383, %r23382, %r28849;
	add.s32 	%r23384, %r23383, %r73;
	add.s32 	%r23385, %r23384, %r74;
	add.s32 	%r23386, %r23385, %r75;
	mul.wide.s32 	%rd1112, %r23386, 4;
	add.s64 	%rd1114, %rd52, %rd1112;
	st.shared.u32 	[%rd1114], %r23331;
	add.s32 	%r772, %r771, 8256;
	add.s32 	%r23387, %r772, %r71;
	add.s32 	%r23388, %r23387, %r28848;
	add.s32 	%r23389, %r23388, %r72;
	add.s32 	%r23390, %r23389, %r28849;
	add.s32 	%r23391, %r23390, %r73;
	add.s32 	%r23392, %r23391, %r74;
	add.s32 	%r23393, %r23392, %r75;
	mul.wide.u32 	%rd1115, %r23393, 4;
	add.s64 	%rd1116, %rd52, %rd1115;
	st.shared.u32 	[%rd1116], %r23335;
	add.s32 	%r23394, %r23380, %r76;
	add.s32 	%r23395, %r23394, %r72;
	add.s32 	%r23396, %r23395, %r28849;
	add.s32 	%r23397, %r23396, %r73;
	add.s32 	%r23398, %r23397, %r74;
	add.s32 	%r23399, %r23398, %r75;
	mul.wide.s32 	%rd1117, %r23399, 4;
	add.s64 	%rd1118, %rd52, %rd1117;
	st.shared.u32 	[%rd1118], %r23339;
	add.s32 	%r23400, %r23387, %r76;
	add.s32 	%r23401, %r23400, %r72;
	add.s32 	%r23402, %r23401, %r28849;
	add.s32 	%r23403, %r23402, %r73;
	add.s32 	%r23404, %r23403, %r74;
	add.s32 	%r23405, %r23404, %r75;
	mul.wide.s32 	%rd1119, %r23405, 4;
	add.s64 	%rd1120, %rd52, %rd1119;
	st.shared.u32 	[%rd1120], %r23343;
	add.s16 	%rs3154, %rs3785, 15;
	shr.s16 	%rs3155, %rs3154, 15;
	shr.u16 	%rs3156, %rs3155, 14;
	add.s16 	%rs3157, %rs3154, %rs3156;
	and.b16  	%rs3158, %rs3157, -4;
	sub.s16 	%rs3159, %rs3154, %rs3158;
	mul.wide.s16 	%r773, %rs3159, 64;
	add.s32 	%r23406, %r71, %r773;
	add.s32 	%r23407, %r23406, %r28848;
	add.s32 	%r23408, %r23407, %r72;
	add.s32 	%r23409, %r23408, %r28849;
	add.s32 	%r23410, %r23409, %r73;
	add.s32 	%r23411, %r23410, %r74;
	add.s32 	%r23412, %r23411, %r75;
	mul.wide.s32 	%rd1121, %r23412, 4;
	add.s64 	%rd1122, %rd52, %rd1121;
	st.shared.u32 	[%rd1122], %r23347;
	add.s32 	%r774, %r773, 8256;
	add.s32 	%r23413, %r774, %r71;
	add.s32 	%r23414, %r23413, %r28848;
	add.s32 	%r23415, %r23414, %r72;
	add.s32 	%r23416, %r23415, %r28849;
	add.s32 	%r23417, %r23416, %r73;
	add.s32 	%r23418, %r23417, %r74;
	add.s32 	%r23419, %r23418, %r75;
	mul.wide.u32 	%rd1123, %r23419, 4;
	add.s64 	%rd1124, %rd52, %rd1123;
	st.shared.u32 	[%rd1124], %r23351;
	add.s32 	%r23420, %r23406, %r76;
	add.s32 	%r23421, %r23420, %r72;
	add.s32 	%r23422, %r23421, %r28849;
	add.s32 	%r23423, %r23422, %r73;
	add.s32 	%r23424, %r23423, %r74;
	add.s32 	%r23425, %r23424, %r75;
	mul.wide.s32 	%rd1125, %r23425, 4;
	add.s64 	%rd1126, %rd52, %rd1125;
	st.shared.u32 	[%rd1126], %r23355;
	add.s32 	%r23426, %r23413, %r76;
	add.s32 	%r23427, %r23426, %r72;
	add.s32 	%r23428, %r23427, %r28849;
	add.s32 	%r23429, %r23428, %r73;
	add.s32 	%r23430, %r23429, %r74;
	add.s32 	%r23431, %r23430, %r75;
	mul.wide.s32 	%rd1127, %r23431, 4;
	add.s64 	%rd1128, %rd52, %rd1127;
	st.shared.u32 	[%rd1128], %r23359;
	add.s16 	%rs3160, %rs3785, 16;
	shr.s16 	%rs3161, %rs3160, 15;
	shr.u16 	%rs3162, %rs3161, 14;
	add.s16 	%rs3163, %rs3160, %rs3162;
	and.b16  	%rs3164, %rs3163, -4;
	sub.s16 	%rs3165, %rs3160, %rs3164;
	mul.wide.s16 	%r775, %rs3165, 64;
	add.s32 	%r23432, %r71, %r775;
	add.s32 	%r23433, %r23432, %r28848;
	add.s32 	%r23434, %r23433, %r72;
	add.s32 	%r23435, %r23434, %r28849;
	add.s32 	%r23436, %r23435, %r73;
	add.s32 	%r23437, %r23436, %r74;
	add.s32 	%r23438, %r23437, %r75;
	mul.wide.s32 	%rd1129, %r23438, 4;
	add.s64 	%rd1130, %rd52, %rd1129;
	st.shared.u32 	[%rd1130], %r23363;
	add.s32 	%r776, %r775, 8256;
	add.s32 	%r23439, %r776, %r71;
	add.s32 	%r23440, %r23439, %r28848;
	add.s32 	%r23441, %r23440, %r72;
	add.s32 	%r23442, %r23441, %r28849;
	add.s32 	%r23443, %r23442, %r73;
	add.s32 	%r23444, %r23443, %r74;
	add.s32 	%r23445, %r23444, %r75;
	mul.wide.u32 	%rd1131, %r23445, 4;
	add.s64 	%rd1132, %rd52, %rd1131;
	st.shared.u32 	[%rd1132], %r23367;
	add.s32 	%r23446, %r23432, %r76;
	add.s32 	%r23447, %r23446, %r72;
	add.s32 	%r23448, %r23447, %r28849;
	add.s32 	%r23449, %r23448, %r73;
	add.s32 	%r23450, %r23449, %r74;
	add.s32 	%r23451, %r23450, %r75;
	mul.wide.s32 	%rd1133, %r23451, 4;
	add.s64 	%rd1134, %rd52, %rd1133;
	st.shared.u32 	[%rd1134], %r23371;
	add.s32 	%r23452, %r23439, %r76;
	add.s32 	%r23453, %r23452, %r72;
	add.s32 	%r23454, %r23453, %r28849;
	add.s32 	%r23455, %r23454, %r73;
	add.s32 	%r23456, %r23455, %r74;
	add.s32 	%r23457, %r23456, %r75;
	mul.wide.s32 	%rd1135, %r23457, 4;
	add.s64 	%rd1136, %rd52, %rd1135;
	st.shared.u32 	[%rd1136], %r23375;
	bar.sync 	0;
	mov.u32 	%r29245, %r23379;
	mov.u32 	%r29246, %r23379;
	mov.u32 	%r29247, %r23379;
	mov.u32 	%r29248, %r23379;
	@%p50 bra 	LBB0_298;
	bra.uni 	LBB0_198;
LBB0_298:                               // %oksrem49150
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29245, [%rd10];
	ld.shared.u32 	%r29246, [%rd11+33024];
	ld.shared.u32 	%r29247, [%rd11+128];
	ld.shared.u32 	%r29248, [%rd11+33152];
LBB0_198:                               // %L143133
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23490, %r23493}, {%r1202, %r1199}, {%r29245}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23499, %r23502}, {%r1202, %r1199}, {%r29246}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23508, %r23511}, {%r1202, %r1199}, {%r29247}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23517, %r23520}, {%r1202, %r1199}, {%r29248}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23486, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23488, %r23486, %r23490;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23491, %r1237, %r23493, %r23488;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23495, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23497, %r23495, %r23499;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23500, %r1237, %r23502, %r23497;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23504, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23506, %r23504, %r23508;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23509, %r1237, %r23511, %r23506;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23513, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23515, %r23513, %r23517;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23518, %r1237, %r23520, %r23515;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23522, %r1240, %r23493;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23525, %r1237, %r23490, %r23522;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23529, %r1240, %r23502;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23532, %r1237, %r23499, %r23529;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23536, %r1240, %r23511;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23539, %r1237, %r23508, %r23536;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23543, %r1240, %r23520;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23546, %r1237, %r23517, %r23543;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23591, %r23595}, {%r1270, %r1264, %r1267, %r1261}, {%r23525, %r23491}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23599, %r23603}, {%r1270, %r1264, %r1267, %r1261}, {%r23532, %r23500}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23607, %r23611}, {%r1270, %r1264, %r1267, %r1261}, {%r23539, %r23509}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23615, %r23619}, {%r1270, %r1264, %r1267, %r1261}, {%r23546, %r23518}, {%r23379, %r23379};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23590, %r23591, %r23591, %r29244;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29254, %r23595, %r23595, %r23590;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23598, %r23599, %r23599, %r29243;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29253, %r23603, %r23603, %r23598;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23606, %r23607, %r23607, %r29242;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29252, %r23611, %r23611, %r23606;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23614, %r23615, %r23615, %r29241;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29251, %r23619, %r23619, %r23614;
	// end inline asm
	add.s32 	%r29249, %r29239, 1;
	setp.ne.s32 	%p159, %r29249, 40;
	@%p159 bra 	LBB0_202;
// %bb.199:                             // %L143316
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_201;
// %bb.200:                             // %L143381
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r23622, %r28856, 193536;
	add.s32 	%r23623, %r83, %r23622;
	or.b32  	%r23624, %r23623, %r43;
	add.s32 	%r23625, %r23624, %r84;
	mul.wide.u32 	%rd1137, %r23625, 4;
	add.s64 	%rd1138, %rd4, %rd1137;
	st.global.u32 	[%rd1138], %r29254;
	add.s32 	%r23626, %r23624, %r85;
	mul.wide.u32 	%rd1139, %r23626, 4;
	add.s64 	%rd1140, %rd4, %rd1139;
	st.global.u32 	[%rd1140], %r29253;
	cvt.u64.u32 	%rd1141, %r84;
	cvt.u64.u32 	%rd1142, %r43;
	cvt.u64.u32 	%rd1143, %r83;
	cvt.u64.u32 	%rd1144, %r23622;
	add.s64 	%rd1145, %rd1144, %rd1143;
	add.s64 	%rd1146, %rd1145, %rd1142;
	add.s64 	%rd1147, %rd1146, %rd1141;
	shl.b64 	%rd1148, %rd1147, 2;
	add.s64 	%rd1149, %rd4, %rd1148;
	st.global.u32 	[%rd1149+387072], %r29252;
	cvt.u64.u32 	%rd1150, %r85;
	add.s64 	%rd1151, %rd1146, %rd1150;
	shl.b64 	%rd1152, %rd1151, 2;
	add.s64 	%rd1153, %rd4, %rd1152;
	st.global.u32 	[%rd1153+387072], %r29251;
LBB0_201:                               // %L143894
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29249, 0;
	mov.u32 	%r29251, %r29249;
	mov.u32 	%r29252, %r29249;
	mov.u32 	%r29253, %r29249;
	mov.u32 	%r29254, %r29249;
LBB0_202:                               // %L143895
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r23628, 0;
	mov.u32 	%r29255, %r23628;
	mov.u32 	%r29256, %r23628;
	mov.u32 	%r29257, %r23628;
	mov.u32 	%r29258, %r23628;
	@%p50 bra 	LBB0_299;
	bra.uni 	LBB0_203;
LBB0_299:                               // %oksrem49848
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r23629, %r77, %r771;
	add.s32 	%r23630, %r23629, %r78;
	add.s32 	%r23631, %r23630, %r79;
	add.s32 	%r23632, %r23631, %r28851;
	add.s32 	%r23633, %r23632, %r80;
	add.s32 	%r23634, %r23633, %r81;
	add.s32 	%r23635, %r23634, %r82;
	mul.wide.s32 	%rd1154, %r23635, 4;
	add.s64 	%rd1156, %rd52, %rd1154;
	ld.shared.u32 	%r29255, [%rd1156];
	or.b32  	%r23636, %r772, %r77;
	add.s32 	%r23637, %r23636, %r78;
	add.s32 	%r23638, %r23637, %r79;
	add.s32 	%r23639, %r23638, %r28851;
	add.s32 	%r23640, %r23639, %r80;
	add.s32 	%r23641, %r23640, %r81;
	add.s32 	%r23642, %r23641, %r82;
	mul.wide.u32 	%rd1157, %r23642, 4;
	add.s64 	%rd1158, %rd52, %rd1157;
	ld.shared.u32 	%r29256, [%rd1158];
	or.b32  	%r23643, %r23629, 32;
	add.s32 	%r23644, %r23643, %r78;
	add.s32 	%r23645, %r23644, %r79;
	add.s32 	%r23646, %r23645, %r28851;
	add.s32 	%r23647, %r23646, %r80;
	add.s32 	%r23648, %r23647, %r81;
	add.s32 	%r23649, %r23648, %r82;
	mul.wide.s32 	%rd1159, %r23649, 4;
	add.s64 	%rd1160, %rd52, %rd1159;
	ld.shared.u32 	%r29257, [%rd1160];
	add.s32 	%r23650, %r771, 8288;
	or.b32  	%r23651, %r23650, %r77;
	add.s32 	%r23652, %r23651, %r78;
	add.s32 	%r23653, %r23652, %r79;
	add.s32 	%r23654, %r23653, %r28851;
	add.s32 	%r23655, %r23654, %r80;
	add.s32 	%r23656, %r23655, %r81;
	add.s32 	%r23657, %r23656, %r82;
	mul.wide.u32 	%rd1161, %r23657, 4;
	add.s64 	%rd1162, %rd52, %rd1161;
	ld.shared.u32 	%r29258, [%rd1162];
LBB0_203:                               // %L145014
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23690, %r23693}, {%r1202, %r1199}, {%r29255}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23699, %r23702}, {%r1202, %r1199}, {%r29256}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23708, %r23711}, {%r1202, %r1199}, {%r29257}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23717, %r23720}, {%r1202, %r1199}, {%r29258}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23686, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23688, %r23686, %r23690;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23691, %r1237, %r23693, %r23688;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23695, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23697, %r23695, %r23699;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23700, %r1237, %r23702, %r23697;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23704, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23706, %r23704, %r23708;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23709, %r1237, %r23711, %r23706;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23713, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23715, %r23713, %r23717;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23718, %r1237, %r23720, %r23715;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23722, %r1240, %r23693;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23725, %r1237, %r23690, %r23722;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23729, %r1240, %r23702;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23732, %r1237, %r23699, %r23729;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23736, %r1240, %r23711;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23739, %r1237, %r23708, %r23736;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23743, %r1240, %r23720;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23746, %r1237, %r23717, %r23743;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23791, %r23795}, {%r1270, %r1264, %r1267, %r1261}, {%r23725, %r23691}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23799, %r23803}, {%r1270, %r1264, %r1267, %r1261}, {%r23732, %r23700}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23807, %r23811}, {%r1270, %r1264, %r1267, %r1261}, {%r23739, %r23709}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23815, %r23819}, {%r1270, %r1264, %r1267, %r1261}, {%r23746, %r23718}, {%r23628, %r23628};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23790, %r23791, %r23791, %r29254;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29264, %r23795, %r23795, %r23790;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23798, %r23799, %r23799, %r29253;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29263, %r23803, %r23803, %r23798;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23806, %r23807, %r23807, %r29252;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29262, %r23811, %r23811, %r23806;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23814, %r23815, %r23815, %r29251;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29261, %r23819, %r23819, %r23814;
	// end inline asm
	add.s32 	%r29259, %r29249, 1;
	setp.ne.s32 	%p162, %r29259, 40;
	@%p162 bra 	LBB0_207;
// %bb.204:                             // %L145197
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_206;
// %bb.205:                             // %L145262
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r23822, %r28856, 193536;
	add.s32 	%r23823, %r83, %r23822;
	or.b32  	%r23824, %r23823, %r43;
	add.s32 	%r23825, %r23824, %r84;
	mul.wide.u32 	%rd1163, %r23825, 4;
	add.s64 	%rd1164, %rd4, %rd1163;
	st.global.u32 	[%rd1164], %r29264;
	add.s32 	%r23826, %r23824, %r85;
	mul.wide.u32 	%rd1165, %r23826, 4;
	add.s64 	%rd1166, %rd4, %rd1165;
	st.global.u32 	[%rd1166], %r29263;
	cvt.u64.u32 	%rd1167, %r84;
	cvt.u64.u32 	%rd1168, %r43;
	cvt.u64.u32 	%rd1169, %r83;
	cvt.u64.u32 	%rd1170, %r23822;
	add.s64 	%rd1171, %rd1170, %rd1169;
	add.s64 	%rd1172, %rd1171, %rd1168;
	add.s64 	%rd1173, %rd1172, %rd1167;
	shl.b64 	%rd1174, %rd1173, 2;
	add.s64 	%rd1175, %rd4, %rd1174;
	st.global.u32 	[%rd1175+387072], %r29262;
	cvt.u64.u32 	%rd1176, %r85;
	add.s64 	%rd1177, %rd1172, %rd1176;
	shl.b64 	%rd1178, %rd1177, 2;
	add.s64 	%rd1179, %rd4, %rd1178;
	st.global.u32 	[%rd1179+387072], %r29261;
LBB0_206:                               // %L145775
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29259, 0;
	mov.u32 	%r29261, %r29259;
	mov.u32 	%r29262, %r29259;
	mov.u32 	%r29263, %r29259;
	mov.u32 	%r29264, %r29259;
LBB0_207:                               // %L145776
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r23828, 0;
	mov.u32 	%r29265, %r23828;
	mov.u32 	%r29266, %r23828;
	mov.u32 	%r29267, %r23828;
	mov.u32 	%r29268, %r23828;
	@%p50 bra 	LBB0_300;
	bra.uni 	LBB0_208;
LBB0_300:                               // %oksrem50546
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r23829, %r77, %r773;
	add.s32 	%r23830, %r23829, %r78;
	add.s32 	%r23831, %r23830, %r79;
	add.s32 	%r23832, %r23831, %r28851;
	add.s32 	%r23833, %r23832, %r80;
	add.s32 	%r23834, %r23833, %r81;
	add.s32 	%r23835, %r23834, %r82;
	mul.wide.s32 	%rd1180, %r23835, 4;
	add.s64 	%rd1182, %rd52, %rd1180;
	ld.shared.u32 	%r29265, [%rd1182];
	or.b32  	%r23836, %r774, %r77;
	add.s32 	%r23837, %r23836, %r78;
	add.s32 	%r23838, %r23837, %r79;
	add.s32 	%r23839, %r23838, %r28851;
	add.s32 	%r23840, %r23839, %r80;
	add.s32 	%r23841, %r23840, %r81;
	add.s32 	%r23842, %r23841, %r82;
	mul.wide.u32 	%rd1183, %r23842, 4;
	add.s64 	%rd1184, %rd52, %rd1183;
	ld.shared.u32 	%r29266, [%rd1184];
	or.b32  	%r23843, %r23829, 32;
	add.s32 	%r23844, %r23843, %r78;
	add.s32 	%r23845, %r23844, %r79;
	add.s32 	%r23846, %r23845, %r28851;
	add.s32 	%r23847, %r23846, %r80;
	add.s32 	%r23848, %r23847, %r81;
	add.s32 	%r23849, %r23848, %r82;
	mul.wide.s32 	%rd1185, %r23849, 4;
	add.s64 	%rd1186, %rd52, %rd1185;
	ld.shared.u32 	%r29267, [%rd1186];
	add.s32 	%r23850, %r773, 8288;
	or.b32  	%r23851, %r23850, %r77;
	add.s32 	%r23852, %r23851, %r78;
	add.s32 	%r23853, %r23852, %r79;
	add.s32 	%r23854, %r23853, %r28851;
	add.s32 	%r23855, %r23854, %r80;
	add.s32 	%r23856, %r23855, %r81;
	add.s32 	%r23857, %r23856, %r82;
	mul.wide.u32 	%rd1187, %r23857, 4;
	add.s64 	%rd1188, %rd52, %rd1187;
	ld.shared.u32 	%r29268, [%rd1188];
LBB0_208:                               // %L146895
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23890, %r23893}, {%r1202, %r1199}, {%r29265}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23899, %r23902}, {%r1202, %r1199}, {%r29266}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23908, %r23911}, {%r1202, %r1199}, {%r29267}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r23917, %r23920}, {%r1202, %r1199}, {%r29268}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23886, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23888, %r23886, %r23890;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23891, %r1237, %r23893, %r23888;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23895, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23897, %r23895, %r23899;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23900, %r1237, %r23902, %r23897;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23904, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23906, %r23904, %r23908;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23909, %r1237, %r23911, %r23906;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r23913, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23915, %r23913, %r23917;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23918, %r1237, %r23920, %r23915;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23922, %r1240, %r23893;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23925, %r1237, %r23890, %r23922;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23929, %r1240, %r23902;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23932, %r1237, %r23899, %r23929;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23936, %r1240, %r23911;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23939, %r1237, %r23908, %r23936;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r23943, %r1240, %r23920;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23946, %r1237, %r23917, %r23943;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23991, %r23995}, {%r1270, %r1264, %r1267, %r1261}, {%r23925, %r23891}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r23999, %r24003}, {%r1270, %r1264, %r1267, %r1261}, {%r23932, %r23900}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r24007, %r24011}, {%r1270, %r1264, %r1267, %r1261}, {%r23939, %r23909}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r24015, %r24019}, {%r1270, %r1264, %r1267, %r1261}, {%r23946, %r23918}, {%r23828, %r23828};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23990, %r23991, %r23991, %r29264;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29274, %r23995, %r23995, %r23990;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r23998, %r23999, %r23999, %r29263;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29273, %r24003, %r24003, %r23998;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24006, %r24007, %r24007, %r29262;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29272, %r24011, %r24011, %r24006;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24014, %r24015, %r24015, %r29261;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29271, %r24019, %r24019, %r24014;
	// end inline asm
	add.s32 	%r29269, %r29259, 1;
	setp.ne.s32 	%p165, %r29269, 40;
	@%p165 bra 	LBB0_212;
// %bb.209:                             // %L147078
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_211;
// %bb.210:                             // %L147143
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r24022, %r28856, 193536;
	add.s32 	%r24023, %r83, %r24022;
	or.b32  	%r24024, %r24023, %r43;
	add.s32 	%r24025, %r24024, %r84;
	mul.wide.u32 	%rd1189, %r24025, 4;
	add.s64 	%rd1190, %rd4, %rd1189;
	st.global.u32 	[%rd1190], %r29274;
	add.s32 	%r24026, %r24024, %r85;
	mul.wide.u32 	%rd1191, %r24026, 4;
	add.s64 	%rd1192, %rd4, %rd1191;
	st.global.u32 	[%rd1192], %r29273;
	cvt.u64.u32 	%rd1193, %r84;
	cvt.u64.u32 	%rd1194, %r43;
	cvt.u64.u32 	%rd1195, %r83;
	cvt.u64.u32 	%rd1196, %r24022;
	add.s64 	%rd1197, %rd1196, %rd1195;
	add.s64 	%rd1198, %rd1197, %rd1194;
	add.s64 	%rd1199, %rd1198, %rd1193;
	shl.b64 	%rd1200, %rd1199, 2;
	add.s64 	%rd1201, %rd4, %rd1200;
	st.global.u32 	[%rd1201+387072], %r29272;
	cvt.u64.u32 	%rd1202, %r85;
	add.s64 	%rd1203, %rd1198, %rd1202;
	shl.b64 	%rd1204, %rd1203, 2;
	add.s64 	%rd1205, %rd4, %rd1204;
	st.global.u32 	[%rd1205+387072], %r29271;
LBB0_211:                               // %L147656
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29269, 0;
	mov.u32 	%r29271, %r29269;
	mov.u32 	%r29272, %r29269;
	mov.u32 	%r29273, %r29269;
	mov.u32 	%r29274, %r29269;
LBB0_212:                               // %L147657
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r24028, 0;
	mov.u32 	%r29275, %r24028;
	mov.u32 	%r29276, %r24028;
	mov.u32 	%r29277, %r24028;
	mov.u32 	%r29278, %r24028;
	@%p50 bra 	LBB0_301;
	bra.uni 	LBB0_213;
LBB0_301:                               // %oksrem51244
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r24029, %r77, %r775;
	add.s32 	%r24030, %r24029, %r78;
	add.s32 	%r24031, %r24030, %r79;
	add.s32 	%r24032, %r24031, %r28851;
	add.s32 	%r24033, %r24032, %r80;
	add.s32 	%r24034, %r24033, %r81;
	add.s32 	%r24035, %r24034, %r82;
	mul.wide.s32 	%rd1206, %r24035, 4;
	add.s64 	%rd1208, %rd52, %rd1206;
	ld.shared.u32 	%r29275, [%rd1208];
	or.b32  	%r24036, %r776, %r77;
	add.s32 	%r24037, %r24036, %r78;
	add.s32 	%r24038, %r24037, %r79;
	add.s32 	%r24039, %r24038, %r28851;
	add.s32 	%r24040, %r24039, %r80;
	add.s32 	%r24041, %r24040, %r81;
	add.s32 	%r24042, %r24041, %r82;
	mul.wide.u32 	%rd1209, %r24042, 4;
	add.s64 	%rd1210, %rd52, %rd1209;
	ld.shared.u32 	%r29276, [%rd1210];
	or.b32  	%r24043, %r24029, 32;
	add.s32 	%r24044, %r24043, %r78;
	add.s32 	%r24045, %r24044, %r79;
	add.s32 	%r24046, %r24045, %r28851;
	add.s32 	%r24047, %r24046, %r80;
	add.s32 	%r24048, %r24047, %r81;
	add.s32 	%r24049, %r24048, %r82;
	mul.wide.s32 	%rd1211, %r24049, 4;
	add.s64 	%rd1212, %rd52, %rd1211;
	ld.shared.u32 	%r29277, [%rd1212];
	add.s32 	%r24050, %r775, 8288;
	or.b32  	%r24051, %r24050, %r77;
	add.s32 	%r24052, %r24051, %r78;
	add.s32 	%r24053, %r24052, %r79;
	add.s32 	%r24054, %r24053, %r28851;
	add.s32 	%r24055, %r24054, %r80;
	add.s32 	%r24056, %r24055, %r81;
	add.s32 	%r24057, %r24056, %r82;
	mul.wide.u32 	%rd1213, %r24057, 4;
	add.s64 	%rd1214, %rd52, %rd1213;
	ld.shared.u32 	%r29278, [%rd1214];
LBB0_213:                               // %L148776
                                        //   in Loop: Header=BB0_16 Depth=1
	shr.u32 	%r24990, %r149, 8;
	shr.u32 	%r25036, %r151, 8;
	shr.u32 	%r25082, %r153, 8;
	shr.u32 	%r25128, %r155, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r24090, %r24093}, {%r1202, %r1199}, {%r29275}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r24099, %r24102}, {%r1202, %r1199}, {%r29276}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r24108, %r24111}, {%r1202, %r1199}, {%r29277}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r24117, %r24120}, {%r1202, %r1199}, {%r29278}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r24086, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24088, %r24086, %r24090;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24091, %r1237, %r24093, %r24088;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r24095, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24097, %r24095, %r24099;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24100, %r1237, %r24102, %r24097;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r24104, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24106, %r24104, %r24108;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24109, %r1237, %r24111, %r24106;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r24113, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24115, %r24113, %r24117;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24118, %r1237, %r24120, %r24115;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24122, %r1240, %r24093;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24125, %r1237, %r24090, %r24122;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24129, %r1240, %r24102;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24132, %r1237, %r24099, %r24129;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24136, %r1240, %r24111;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24139, %r1237, %r24108, %r24136;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r24143, %r1240, %r24120;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24146, %r1237, %r24117, %r24143;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r24191, %r24195}, {%r1270, %r1264, %r1267, %r1261}, {%r24125, %r24091}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r24199, %r24203}, {%r1270, %r1264, %r1267, %r1261}, {%r24132, %r24100}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r24207, %r24211}, {%r1270, %r1264, %r1267, %r1261}, {%r24139, %r24109}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r24215, %r24219}, {%r1270, %r1264, %r1267, %r1261}, {%r24146, %r24118}, {%r24028, %r24028};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24190, %r24191, %r24191, %r29274;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29284, %r24195, %r24195, %r24190;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24198, %r24199, %r24199, %r29273;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29283, %r24203, %r24203, %r24198;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24206, %r24207, %r24207, %r29272;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29282, %r24211, %r24211, %r24206;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r24214, %r24215, %r24215, %r29271;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29281, %r24219, %r24219, %r24214;
	// end inline asm
	add.s32 	%r29279, %r29269, 1;
	setp.ne.s32 	%p168, %r29279, 40;
	@%p168 bra 	LBB0_217;
// %bb.214:                             // %L148959
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_216;
// %bb.215:                             // %L149024
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r24222, %r28856, 193536;
	add.s32 	%r24223, %r83, %r24222;
	or.b32  	%r24224, %r24223, %r43;
	add.s32 	%r24225, %r24224, %r84;
	mul.wide.u32 	%rd1215, %r24225, 4;
	add.s64 	%rd1216, %rd4, %rd1215;
	st.global.u32 	[%rd1216], %r29284;
	add.s32 	%r24226, %r24224, %r85;
	mul.wide.u32 	%rd1217, %r24226, 4;
	add.s64 	%rd1218, %rd4, %rd1217;
	st.global.u32 	[%rd1218], %r29283;
	cvt.u64.u32 	%rd1219, %r84;
	cvt.u64.u32 	%rd1220, %r43;
	cvt.u64.u32 	%rd1221, %r83;
	cvt.u64.u32 	%rd1222, %r24222;
	add.s64 	%rd1223, %rd1222, %rd1221;
	add.s64 	%rd1224, %rd1223, %rd1220;
	add.s64 	%rd1225, %rd1224, %rd1219;
	shl.b64 	%rd1226, %rd1225, 2;
	add.s64 	%rd1227, %rd4, %rd1226;
	st.global.u32 	[%rd1227+387072], %r29282;
	cvt.u64.u32 	%rd1228, %r85;
	add.s64 	%rd1229, %rd1224, %rd1228;
	shl.b64 	%rd1230, %rd1229, 2;
	add.s64 	%rd1231, %rd4, %rd1230;
	st.global.u32 	[%rd1231+387072], %r29281;
LBB0_216:                               // %L149537
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29279, 0;
	mov.u32 	%r29281, %r29279;
	mov.u32 	%r29282, %r29279;
	mov.u32 	%r29283, %r29279;
	mov.u32 	%r29284, %r29279;
LBB0_217:                               // %L149538
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r24991, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r25002, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r24988, %r1979, %r24990, %r24991, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r24992, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r24993, %r24991, %r24992;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r24996, %r24988, %r24993;
	// end inline asm
	// begin inline asm
	lop3.b32 %r24999, %r1990, %r24990, %r25002, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25003, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25004, %r25002, %r25003;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25007, %r24999, %r25004;
	// end inline asm
	// begin inline asm
	mov.b32 %r25037, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r25048, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r25034, %r1979, %r25036, %r25037, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25038, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25039, %r25037, %r25038;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25042, %r25034, %r25039;
	// end inline asm
	// begin inline asm
	lop3.b32 %r25045, %r1990, %r25036, %r25048, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25049, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25050, %r25048, %r25049;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25053, %r25045, %r25050;
	// end inline asm
	// begin inline asm
	mov.b32 %r25083, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r25094, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r25080, %r1979, %r25082, %r25083, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25084, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25085, %r25083, %r25084;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25088, %r25080, %r25085;
	// end inline asm
	// begin inline asm
	lop3.b32 %r25091, %r1990, %r25082, %r25094, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25095, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25096, %r25094, %r25095;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25099, %r25091, %r25096;
	// end inline asm
	// begin inline asm
	mov.b32 %r25129, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r25140, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r25126, %r1979, %r25128, %r25129, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25130, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25131, %r25129, %r25130;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25134, %r25126, %r25131;
	// end inline asm
	// begin inline asm
	lop3.b32 %r25137, %r1990, %r25128, %r25140, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r25141, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r25142, %r25140, %r25141;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r25145, %r25137, %r25142;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r24996;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25332, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r25007;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25335, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r25042;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25338, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r25053;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25341, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r25088;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25344, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r25099;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25347, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r25134;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25350, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r25145;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r25353, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r25684, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25416, %r25419}, {%r1202, %r1199}, {%r25332}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25425, %r25428}, {%r1202, %r1199}, {%r25335}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25434, %r25437}, {%r1202, %r1199}, {%r25338}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25443, %r25446}, {%r1202, %r1199}, {%r25341}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25452, %r25455}, {%r1202, %r1199}, {%r25344}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25461, %r25464}, {%r1202, %r1199}, {%r25347}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25470, %r25473}, {%r1202, %r1199}, {%r25350}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25479, %r25482}, {%r1202, %r1199}, {%r25353}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25412, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25414, %r25412, %r25416;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25417, %r1237, %r25419, %r25414;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25421, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25423, %r25421, %r25425;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25426, %r1237, %r25428, %r25423;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25430, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25432, %r25430, %r25434;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25435, %r1237, %r25437, %r25432;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25439, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25441, %r25439, %r25443;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25444, %r1237, %r25446, %r25441;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25448, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25450, %r25448, %r25452;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25453, %r1237, %r25455, %r25450;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25457, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25459, %r25457, %r25461;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25462, %r1237, %r25464, %r25459;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25466, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25468, %r25466, %r25470;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25471, %r1237, %r25473, %r25468;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25475, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25477, %r25475, %r25479;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25480, %r1237, %r25482, %r25477;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25484, %r1240, %r25419;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25487, %r1237, %r25416, %r25484;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25491, %r1240, %r25428;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25494, %r1237, %r25425, %r25491;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25498, %r1240, %r25437;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25501, %r1237, %r25434, %r25498;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25505, %r1240, %r25446;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25508, %r1237, %r25443, %r25505;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25512, %r1240, %r25455;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25515, %r1237, %r25452, %r25512;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25519, %r1240, %r25464;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25522, %r1237, %r25461, %r25519;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25526, %r1240, %r25473;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25529, %r1237, %r25470, %r25526;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25533, %r1240, %r25482;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25536, %r1237, %r25479, %r25533;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25621, %r25622}, {%r1270, %r1264, %r1267, %r1261}, {%r25487, %r25417}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25629, %r25630}, {%r1270, %r1264, %r1267, %r1261}, {%r25494, %r25426}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25637, %r25638}, {%r1270, %r1264, %r1267, %r1261}, {%r25501, %r25435}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25645, %r25646}, {%r1270, %r1264, %r1267, %r1261}, {%r25508, %r25444}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25653, %r25654}, {%r1270, %r1264, %r1267, %r1261}, {%r25515, %r25453}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25661, %r25662}, {%r1270, %r1264, %r1267, %r1261}, {%r25522, %r25462}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25669, %r25670}, {%r1270, %r1264, %r1267, %r1261}, {%r25529, %r25471}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25677, %r25678}, {%r1270, %r1264, %r1267, %r1261}, {%r25536, %r25480}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	prmt.b32 %r25620, %r25621, %r25622, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25624, %r25621, %r25622, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25628, %r25629, %r25630, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25632, %r25629, %r25630, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25636, %r25637, %r25638, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25640, %r25637, %r25638, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25644, %r25645, %r25646, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25648, %r25645, %r25646, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25652, %r25653, %r25654, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25656, %r25653, %r25654, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25660, %r25661, %r25662, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25664, %r25661, %r25662, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25668, %r25669, %r25670, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25672, %r25669, %r25670, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25676, %r25677, %r25678, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r25680, %r25677, %r25678, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r25620;
	st.shared.u32 	[%rd7+33024], %r25624;
	st.shared.u32 	[%rd8], %r25628;
	st.shared.u32 	[%rd9], %r25632;
	add.s16 	%rs3454, %rs3785, 18;
	shr.s16 	%rs3455, %rs3454, 15;
	shr.u16 	%rs3456, %rs3455, 14;
	add.s16 	%rs3457, %rs3454, %rs3456;
	and.b16  	%rs3458, %rs3457, -4;
	sub.s16 	%rs3459, %rs3454, %rs3458;
	mul.wide.s16 	%r841, %rs3459, 64;
	add.s32 	%r25685, %r71, %r841;
	add.s32 	%r25686, %r25685, %r28848;
	add.s32 	%r25687, %r25686, %r72;
	add.s32 	%r25688, %r25687, %r28849;
	add.s32 	%r25689, %r25688, %r73;
	add.s32 	%r25690, %r25689, %r74;
	add.s32 	%r25691, %r25690, %r75;
	mul.wide.s32 	%rd1232, %r25691, 4;
	add.s64 	%rd1234, %rd52, %rd1232;
	st.shared.u32 	[%rd1234], %r25636;
	add.s32 	%r842, %r841, 8256;
	add.s32 	%r25692, %r842, %r71;
	add.s32 	%r25693, %r25692, %r28848;
	add.s32 	%r25694, %r25693, %r72;
	add.s32 	%r25695, %r25694, %r28849;
	add.s32 	%r25696, %r25695, %r73;
	add.s32 	%r25697, %r25696, %r74;
	add.s32 	%r25698, %r25697, %r75;
	mul.wide.u32 	%rd1235, %r25698, 4;
	add.s64 	%rd1236, %rd52, %rd1235;
	st.shared.u32 	[%rd1236], %r25640;
	add.s32 	%r25699, %r25685, %r76;
	add.s32 	%r25700, %r25699, %r72;
	add.s32 	%r25701, %r25700, %r28849;
	add.s32 	%r25702, %r25701, %r73;
	add.s32 	%r25703, %r25702, %r74;
	add.s32 	%r25704, %r25703, %r75;
	mul.wide.s32 	%rd1237, %r25704, 4;
	add.s64 	%rd1238, %rd52, %rd1237;
	st.shared.u32 	[%rd1238], %r25644;
	add.s32 	%r25705, %r25692, %r76;
	add.s32 	%r25706, %r25705, %r72;
	add.s32 	%r25707, %r25706, %r28849;
	add.s32 	%r25708, %r25707, %r73;
	add.s32 	%r25709, %r25708, %r74;
	add.s32 	%r25710, %r25709, %r75;
	mul.wide.s32 	%rd1239, %r25710, 4;
	add.s64 	%rd1240, %rd52, %rd1239;
	st.shared.u32 	[%rd1240], %r25648;
	add.s16 	%rs3460, %rs3785, 19;
	shr.s16 	%rs3461, %rs3460, 15;
	shr.u16 	%rs3462, %rs3461, 14;
	add.s16 	%rs3463, %rs3460, %rs3462;
	and.b16  	%rs3464, %rs3463, -4;
	sub.s16 	%rs3465, %rs3460, %rs3464;
	mul.wide.s16 	%r843, %rs3465, 64;
	add.s32 	%r25711, %r71, %r843;
	add.s32 	%r25712, %r25711, %r28848;
	add.s32 	%r25713, %r25712, %r72;
	add.s32 	%r25714, %r25713, %r28849;
	add.s32 	%r25715, %r25714, %r73;
	add.s32 	%r25716, %r25715, %r74;
	add.s32 	%r25717, %r25716, %r75;
	mul.wide.s32 	%rd1241, %r25717, 4;
	add.s64 	%rd1242, %rd52, %rd1241;
	st.shared.u32 	[%rd1242], %r25652;
	add.s32 	%r844, %r843, 8256;
	add.s32 	%r25718, %r844, %r71;
	add.s32 	%r25719, %r25718, %r28848;
	add.s32 	%r25720, %r25719, %r72;
	add.s32 	%r25721, %r25720, %r28849;
	add.s32 	%r25722, %r25721, %r73;
	add.s32 	%r25723, %r25722, %r74;
	add.s32 	%r25724, %r25723, %r75;
	mul.wide.u32 	%rd1243, %r25724, 4;
	add.s64 	%rd1244, %rd52, %rd1243;
	st.shared.u32 	[%rd1244], %r25656;
	add.s32 	%r25725, %r25711, %r76;
	add.s32 	%r25726, %r25725, %r72;
	add.s32 	%r25727, %r25726, %r28849;
	add.s32 	%r25728, %r25727, %r73;
	add.s32 	%r25729, %r25728, %r74;
	add.s32 	%r25730, %r25729, %r75;
	mul.wide.s32 	%rd1245, %r25730, 4;
	add.s64 	%rd1246, %rd52, %rd1245;
	st.shared.u32 	[%rd1246], %r25660;
	add.s32 	%r25731, %r25718, %r76;
	add.s32 	%r25732, %r25731, %r72;
	add.s32 	%r25733, %r25732, %r28849;
	add.s32 	%r25734, %r25733, %r73;
	add.s32 	%r25735, %r25734, %r74;
	add.s32 	%r25736, %r25735, %r75;
	mul.wide.s32 	%rd1247, %r25736, 4;
	add.s64 	%rd1248, %rd52, %rd1247;
	st.shared.u32 	[%rd1248], %r25664;
	add.s16 	%rs3466, %rs3785, 20;
	shr.s16 	%rs3467, %rs3466, 15;
	shr.u16 	%rs3468, %rs3467, 14;
	add.s16 	%rs3469, %rs3466, %rs3468;
	and.b16  	%rs3470, %rs3469, -4;
	sub.s16 	%rs3471, %rs3466, %rs3470;
	mul.wide.s16 	%r845, %rs3471, 64;
	add.s32 	%r25737, %r71, %r845;
	add.s32 	%r25738, %r25737, %r28848;
	add.s32 	%r25739, %r25738, %r72;
	add.s32 	%r25740, %r25739, %r28849;
	add.s32 	%r25741, %r25740, %r73;
	add.s32 	%r25742, %r25741, %r74;
	add.s32 	%r25743, %r25742, %r75;
	mul.wide.s32 	%rd1249, %r25743, 4;
	add.s64 	%rd1250, %rd52, %rd1249;
	st.shared.u32 	[%rd1250], %r25668;
	add.s32 	%r846, %r845, 8256;
	add.s32 	%r25744, %r846, %r71;
	add.s32 	%r25745, %r25744, %r28848;
	add.s32 	%r25746, %r25745, %r72;
	add.s32 	%r25747, %r25746, %r28849;
	add.s32 	%r25748, %r25747, %r73;
	add.s32 	%r25749, %r25748, %r74;
	add.s32 	%r25750, %r25749, %r75;
	mul.wide.u32 	%rd1251, %r25750, 4;
	add.s64 	%rd1252, %rd52, %rd1251;
	st.shared.u32 	[%rd1252], %r25672;
	add.s32 	%r25751, %r25737, %r76;
	add.s32 	%r25752, %r25751, %r72;
	add.s32 	%r25753, %r25752, %r28849;
	add.s32 	%r25754, %r25753, %r73;
	add.s32 	%r25755, %r25754, %r74;
	add.s32 	%r25756, %r25755, %r75;
	mul.wide.s32 	%rd1253, %r25756, 4;
	add.s64 	%rd1254, %rd52, %rd1253;
	st.shared.u32 	[%rd1254], %r25676;
	add.s32 	%r25757, %r25744, %r76;
	add.s32 	%r25758, %r25757, %r72;
	add.s32 	%r25759, %r25758, %r28849;
	add.s32 	%r25760, %r25759, %r73;
	add.s32 	%r25761, %r25760, %r74;
	add.s32 	%r25762, %r25761, %r75;
	mul.wide.s32 	%rd1255, %r25762, 4;
	add.s64 	%rd1256, %rd52, %rd1255;
	st.shared.u32 	[%rd1256], %r25680;
	bar.sync 	0;
	mov.u32 	%r29285, %r25684;
	mov.u32 	%r29286, %r25684;
	mov.u32 	%r29287, %r25684;
	mov.u32 	%r29288, %r25684;
	@%p50 bra 	LBB0_302;
	bra.uni 	LBB0_218;
LBB0_302:                               // %oksrem53846
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29285, [%rd10];
	ld.shared.u32 	%r29286, [%rd11+33024];
	ld.shared.u32 	%r29287, [%rd11+128];
	ld.shared.u32 	%r29288, [%rd11+33152];
LBB0_218:                               // %L156807
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25795, %r25798}, {%r1202, %r1199}, {%r29285}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25804, %r25807}, {%r1202, %r1199}, {%r29286}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25813, %r25816}, {%r1202, %r1199}, {%r29287}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25822, %r25825}, {%r1202, %r1199}, {%r29288}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25791, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25793, %r25791, %r25795;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25796, %r1237, %r25798, %r25793;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25800, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25802, %r25800, %r25804;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25805, %r1237, %r25807, %r25802;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25809, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25811, %r25809, %r25813;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25814, %r1237, %r25816, %r25811;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25818, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25820, %r25818, %r25822;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25823, %r1237, %r25825, %r25820;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25827, %r1240, %r25798;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25830, %r1237, %r25795, %r25827;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25834, %r1240, %r25807;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25837, %r1237, %r25804, %r25834;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25841, %r1240, %r25816;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25844, %r1237, %r25813, %r25841;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25848, %r1240, %r25825;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25851, %r1237, %r25822, %r25848;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25896, %r25900}, {%r1270, %r1264, %r1267, %r1261}, {%r25830, %r25796}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25904, %r25908}, {%r1270, %r1264, %r1267, %r1261}, {%r25837, %r25805}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25912, %r25916}, {%r1270, %r1264, %r1267, %r1261}, {%r25844, %r25814}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r25920, %r25924}, {%r1270, %r1264, %r1267, %r1261}, {%r25851, %r25823}, {%r25684, %r25684};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25895, %r25896, %r25896, %r29284;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29294, %r25900, %r25900, %r25895;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25903, %r25904, %r25904, %r29283;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29293, %r25908, %r25908, %r25903;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25911, %r25912, %r25912, %r29282;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29292, %r25916, %r25916, %r25911;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25919, %r25920, %r25920, %r29281;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29291, %r25924, %r25924, %r25919;
	// end inline asm
	add.s32 	%r29289, %r29279, 1;
	setp.ne.s32 	%p171, %r29289, 40;
	@%p171 bra 	LBB0_222;
// %bb.219:                             // %L156990
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_221;
// %bb.220:                             // %L157055
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r25927, %r28856, 193536;
	add.s32 	%r25928, %r83, %r25927;
	or.b32  	%r25929, %r25928, %r43;
	add.s32 	%r25930, %r25929, %r84;
	mul.wide.u32 	%rd1257, %r25930, 4;
	add.s64 	%rd1258, %rd4, %rd1257;
	st.global.u32 	[%rd1258], %r29294;
	add.s32 	%r25931, %r25929, %r85;
	mul.wide.u32 	%rd1259, %r25931, 4;
	add.s64 	%rd1260, %rd4, %rd1259;
	st.global.u32 	[%rd1260], %r29293;
	cvt.u64.u32 	%rd1261, %r84;
	cvt.u64.u32 	%rd1262, %r43;
	cvt.u64.u32 	%rd1263, %r83;
	cvt.u64.u32 	%rd1264, %r25927;
	add.s64 	%rd1265, %rd1264, %rd1263;
	add.s64 	%rd1266, %rd1265, %rd1262;
	add.s64 	%rd1267, %rd1266, %rd1261;
	shl.b64 	%rd1268, %rd1267, 2;
	add.s64 	%rd1269, %rd4, %rd1268;
	st.global.u32 	[%rd1269+387072], %r29292;
	cvt.u64.u32 	%rd1270, %r85;
	add.s64 	%rd1271, %rd1266, %rd1270;
	shl.b64 	%rd1272, %rd1271, 2;
	add.s64 	%rd1273, %rd4, %rd1272;
	st.global.u32 	[%rd1273+387072], %r29291;
LBB0_221:                               // %L157568
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29289, 0;
	mov.u32 	%r29291, %r29289;
	mov.u32 	%r29292, %r29289;
	mov.u32 	%r29293, %r29289;
	mov.u32 	%r29294, %r29289;
LBB0_222:                               // %L157569
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r25933, 0;
	mov.u32 	%r29295, %r25933;
	mov.u32 	%r29296, %r25933;
	mov.u32 	%r29297, %r25933;
	mov.u32 	%r29298, %r25933;
	@%p50 bra 	LBB0_303;
	bra.uni 	LBB0_223;
LBB0_303:                               // %oksrem54544
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r25934, %r77, %r841;
	add.s32 	%r25935, %r25934, %r78;
	add.s32 	%r25936, %r25935, %r79;
	add.s32 	%r25937, %r25936, %r28851;
	add.s32 	%r25938, %r25937, %r80;
	add.s32 	%r25939, %r25938, %r81;
	add.s32 	%r25940, %r25939, %r82;
	mul.wide.s32 	%rd1274, %r25940, 4;
	add.s64 	%rd1276, %rd52, %rd1274;
	ld.shared.u32 	%r29295, [%rd1276];
	or.b32  	%r25941, %r842, %r77;
	add.s32 	%r25942, %r25941, %r78;
	add.s32 	%r25943, %r25942, %r79;
	add.s32 	%r25944, %r25943, %r28851;
	add.s32 	%r25945, %r25944, %r80;
	add.s32 	%r25946, %r25945, %r81;
	add.s32 	%r25947, %r25946, %r82;
	mul.wide.u32 	%rd1277, %r25947, 4;
	add.s64 	%rd1278, %rd52, %rd1277;
	ld.shared.u32 	%r29296, [%rd1278];
	or.b32  	%r25948, %r25934, 32;
	add.s32 	%r25949, %r25948, %r78;
	add.s32 	%r25950, %r25949, %r79;
	add.s32 	%r25951, %r25950, %r28851;
	add.s32 	%r25952, %r25951, %r80;
	add.s32 	%r25953, %r25952, %r81;
	add.s32 	%r25954, %r25953, %r82;
	mul.wide.s32 	%rd1279, %r25954, 4;
	add.s64 	%rd1280, %rd52, %rd1279;
	ld.shared.u32 	%r29297, [%rd1280];
	add.s32 	%r25955, %r841, 8288;
	or.b32  	%r25956, %r25955, %r77;
	add.s32 	%r25957, %r25956, %r78;
	add.s32 	%r25958, %r25957, %r79;
	add.s32 	%r25959, %r25958, %r28851;
	add.s32 	%r25960, %r25959, %r80;
	add.s32 	%r25961, %r25960, %r81;
	add.s32 	%r25962, %r25961, %r82;
	mul.wide.u32 	%rd1281, %r25962, 4;
	add.s64 	%rd1282, %rd52, %rd1281;
	ld.shared.u32 	%r29298, [%rd1282];
LBB0_223:                               // %L158688
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r25995, %r25998}, {%r1202, %r1199}, {%r29295}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26004, %r26007}, {%r1202, %r1199}, {%r29296}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26013, %r26016}, {%r1202, %r1199}, {%r29297}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26022, %r26025}, {%r1202, %r1199}, {%r29298}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r25991, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r25993, %r25991, %r25995;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r25996, %r1237, %r25998, %r25993;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26000, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26002, %r26000, %r26004;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26005, %r1237, %r26007, %r26002;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26009, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26011, %r26009, %r26013;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26014, %r1237, %r26016, %r26011;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26018, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26020, %r26018, %r26022;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26023, %r1237, %r26025, %r26020;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26027, %r1240, %r25998;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26030, %r1237, %r25995, %r26027;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26034, %r1240, %r26007;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26037, %r1237, %r26004, %r26034;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26041, %r1240, %r26016;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26044, %r1237, %r26013, %r26041;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26048, %r1240, %r26025;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26051, %r1237, %r26022, %r26048;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26096, %r26100}, {%r1270, %r1264, %r1267, %r1261}, {%r26030, %r25996}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26104, %r26108}, {%r1270, %r1264, %r1267, %r1261}, {%r26037, %r26005}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26112, %r26116}, {%r1270, %r1264, %r1267, %r1261}, {%r26044, %r26014}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26120, %r26124}, {%r1270, %r1264, %r1267, %r1261}, {%r26051, %r26023}, {%r25933, %r25933};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26095, %r26096, %r26096, %r29294;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29304, %r26100, %r26100, %r26095;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26103, %r26104, %r26104, %r29293;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29303, %r26108, %r26108, %r26103;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26111, %r26112, %r26112, %r29292;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29302, %r26116, %r26116, %r26111;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26119, %r26120, %r26120, %r29291;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29301, %r26124, %r26124, %r26119;
	// end inline asm
	add.s32 	%r29299, %r29289, 1;
	setp.ne.s32 	%p174, %r29299, 40;
	@%p174 bra 	LBB0_227;
// %bb.224:                             // %L158871
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_226;
// %bb.225:                             // %L158936
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r26127, %r28856, 193536;
	add.s32 	%r26128, %r83, %r26127;
	or.b32  	%r26129, %r26128, %r43;
	add.s32 	%r26130, %r26129, %r84;
	mul.wide.u32 	%rd1283, %r26130, 4;
	add.s64 	%rd1284, %rd4, %rd1283;
	st.global.u32 	[%rd1284], %r29304;
	add.s32 	%r26131, %r26129, %r85;
	mul.wide.u32 	%rd1285, %r26131, 4;
	add.s64 	%rd1286, %rd4, %rd1285;
	st.global.u32 	[%rd1286], %r29303;
	cvt.u64.u32 	%rd1287, %r84;
	cvt.u64.u32 	%rd1288, %r43;
	cvt.u64.u32 	%rd1289, %r83;
	cvt.u64.u32 	%rd1290, %r26127;
	add.s64 	%rd1291, %rd1290, %rd1289;
	add.s64 	%rd1292, %rd1291, %rd1288;
	add.s64 	%rd1293, %rd1292, %rd1287;
	shl.b64 	%rd1294, %rd1293, 2;
	add.s64 	%rd1295, %rd4, %rd1294;
	st.global.u32 	[%rd1295+387072], %r29302;
	cvt.u64.u32 	%rd1296, %r85;
	add.s64 	%rd1297, %rd1292, %rd1296;
	shl.b64 	%rd1298, %rd1297, 2;
	add.s64 	%rd1299, %rd4, %rd1298;
	st.global.u32 	[%rd1299+387072], %r29301;
LBB0_226:                               // %L159449
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29299, 0;
	mov.u32 	%r29301, %r29299;
	mov.u32 	%r29302, %r29299;
	mov.u32 	%r29303, %r29299;
	mov.u32 	%r29304, %r29299;
LBB0_227:                               // %L159450
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r26133, 0;
	mov.u32 	%r29305, %r26133;
	mov.u32 	%r29306, %r26133;
	mov.u32 	%r29307, %r26133;
	mov.u32 	%r29308, %r26133;
	@%p50 bra 	LBB0_304;
	bra.uni 	LBB0_228;
LBB0_304:                               // %oksrem55242
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r26134, %r77, %r843;
	add.s32 	%r26135, %r26134, %r78;
	add.s32 	%r26136, %r26135, %r79;
	add.s32 	%r26137, %r26136, %r28851;
	add.s32 	%r26138, %r26137, %r80;
	add.s32 	%r26139, %r26138, %r81;
	add.s32 	%r26140, %r26139, %r82;
	mul.wide.s32 	%rd1300, %r26140, 4;
	add.s64 	%rd1302, %rd52, %rd1300;
	ld.shared.u32 	%r29308, [%rd1302];
	or.b32  	%r26141, %r844, %r77;
	add.s32 	%r26142, %r26141, %r78;
	add.s32 	%r26143, %r26142, %r79;
	add.s32 	%r26144, %r26143, %r28851;
	add.s32 	%r26145, %r26144, %r80;
	add.s32 	%r26146, %r26145, %r81;
	add.s32 	%r26147, %r26146, %r82;
	mul.wide.u32 	%rd1303, %r26147, 4;
	add.s64 	%rd1304, %rd52, %rd1303;
	ld.shared.u32 	%r29307, [%rd1304];
	or.b32  	%r26148, %r26134, 32;
	add.s32 	%r26149, %r26148, %r78;
	add.s32 	%r26150, %r26149, %r79;
	add.s32 	%r26151, %r26150, %r28851;
	add.s32 	%r26152, %r26151, %r80;
	add.s32 	%r26153, %r26152, %r81;
	add.s32 	%r26154, %r26153, %r82;
	mul.wide.s32 	%rd1305, %r26154, 4;
	add.s64 	%rd1306, %rd52, %rd1305;
	ld.shared.u32 	%r29306, [%rd1306];
	add.s32 	%r26155, %r843, 8288;
	or.b32  	%r26156, %r26155, %r77;
	add.s32 	%r26157, %r26156, %r78;
	add.s32 	%r26158, %r26157, %r79;
	add.s32 	%r26159, %r26158, %r28851;
	add.s32 	%r26160, %r26159, %r80;
	add.s32 	%r26161, %r26160, %r81;
	add.s32 	%r26162, %r26161, %r82;
	mul.wide.u32 	%rd1307, %r26162, 4;
	add.s64 	%rd1308, %rd52, %rd1307;
	ld.shared.u32 	%r29305, [%rd1308];
LBB0_228:                               // %L160569
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26195, %r26198}, {%r1202, %r1199}, {%r29308}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26204, %r26207}, {%r1202, %r1199}, {%r29307}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26213, %r26216}, {%r1202, %r1199}, {%r29306}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26222, %r26225}, {%r1202, %r1199}, {%r29305}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26191, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26193, %r26191, %r26195;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26196, %r1237, %r26198, %r26193;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26200, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26202, %r26200, %r26204;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26205, %r1237, %r26207, %r26202;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26209, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26211, %r26209, %r26213;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26214, %r1237, %r26216, %r26211;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26218, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26220, %r26218, %r26222;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26223, %r1237, %r26225, %r26220;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26227, %r1240, %r26198;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26230, %r1237, %r26195, %r26227;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26234, %r1240, %r26207;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26237, %r1237, %r26204, %r26234;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26241, %r1240, %r26216;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26244, %r1237, %r26213, %r26241;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26248, %r1240, %r26225;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26251, %r1237, %r26222, %r26248;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26296, %r26300}, {%r1270, %r1264, %r1267, %r1261}, {%r26230, %r26196}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26304, %r26308}, {%r1270, %r1264, %r1267, %r1261}, {%r26237, %r26205}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26312, %r26316}, {%r1270, %r1264, %r1267, %r1261}, {%r26244, %r26214}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26320, %r26324}, {%r1270, %r1264, %r1267, %r1261}, {%r26251, %r26223}, {%r26133, %r26133};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26295, %r26296, %r26296, %r29304;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29314, %r26300, %r26300, %r26295;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26303, %r26304, %r26304, %r29303;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29313, %r26308, %r26308, %r26303;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26311, %r26312, %r26312, %r29302;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29312, %r26316, %r26316, %r26311;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26319, %r26320, %r26320, %r29301;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29311, %r26324, %r26324, %r26319;
	// end inline asm
	add.s32 	%r29309, %r29299, 1;
	setp.ne.s32 	%p177, %r29309, 40;
	@%p177 bra 	LBB0_232;
// %bb.229:                             // %L160752
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_231;
// %bb.230:                             // %L160817
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r26327, %r28856, 193536;
	add.s32 	%r26328, %r83, %r26327;
	or.b32  	%r26329, %r26328, %r43;
	add.s32 	%r26330, %r26329, %r84;
	mul.wide.u32 	%rd1309, %r26330, 4;
	add.s64 	%rd1310, %rd4, %rd1309;
	st.global.u32 	[%rd1310], %r29314;
	add.s32 	%r26331, %r26329, %r85;
	mul.wide.u32 	%rd1311, %r26331, 4;
	add.s64 	%rd1312, %rd4, %rd1311;
	st.global.u32 	[%rd1312], %r29313;
	cvt.u64.u32 	%rd1313, %r84;
	cvt.u64.u32 	%rd1314, %r43;
	cvt.u64.u32 	%rd1315, %r83;
	cvt.u64.u32 	%rd1316, %r26327;
	add.s64 	%rd1317, %rd1316, %rd1315;
	add.s64 	%rd1318, %rd1317, %rd1314;
	add.s64 	%rd1319, %rd1318, %rd1313;
	shl.b64 	%rd1320, %rd1319, 2;
	add.s64 	%rd1321, %rd4, %rd1320;
	st.global.u32 	[%rd1321+387072], %r29312;
	cvt.u64.u32 	%rd1322, %r85;
	add.s64 	%rd1323, %rd1318, %rd1322;
	shl.b64 	%rd1324, %rd1323, 2;
	add.s64 	%rd1325, %rd4, %rd1324;
	st.global.u32 	[%rd1325+387072], %r29311;
LBB0_231:                               // %L161330
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29309, 0;
	mov.u32 	%r29311, %r29309;
	mov.u32 	%r29312, %r29309;
	mov.u32 	%r29313, %r29309;
	mov.u32 	%r29314, %r29309;
LBB0_232:                               // %L161331
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r26333, 0;
	mov.u32 	%r29315, %r26333;
	mov.u32 	%r29316, %r26333;
	mov.u32 	%r29317, %r26333;
	mov.u32 	%r29318, %r26333;
	@%p50 bra 	LBB0_305;
	bra.uni 	LBB0_233;
LBB0_305:                               // %oksrem55940
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r26334, %r77, %r845;
	add.s32 	%r26335, %r26334, %r78;
	add.s32 	%r26336, %r26335, %r79;
	add.s32 	%r26337, %r26336, %r28851;
	add.s32 	%r26338, %r26337, %r80;
	add.s32 	%r26339, %r26338, %r81;
	add.s32 	%r26340, %r26339, %r82;
	mul.wide.s32 	%rd1326, %r26340, 4;
	add.s64 	%rd1328, %rd52, %rd1326;
	ld.shared.u32 	%r29318, [%rd1328];
	or.b32  	%r26341, %r846, %r77;
	add.s32 	%r26342, %r26341, %r78;
	add.s32 	%r26343, %r26342, %r79;
	add.s32 	%r26344, %r26343, %r28851;
	add.s32 	%r26345, %r26344, %r80;
	add.s32 	%r26346, %r26345, %r81;
	add.s32 	%r26347, %r26346, %r82;
	mul.wide.u32 	%rd1329, %r26347, 4;
	add.s64 	%rd1330, %rd52, %rd1329;
	ld.shared.u32 	%r29317, [%rd1330];
	or.b32  	%r26348, %r26334, 32;
	add.s32 	%r26349, %r26348, %r78;
	add.s32 	%r26350, %r26349, %r79;
	add.s32 	%r26351, %r26350, %r28851;
	add.s32 	%r26352, %r26351, %r80;
	add.s32 	%r26353, %r26352, %r81;
	add.s32 	%r26354, %r26353, %r82;
	mul.wide.s32 	%rd1331, %r26354, 4;
	add.s64 	%rd1332, %rd52, %rd1331;
	ld.shared.u32 	%r29316, [%rd1332];
	add.s32 	%r26355, %r845, 8288;
	or.b32  	%r26356, %r26355, %r77;
	add.s32 	%r26357, %r26356, %r78;
	add.s32 	%r26358, %r26357, %r79;
	add.s32 	%r26359, %r26358, %r28851;
	add.s32 	%r26360, %r26359, %r80;
	add.s32 	%r26361, %r26360, %r81;
	add.s32 	%r26362, %r26361, %r82;
	mul.wide.u32 	%rd1333, %r26362, 4;
	add.s64 	%rd1334, %rd52, %rd1333;
	ld.shared.u32 	%r29315, [%rd1334];
LBB0_233:                               // %L162450
                                        //   in Loop: Header=BB0_16 Depth=1
	shr.u32 	%r27479, %r157, 8;
	shr.u32 	%r27525, %r159, 8;
	shr.u32 	%r27571, %r161, 8;
	shr.u32 	%r27617, %r163, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26395, %r26398}, {%r1202, %r1199}, {%r29318}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26404, %r26407}, {%r1202, %r1199}, {%r29317}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26413, %r26416}, {%r1202, %r1199}, {%r29316}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r26422, %r26425}, {%r1202, %r1199}, {%r29315}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26391, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26393, %r26391, %r26395;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26396, %r1237, %r26398, %r26393;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26400, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26402, %r26400, %r26404;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26405, %r1237, %r26407, %r26402;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26409, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26411, %r26409, %r26413;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26414, %r1237, %r26416, %r26411;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r26418, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26420, %r26418, %r26422;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26423, %r1237, %r26425, %r26420;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26427, %r1240, %r26398;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26430, %r1237, %r26395, %r26427;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26434, %r1240, %r26407;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26437, %r1237, %r26404, %r26434;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26441, %r1240, %r26416;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26444, %r1237, %r26413, %r26441;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r26448, %r1240, %r26425;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26451, %r1237, %r26422, %r26448;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26496, %r26500}, {%r1270, %r1264, %r1267, %r1261}, {%r26430, %r26396}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26504, %r26508}, {%r1270, %r1264, %r1267, %r1261}, {%r26437, %r26405}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26512, %r26516}, {%r1270, %r1264, %r1267, %r1261}, {%r26444, %r26414}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r26520, %r26524}, {%r1270, %r1264, %r1267, %r1261}, {%r26451, %r26423}, {%r26333, %r26333};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26495, %r26496, %r26496, %r29314;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29324, %r26500, %r26500, %r26495;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26503, %r26504, %r26504, %r29313;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29323, %r26508, %r26508, %r26503;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26511, %r26512, %r26512, %r29312;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29322, %r26516, %r26516, %r26511;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r26519, %r26520, %r26520, %r29311;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29321, %r26524, %r26524, %r26519;
	// end inline asm
	add.s32 	%r29319, %r29309, 1;
	setp.ne.s32 	%p180, %r29319, 40;
	@%p180 bra 	LBB0_237;
// %bb.234:                             // %L162633
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_236;
// %bb.235:                             // %L162698
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r26527, %r28856, 193536;
	add.s32 	%r26528, %r83, %r26527;
	or.b32  	%r26529, %r26528, %r43;
	add.s32 	%r26530, %r26529, %r84;
	mul.wide.u32 	%rd1335, %r26530, 4;
	add.s64 	%rd1336, %rd4, %rd1335;
	st.global.u32 	[%rd1336], %r29324;
	add.s32 	%r26531, %r26529, %r85;
	mul.wide.u32 	%rd1337, %r26531, 4;
	add.s64 	%rd1338, %rd4, %rd1337;
	st.global.u32 	[%rd1338], %r29323;
	cvt.u64.u32 	%rd1339, %r84;
	cvt.u64.u32 	%rd1340, %r43;
	cvt.u64.u32 	%rd1341, %r83;
	cvt.u64.u32 	%rd1342, %r26527;
	add.s64 	%rd1343, %rd1342, %rd1341;
	add.s64 	%rd1344, %rd1343, %rd1340;
	add.s64 	%rd1345, %rd1344, %rd1339;
	shl.b64 	%rd1346, %rd1345, 2;
	add.s64 	%rd1347, %rd4, %rd1346;
	st.global.u32 	[%rd1347+387072], %r29322;
	cvt.u64.u32 	%rd1348, %r85;
	add.s64 	%rd1349, %rd1344, %rd1348;
	shl.b64 	%rd1350, %rd1349, 2;
	add.s64 	%rd1351, %rd4, %rd1350;
	st.global.u32 	[%rd1351+387072], %r29321;
LBB0_236:                               // %L163211
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29319, 0;
	mov.u32 	%r29321, %r29319;
	mov.u32 	%r29322, %r29319;
	mov.u32 	%r29323, %r29319;
	mov.u32 	%r29324, %r29319;
LBB0_237:                               // %L163212
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r27480, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r27491, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r27477, %r1979, %r27479, %r27480, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27481, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27482, %r27480, %r27481;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27485, %r27477, %r27482;
	// end inline asm
	// begin inline asm
	lop3.b32 %r27488, %r1990, %r27479, %r27491, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27492, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27493, %r27491, %r27492;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27496, %r27488, %r27493;
	// end inline asm
	// begin inline asm
	mov.b32 %r27526, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r27537, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r27523, %r1979, %r27525, %r27526, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27527, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27528, %r27526, %r27527;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27531, %r27523, %r27528;
	// end inline asm
	// begin inline asm
	lop3.b32 %r27534, %r1990, %r27525, %r27537, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27538, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27539, %r27537, %r27538;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27542, %r27534, %r27539;
	// end inline asm
	// begin inline asm
	mov.b32 %r27572, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r27583, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r27569, %r1979, %r27571, %r27572, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27573, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27574, %r27572, %r27573;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27577, %r27569, %r27574;
	// end inline asm
	// begin inline asm
	lop3.b32 %r27580, %r1990, %r27571, %r27583, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27584, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27585, %r27583, %r27584;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27588, %r27580, %r27585;
	// end inline asm
	// begin inline asm
	mov.b32 %r27618, {%rs216, %rs216};
	// end inline asm
	// begin inline asm
	mov.b32 %r27629, {%rs218, %rs218};
	// end inline asm
	// begin inline asm
	lop3.b32 %r27615, %r1979, %r27617, %r27618, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27619, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27620, %r27618, %r27619;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27623, %r27615, %r27620;
	// end inline asm
	// begin inline asm
	lop3.b32 %r27626, %r1990, %r27617, %r27629, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r27630, {%rs222, %rs222};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r27631, %r27629, %r27630;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r27634, %r27626, %r27631;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r27485;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27637, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r27496;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27640, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r27531;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27643, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r27542;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27646, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r27577;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27649, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r27588;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27652, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28852;
    mov.b32 {%r2im, %r2re}, %r27623;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27655, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r28853;
    mov.b32 {%r2im, %r2re}, %r27634;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r27658, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r27989, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27721, %r27724}, {%r1202, %r1199}, {%r27637}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27730, %r27733}, {%r1202, %r1199}, {%r27640}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27739, %r27742}, {%r1202, %r1199}, {%r27643}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27748, %r27751}, {%r1202, %r1199}, {%r27646}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27757, %r27760}, {%r1202, %r1199}, {%r27649}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27766, %r27769}, {%r1202, %r1199}, {%r27652}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27775, %r27778}, {%r1202, %r1199}, {%r27655}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r27784, %r27787}, {%r1202, %r1199}, {%r27658}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27717, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27719, %r27717, %r27721;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27722, %r1237, %r27724, %r27719;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27726, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27728, %r27726, %r27730;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27731, %r1237, %r27733, %r27728;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27735, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27737, %r27735, %r27739;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27740, %r1237, %r27742, %r27737;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27744, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27746, %r27744, %r27748;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27749, %r1237, %r27751, %r27746;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27753, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27755, %r27753, %r27757;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27758, %r1237, %r27760, %r27755;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27762, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27764, %r27762, %r27766;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27767, %r1237, %r27769, %r27764;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27771, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27773, %r27771, %r27775;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27776, %r1237, %r27778, %r27773;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r27780, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27782, %r27780, %r27784;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27785, %r1237, %r27787, %r27782;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27789, %r1240, %r27724;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27792, %r1237, %r27721, %r27789;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27796, %r1240, %r27733;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27799, %r1237, %r27730, %r27796;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27803, %r1240, %r27742;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27806, %r1237, %r27739, %r27803;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27810, %r1240, %r27751;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27813, %r1237, %r27748, %r27810;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27817, %r1240, %r27760;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27820, %r1237, %r27757, %r27817;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27824, %r1240, %r27769;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27827, %r1237, %r27766, %r27824;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27831, %r1240, %r27778;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27834, %r1237, %r27775, %r27831;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r27838, %r1240, %r27787;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r27841, %r1237, %r27784, %r27838;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27926, %r27927}, {%r1270, %r1264, %r1267, %r1261}, {%r27792, %r27722}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27934, %r27935}, {%r1270, %r1264, %r1267, %r1261}, {%r27799, %r27731}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27942, %r27943}, {%r1270, %r1264, %r1267, %r1261}, {%r27806, %r27740}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27950, %r27951}, {%r1270, %r1264, %r1267, %r1261}, {%r27813, %r27749}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27958, %r27959}, {%r1270, %r1264, %r1267, %r1261}, {%r27820, %r27758}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27966, %r27967}, {%r1270, %r1264, %r1267, %r1261}, {%r27827, %r27767}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27974, %r27975}, {%r1270, %r1264, %r1267, %r1261}, {%r27834, %r27776}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r27982, %r27983}, {%r1270, %r1264, %r1267, %r1261}, {%r27841, %r27785}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	prmt.b32 %r27925, %r27926, %r27927, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27929, %r27926, %r27927, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27933, %r27934, %r27935, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27937, %r27934, %r27935, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27941, %r27942, %r27943, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27945, %r27942, %r27943, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27949, %r27950, %r27951, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27953, %r27950, %r27951, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27957, %r27958, %r27959, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27961, %r27958, %r27959, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27965, %r27966, %r27967, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27969, %r27966, %r27967, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27973, %r27974, %r27975, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27977, %r27974, %r27975, %r1651;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27981, %r27982, %r27983, %r1647;
	// end inline asm
	// begin inline asm
	prmt.b32 %r27985, %r27982, %r27983, %r1651;
	// end inline asm
	st.shared.u32 	[%rd6], %r27925;
	st.shared.u32 	[%rd7+33024], %r27929;
	st.shared.u32 	[%rd8], %r27933;
	st.shared.u32 	[%rd9], %r27937;
	add.s16 	%rs3760, %rs3785, 22;
	shr.s16 	%rs3761, %rs3760, 15;
	shr.u16 	%rs3762, %rs3761, 14;
	add.s16 	%rs3763, %rs3760, %rs3762;
	and.b16  	%rs3764, %rs3763, -4;
	sub.s16 	%rs3765, %rs3760, %rs3764;
	mul.wide.s16 	%r911, %rs3765, 64;
	add.s32 	%r27990, %r71, %r911;
	add.s32 	%r27991, %r27990, %r28848;
	add.s32 	%r27992, %r27991, %r72;
	add.s32 	%r27993, %r27992, %r28849;
	add.s32 	%r27994, %r27993, %r73;
	add.s32 	%r27995, %r27994, %r74;
	add.s32 	%r27996, %r27995, %r75;
	mul.wide.s32 	%rd1352, %r27996, 4;
	add.s64 	%rd1354, %rd52, %rd1352;
	st.shared.u32 	[%rd1354], %r27941;
	add.s32 	%r912, %r911, 8256;
	add.s32 	%r27997, %r912, %r71;
	add.s32 	%r27998, %r27997, %r28848;
	add.s32 	%r27999, %r27998, %r72;
	add.s32 	%r28000, %r27999, %r28849;
	add.s32 	%r28001, %r28000, %r73;
	add.s32 	%r28002, %r28001, %r74;
	add.s32 	%r28003, %r28002, %r75;
	mul.wide.u32 	%rd1355, %r28003, 4;
	add.s64 	%rd1356, %rd52, %rd1355;
	st.shared.u32 	[%rd1356], %r27945;
	add.s32 	%r28004, %r27990, %r76;
	add.s32 	%r28005, %r28004, %r72;
	add.s32 	%r28006, %r28005, %r28849;
	add.s32 	%r28007, %r28006, %r73;
	add.s32 	%r28008, %r28007, %r74;
	add.s32 	%r28009, %r28008, %r75;
	mul.wide.s32 	%rd1357, %r28009, 4;
	add.s64 	%rd1358, %rd52, %rd1357;
	st.shared.u32 	[%rd1358], %r27949;
	add.s32 	%r28010, %r27997, %r76;
	add.s32 	%r28011, %r28010, %r72;
	add.s32 	%r28012, %r28011, %r28849;
	add.s32 	%r28013, %r28012, %r73;
	add.s32 	%r28014, %r28013, %r74;
	add.s32 	%r28015, %r28014, %r75;
	mul.wide.s32 	%rd1359, %r28015, 4;
	add.s64 	%rd1360, %rd52, %rd1359;
	st.shared.u32 	[%rd1360], %r27953;
	add.s16 	%rs3766, %rs3785, 23;
	shr.s16 	%rs3767, %rs3766, 15;
	shr.u16 	%rs3768, %rs3767, 14;
	add.s16 	%rs3769, %rs3766, %rs3768;
	and.b16  	%rs3770, %rs3769, -4;
	sub.s16 	%rs3771, %rs3766, %rs3770;
	mul.wide.s16 	%r913, %rs3771, 64;
	add.s32 	%r28016, %r71, %r913;
	add.s32 	%r28017, %r28016, %r28848;
	add.s32 	%r28018, %r28017, %r72;
	add.s32 	%r28019, %r28018, %r28849;
	add.s32 	%r28020, %r28019, %r73;
	add.s32 	%r28021, %r28020, %r74;
	add.s32 	%r28022, %r28021, %r75;
	mul.wide.s32 	%rd1361, %r28022, 4;
	add.s64 	%rd1362, %rd52, %rd1361;
	st.shared.u32 	[%rd1362], %r27957;
	add.s32 	%r914, %r913, 8256;
	add.s32 	%r28023, %r914, %r71;
	add.s32 	%r28024, %r28023, %r28848;
	add.s32 	%r28025, %r28024, %r72;
	add.s32 	%r28026, %r28025, %r28849;
	add.s32 	%r28027, %r28026, %r73;
	add.s32 	%r28028, %r28027, %r74;
	add.s32 	%r28029, %r28028, %r75;
	mul.wide.u32 	%rd1363, %r28029, 4;
	add.s64 	%rd1364, %rd52, %rd1363;
	st.shared.u32 	[%rd1364], %r27961;
	add.s32 	%r28030, %r28016, %r76;
	add.s32 	%r28031, %r28030, %r72;
	add.s32 	%r28032, %r28031, %r28849;
	add.s32 	%r28033, %r28032, %r73;
	add.s32 	%r28034, %r28033, %r74;
	add.s32 	%r28035, %r28034, %r75;
	mul.wide.s32 	%rd1365, %r28035, 4;
	add.s64 	%rd1366, %rd52, %rd1365;
	st.shared.u32 	[%rd1366], %r27965;
	add.s32 	%r28036, %r28023, %r76;
	add.s32 	%r28037, %r28036, %r72;
	add.s32 	%r28038, %r28037, %r28849;
	add.s32 	%r28039, %r28038, %r73;
	add.s32 	%r28040, %r28039, %r74;
	add.s32 	%r28041, %r28040, %r75;
	mul.wide.s32 	%rd1367, %r28041, 4;
	add.s64 	%rd1368, %rd52, %rd1367;
	st.shared.u32 	[%rd1368], %r27969;
	add.s16 	%rs3772, %rs3785, 24;
	shr.s16 	%rs3773, %rs3772, 15;
	shr.u16 	%rs3774, %rs3773, 14;
	add.s16 	%rs3775, %rs3772, %rs3774;
	and.b16  	%rs3776, %rs3775, -4;
	sub.s16 	%rs3777, %rs3772, %rs3776;
	mul.wide.s16 	%r915, %rs3777, 64;
	add.s32 	%r28042, %r71, %r915;
	add.s32 	%r28043, %r28042, %r28848;
	add.s32 	%r28044, %r28043, %r72;
	add.s32 	%r28045, %r28044, %r28849;
	add.s32 	%r28046, %r28045, %r73;
	add.s32 	%r28047, %r28046, %r74;
	add.s32 	%r28048, %r28047, %r75;
	mul.wide.s32 	%rd1369, %r28048, 4;
	add.s64 	%rd1370, %rd52, %rd1369;
	st.shared.u32 	[%rd1370], %r27973;
	add.s32 	%r916, %r915, 8256;
	add.s32 	%r28049, %r916, %r71;
	add.s32 	%r28050, %r28049, %r28848;
	add.s32 	%r28051, %r28050, %r72;
	add.s32 	%r28052, %r28051, %r28849;
	add.s32 	%r28053, %r28052, %r73;
	add.s32 	%r28054, %r28053, %r74;
	add.s32 	%r28055, %r28054, %r75;
	mul.wide.u32 	%rd1371, %r28055, 4;
	add.s64 	%rd1372, %rd52, %rd1371;
	st.shared.u32 	[%rd1372], %r27977;
	add.s32 	%r28056, %r28042, %r76;
	add.s32 	%r28057, %r28056, %r72;
	add.s32 	%r28058, %r28057, %r28849;
	add.s32 	%r28059, %r28058, %r73;
	add.s32 	%r28060, %r28059, %r74;
	add.s32 	%r28061, %r28060, %r75;
	mul.wide.s32 	%rd1373, %r28061, 4;
	add.s64 	%rd1374, %rd52, %rd1373;
	st.shared.u32 	[%rd1374], %r27981;
	add.s32 	%r28062, %r28049, %r76;
	add.s32 	%r28063, %r28062, %r72;
	add.s32 	%r28064, %r28063, %r28849;
	add.s32 	%r28065, %r28064, %r73;
	add.s32 	%r28066, %r28065, %r74;
	add.s32 	%r28067, %r28066, %r75;
	mul.wide.s32 	%rd1375, %r28067, 4;
	add.s64 	%rd1376, %rd52, %rd1375;
	st.shared.u32 	[%rd1376], %r27985;
	bar.sync 	0;
	mov.u32 	%r29325, %r27989;
	mov.u32 	%r29326, %r27989;
	mov.u32 	%r29327, %r27989;
	mov.u32 	%r29328, %r27989;
	@%p50 bra 	LBB0_306;
	bra.uni 	LBB0_238;
LBB0_306:                               // %oksrem58542
                                        //   in Loop: Header=BB0_16 Depth=1
	ld.shared.u32 	%r29328, [%rd10];
	ld.shared.u32 	%r29327, [%rd11+33024];
	ld.shared.u32 	%r29326, [%rd11+128];
	ld.shared.u32 	%r29325, [%rd11+33152];
LBB0_238:                               // %L170481
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28100, %r28103}, {%r1202, %r1199}, {%r29328}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28109, %r28112}, {%r1202, %r1199}, {%r29327}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28118, %r28121}, {%r1202, %r1199}, {%r29326}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28127, %r28130}, {%r1202, %r1199}, {%r29325}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28096, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28098, %r28096, %r28100;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28101, %r1237, %r28103, %r28098;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28105, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28107, %r28105, %r28109;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28110, %r1237, %r28112, %r28107;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28114, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28116, %r28114, %r28118;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28119, %r1237, %r28121, %r28116;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28123, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28125, %r28123, %r28127;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28128, %r1237, %r28130, %r28125;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28132, %r1240, %r28103;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28135, %r1237, %r28100, %r28132;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28139, %r1240, %r28112;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28142, %r1237, %r28109, %r28139;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28146, %r1240, %r28121;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28149, %r1237, %r28118, %r28146;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28153, %r1240, %r28130;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28156, %r1237, %r28127, %r28153;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28201, %r28205}, {%r1270, %r1264, %r1267, %r1261}, {%r28135, %r28101}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28209, %r28213}, {%r1270, %r1264, %r1267, %r1261}, {%r28142, %r28110}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28217, %r28221}, {%r1270, %r1264, %r1267, %r1261}, {%r28149, %r28119}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28225, %r28229}, {%r1270, %r1264, %r1267, %r1261}, {%r28156, %r28128}, {%r27989, %r27989};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28200, %r28201, %r28201, %r29324;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29334, %r28205, %r28205, %r28200;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28208, %r28209, %r28209, %r29323;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29333, %r28213, %r28213, %r28208;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28216, %r28217, %r28217, %r29322;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29332, %r28221, %r28221, %r28216;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28224, %r28225, %r28225, %r29321;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29331, %r28229, %r28229, %r28224;
	// end inline asm
	add.s32 	%r29329, %r29319, 1;
	setp.ne.s32 	%p183, %r29329, 40;
	@%p183 bra 	LBB0_242;
// %bb.239:                             // %L170664
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_241;
// %bb.240:                             // %L170729
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r28232, %r28856, 193536;
	add.s32 	%r28233, %r83, %r28232;
	or.b32  	%r28234, %r28233, %r43;
	add.s32 	%r28235, %r28234, %r84;
	mul.wide.u32 	%rd1377, %r28235, 4;
	add.s64 	%rd1378, %rd4, %rd1377;
	st.global.u32 	[%rd1378], %r29334;
	add.s32 	%r28236, %r28234, %r85;
	mul.wide.u32 	%rd1379, %r28236, 4;
	add.s64 	%rd1380, %rd4, %rd1379;
	st.global.u32 	[%rd1380], %r29333;
	cvt.u64.u32 	%rd1381, %r84;
	cvt.u64.u32 	%rd1382, %r43;
	cvt.u64.u32 	%rd1383, %r83;
	cvt.u64.u32 	%rd1384, %r28232;
	add.s64 	%rd1385, %rd1384, %rd1383;
	add.s64 	%rd1386, %rd1385, %rd1382;
	add.s64 	%rd1387, %rd1386, %rd1381;
	shl.b64 	%rd1388, %rd1387, 2;
	add.s64 	%rd1389, %rd4, %rd1388;
	st.global.u32 	[%rd1389+387072], %r29332;
	cvt.u64.u32 	%rd1390, %r85;
	add.s64 	%rd1391, %rd1386, %rd1390;
	shl.b64 	%rd1392, %rd1391, 2;
	add.s64 	%rd1393, %rd4, %rd1392;
	st.global.u32 	[%rd1393+387072], %r29331;
LBB0_241:                               // %L171242
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29329, 0;
	mov.u32 	%r29331, %r29329;
	mov.u32 	%r29332, %r29329;
	mov.u32 	%r29333, %r29329;
	mov.u32 	%r29334, %r29329;
LBB0_242:                               // %L171243
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r28238, 0;
	mov.u32 	%r29335, %r28238;
	mov.u32 	%r29336, %r28238;
	mov.u32 	%r29337, %r28238;
	mov.u32 	%r29338, %r28238;
	@%p50 bra 	LBB0_307;
	bra.uni 	LBB0_243;
LBB0_307:                               // %oksrem59240
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r28239, %r77, %r911;
	add.s32 	%r28240, %r28239, %r78;
	add.s32 	%r28241, %r28240, %r79;
	add.s32 	%r28242, %r28241, %r28851;
	add.s32 	%r28243, %r28242, %r80;
	add.s32 	%r28244, %r28243, %r81;
	add.s32 	%r28245, %r28244, %r82;
	mul.wide.s32 	%rd1394, %r28245, 4;
	add.s64 	%rd1396, %rd52, %rd1394;
	ld.shared.u32 	%r29338, [%rd1396];
	or.b32  	%r28246, %r912, %r77;
	add.s32 	%r28247, %r28246, %r78;
	add.s32 	%r28248, %r28247, %r79;
	add.s32 	%r28249, %r28248, %r28851;
	add.s32 	%r28250, %r28249, %r80;
	add.s32 	%r28251, %r28250, %r81;
	add.s32 	%r28252, %r28251, %r82;
	mul.wide.u32 	%rd1397, %r28252, 4;
	add.s64 	%rd1398, %rd52, %rd1397;
	ld.shared.u32 	%r29337, [%rd1398];
	or.b32  	%r28253, %r28239, 32;
	add.s32 	%r28254, %r28253, %r78;
	add.s32 	%r28255, %r28254, %r79;
	add.s32 	%r28256, %r28255, %r28851;
	add.s32 	%r28257, %r28256, %r80;
	add.s32 	%r28258, %r28257, %r81;
	add.s32 	%r28259, %r28258, %r82;
	mul.wide.s32 	%rd1399, %r28259, 4;
	add.s64 	%rd1400, %rd52, %rd1399;
	ld.shared.u32 	%r29336, [%rd1400];
	add.s32 	%r28260, %r911, 8288;
	or.b32  	%r28261, %r28260, %r77;
	add.s32 	%r28262, %r28261, %r78;
	add.s32 	%r28263, %r28262, %r79;
	add.s32 	%r28264, %r28263, %r28851;
	add.s32 	%r28265, %r28264, %r80;
	add.s32 	%r28266, %r28265, %r81;
	add.s32 	%r28267, %r28266, %r82;
	mul.wide.u32 	%rd1401, %r28267, 4;
	add.s64 	%rd1402, %rd52, %rd1401;
	ld.shared.u32 	%r29335, [%rd1402];
LBB0_243:                               // %L172362
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28300, %r28303}, {%r1202, %r1199}, {%r29338}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28309, %r28312}, {%r1202, %r1199}, {%r29337}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28318, %r28321}, {%r1202, %r1199}, {%r29336}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28327, %r28330}, {%r1202, %r1199}, {%r29335}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28296, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28298, %r28296, %r28300;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28301, %r1237, %r28303, %r28298;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28305, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28307, %r28305, %r28309;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28310, %r1237, %r28312, %r28307;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28314, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28316, %r28314, %r28318;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28319, %r1237, %r28321, %r28316;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28323, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28325, %r28323, %r28327;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28328, %r1237, %r28330, %r28325;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28332, %r1240, %r28303;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28335, %r1237, %r28300, %r28332;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28339, %r1240, %r28312;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28342, %r1237, %r28309, %r28339;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28346, %r1240, %r28321;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28349, %r1237, %r28318, %r28346;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28353, %r1240, %r28330;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28356, %r1237, %r28327, %r28353;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28401, %r28405}, {%r1270, %r1264, %r1267, %r1261}, {%r28335, %r28301}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28409, %r28413}, {%r1270, %r1264, %r1267, %r1261}, {%r28342, %r28310}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28417, %r28421}, {%r1270, %r1264, %r1267, %r1261}, {%r28349, %r28319}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28425, %r28429}, {%r1270, %r1264, %r1267, %r1261}, {%r28356, %r28328}, {%r28238, %r28238};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28400, %r28401, %r28401, %r29334;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29344, %r28405, %r28405, %r28400;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28408, %r28409, %r28409, %r29333;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29343, %r28413, %r28413, %r28408;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28416, %r28417, %r28417, %r29332;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29342, %r28421, %r28421, %r28416;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28424, %r28425, %r28425, %r29331;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29341, %r28429, %r28429, %r28424;
	// end inline asm
	add.s32 	%r29339, %r29329, 1;
	setp.ne.s32 	%p186, %r29339, 40;
	@%p186 bra 	LBB0_247;
// %bb.244:                             // %L172545
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_246;
// %bb.245:                             // %L172610
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r28432, %r28856, 193536;
	add.s32 	%r28433, %r83, %r28432;
	or.b32  	%r28434, %r28433, %r43;
	add.s32 	%r28435, %r28434, %r84;
	mul.wide.u32 	%rd1403, %r28435, 4;
	add.s64 	%rd1404, %rd4, %rd1403;
	st.global.u32 	[%rd1404], %r29344;
	add.s32 	%r28436, %r28434, %r85;
	mul.wide.u32 	%rd1405, %r28436, 4;
	add.s64 	%rd1406, %rd4, %rd1405;
	st.global.u32 	[%rd1406], %r29343;
	cvt.u64.u32 	%rd1407, %r84;
	cvt.u64.u32 	%rd1408, %r43;
	cvt.u64.u32 	%rd1409, %r83;
	cvt.u64.u32 	%rd1410, %r28432;
	add.s64 	%rd1411, %rd1410, %rd1409;
	add.s64 	%rd1412, %rd1411, %rd1408;
	add.s64 	%rd1413, %rd1412, %rd1407;
	shl.b64 	%rd1414, %rd1413, 2;
	add.s64 	%rd1415, %rd4, %rd1414;
	st.global.u32 	[%rd1415+387072], %r29342;
	cvt.u64.u32 	%rd1416, %r85;
	add.s64 	%rd1417, %rd1412, %rd1416;
	shl.b64 	%rd1418, %rd1417, 2;
	add.s64 	%rd1419, %rd4, %rd1418;
	st.global.u32 	[%rd1419+387072], %r29341;
LBB0_246:                               // %L173123
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29339, 0;
	mov.u32 	%r29341, %r29339;
	mov.u32 	%r29342, %r29339;
	mov.u32 	%r29343, %r29339;
	mov.u32 	%r29344, %r29339;
LBB0_247:                               // %L173124
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r28438, 0;
	mov.u32 	%r29345, %r28438;
	mov.u32 	%r29346, %r28438;
	mov.u32 	%r29347, %r28438;
	mov.u32 	%r29348, %r28438;
	@%p50 bra 	LBB0_308;
	bra.uni 	LBB0_248;
LBB0_308:                               // %oksrem59938
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r28439, %r77, %r913;
	add.s32 	%r28440, %r28439, %r78;
	add.s32 	%r28441, %r28440, %r79;
	add.s32 	%r28442, %r28441, %r28851;
	add.s32 	%r28443, %r28442, %r80;
	add.s32 	%r28444, %r28443, %r81;
	add.s32 	%r28445, %r28444, %r82;
	mul.wide.s32 	%rd1420, %r28445, 4;
	add.s64 	%rd1422, %rd52, %rd1420;
	ld.shared.u32 	%r29348, [%rd1422];
	or.b32  	%r28446, %r914, %r77;
	add.s32 	%r28447, %r28446, %r78;
	add.s32 	%r28448, %r28447, %r79;
	add.s32 	%r28449, %r28448, %r28851;
	add.s32 	%r28450, %r28449, %r80;
	add.s32 	%r28451, %r28450, %r81;
	add.s32 	%r28452, %r28451, %r82;
	mul.wide.u32 	%rd1423, %r28452, 4;
	add.s64 	%rd1424, %rd52, %rd1423;
	ld.shared.u32 	%r29347, [%rd1424];
	or.b32  	%r28453, %r28439, 32;
	add.s32 	%r28454, %r28453, %r78;
	add.s32 	%r28455, %r28454, %r79;
	add.s32 	%r28456, %r28455, %r28851;
	add.s32 	%r28457, %r28456, %r80;
	add.s32 	%r28458, %r28457, %r81;
	add.s32 	%r28459, %r28458, %r82;
	mul.wide.s32 	%rd1425, %r28459, 4;
	add.s64 	%rd1426, %rd52, %rd1425;
	ld.shared.u32 	%r29346, [%rd1426];
	add.s32 	%r28460, %r913, 8288;
	or.b32  	%r28461, %r28460, %r77;
	add.s32 	%r28462, %r28461, %r78;
	add.s32 	%r28463, %r28462, %r79;
	add.s32 	%r28464, %r28463, %r28851;
	add.s32 	%r28465, %r28464, %r80;
	add.s32 	%r28466, %r28465, %r81;
	add.s32 	%r28467, %r28466, %r82;
	mul.wide.u32 	%rd1427, %r28467, 4;
	add.s64 	%rd1428, %rd52, %rd1427;
	ld.shared.u32 	%r29345, [%rd1428];
LBB0_248:                               // %L174243
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28500, %r28503}, {%r1202, %r1199}, {%r29348}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28509, %r28512}, {%r1202, %r1199}, {%r29347}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28518, %r28521}, {%r1202, %r1199}, {%r29346}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28527, %r28530}, {%r1202, %r1199}, {%r29345}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28496, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28498, %r28496, %r28500;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28501, %r1237, %r28503, %r28498;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28505, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28507, %r28505, %r28509;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28510, %r1237, %r28512, %r28507;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28514, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28516, %r28514, %r28518;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28519, %r1237, %r28521, %r28516;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28523, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28525, %r28523, %r28527;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28528, %r1237, %r28530, %r28525;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28532, %r1240, %r28503;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28535, %r1237, %r28500, %r28532;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28539, %r1240, %r28512;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28542, %r1237, %r28509, %r28539;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28546, %r1240, %r28521;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28549, %r1237, %r28518, %r28546;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28553, %r1240, %r28530;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28556, %r1237, %r28527, %r28553;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28601, %r28605}, {%r1270, %r1264, %r1267, %r1261}, {%r28535, %r28501}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28609, %r28613}, {%r1270, %r1264, %r1267, %r1261}, {%r28542, %r28510}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28617, %r28621}, {%r1270, %r1264, %r1267, %r1261}, {%r28549, %r28519}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28625, %r28629}, {%r1270, %r1264, %r1267, %r1261}, {%r28556, %r28528}, {%r28438, %r28438};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28600, %r28601, %r28601, %r29344;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29354, %r28605, %r28605, %r28600;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28608, %r28609, %r28609, %r29343;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29353, %r28613, %r28613, %r28608;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28616, %r28617, %r28617, %r29342;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29352, %r28621, %r28621, %r28616;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28624, %r28625, %r28625, %r29341;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r29351, %r28629, %r28629, %r28624;
	// end inline asm
	add.s32 	%r29349, %r29339, 1;
	setp.ne.s32 	%p189, %r29349, 40;
	@%p189 bra 	LBB0_252;
// %bb.249:                             // %L174426
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_251;
// %bb.250:                             // %L174491
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r28632, %r28856, 193536;
	add.s32 	%r28633, %r83, %r28632;
	or.b32  	%r28634, %r28633, %r43;
	add.s32 	%r28635, %r28634, %r84;
	mul.wide.u32 	%rd1429, %r28635, 4;
	add.s64 	%rd1430, %rd4, %rd1429;
	st.global.u32 	[%rd1430], %r29354;
	add.s32 	%r28636, %r28634, %r85;
	mul.wide.u32 	%rd1431, %r28636, 4;
	add.s64 	%rd1432, %rd4, %rd1431;
	st.global.u32 	[%rd1432], %r29353;
	cvt.u64.u32 	%rd1433, %r84;
	cvt.u64.u32 	%rd1434, %r43;
	cvt.u64.u32 	%rd1435, %r83;
	cvt.u64.u32 	%rd1436, %r28632;
	add.s64 	%rd1437, %rd1436, %rd1435;
	add.s64 	%rd1438, %rd1437, %rd1434;
	add.s64 	%rd1439, %rd1438, %rd1433;
	shl.b64 	%rd1440, %rd1439, 2;
	add.s64 	%rd1441, %rd4, %rd1440;
	st.global.u32 	[%rd1441+387072], %r29352;
	cvt.u64.u32 	%rd1442, %r85;
	add.s64 	%rd1443, %rd1438, %rd1442;
	shl.b64 	%rd1444, %rd1443, 2;
	add.s64 	%rd1445, %rd4, %rd1444;
	st.global.u32 	[%rd1445+387072], %r29351;
LBB0_251:                               // %L175004
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r28856, %r28856, 1;
	mov.u32 	%r29349, 0;
	mov.u32 	%r29351, %r29349;
	mov.u32 	%r29352, %r29349;
	mov.u32 	%r29353, %r29349;
	mov.u32 	%r29354, %r29349;
LBB0_252:                               // %L175005
                                        //   in Loop: Header=BB0_16 Depth=1
	mov.u32 	%r28638, 0;
	mov.u32 	%r29355, %r28638;
	mov.u32 	%r29356, %r28638;
	mov.u32 	%r29357, %r28638;
	mov.u32 	%r29358, %r28638;
	@%p50 bra 	LBB0_309;
	bra.uni 	LBB0_253;
LBB0_309:                               // %oksrem60636
                                        //   in Loop: Header=BB0_16 Depth=1
	or.b32  	%r28639, %r77, %r915;
	add.s32 	%r28640, %r28639, %r78;
	add.s32 	%r28641, %r28640, %r79;
	add.s32 	%r28642, %r28641, %r28851;
	add.s32 	%r28643, %r28642, %r80;
	add.s32 	%r28644, %r28643, %r81;
	add.s32 	%r28645, %r28644, %r82;
	mul.wide.s32 	%rd1446, %r28645, 4;
	add.s64 	%rd1448, %rd52, %rd1446;
	ld.shared.u32 	%r29358, [%rd1448];
	or.b32  	%r28646, %r916, %r77;
	add.s32 	%r28647, %r28646, %r78;
	add.s32 	%r28648, %r28647, %r79;
	add.s32 	%r28649, %r28648, %r28851;
	add.s32 	%r28650, %r28649, %r80;
	add.s32 	%r28651, %r28650, %r81;
	add.s32 	%r28652, %r28651, %r82;
	mul.wide.u32 	%rd1449, %r28652, 4;
	add.s64 	%rd1450, %rd52, %rd1449;
	ld.shared.u32 	%r29357, [%rd1450];
	or.b32  	%r28653, %r28639, 32;
	add.s32 	%r28654, %r28653, %r78;
	add.s32 	%r28655, %r28654, %r79;
	add.s32 	%r28656, %r28655, %r28851;
	add.s32 	%r28657, %r28656, %r80;
	add.s32 	%r28658, %r28657, %r81;
	add.s32 	%r28659, %r28658, %r82;
	mul.wide.s32 	%rd1451, %r28659, 4;
	add.s64 	%rd1452, %rd52, %rd1451;
	ld.shared.u32 	%r29356, [%rd1452];
	add.s32 	%r28660, %r915, 8288;
	or.b32  	%r28661, %r28660, %r77;
	add.s32 	%r28662, %r28661, %r78;
	add.s32 	%r28663, %r28662, %r79;
	add.s32 	%r28664, %r28663, %r28851;
	add.s32 	%r28665, %r28664, %r80;
	add.s32 	%r28666, %r28665, %r81;
	add.s32 	%r28667, %r28666, %r82;
	mul.wide.u32 	%rd1453, %r28667, 4;
	add.s64 	%rd1454, %rd52, %rd1453;
	ld.shared.u32 	%r29355, [%rd1454];
LBB0_253:                               // %L176124
                                        //   in Loop: Header=BB0_16 Depth=1
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28700, %r28703}, {%r1202, %r1199}, {%r29358}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28709, %r28712}, {%r1202, %r1199}, {%r29357}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28718, %r28721}, {%r1202, %r1199}, {%r29356}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r28727, %r28730}, {%r1202, %r1199}, {%r29355}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28696, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28698, %r28696, %r28700;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28701, %r1237, %r28703, %r28698;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28705, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28707, %r28705, %r28709;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28710, %r1237, %r28712, %r28707;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28714, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28716, %r28714, %r28718;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28719, %r1237, %r28721, %r28716;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r28723, %r1240;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28725, %r28723, %r28727;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28728, %r1237, %r28730, %r28725;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28732, %r1240, %r28703;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28735, %r1237, %r28700, %r28732;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28739, %r1240, %r28712;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28742, %r1237, %r28709, %r28739;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28746, %r1240, %r28721;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28749, %r1237, %r28718, %r28746;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r28753, %r1240, %r28730;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28756, %r1237, %r28727, %r28753;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28801, %r28805}, {%r1270, %r1264, %r1267, %r1261}, {%r28735, %r28701}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28809, %r28813}, {%r1270, %r1264, %r1267, %r1261}, {%r28742, %r28710}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28817, %r28821}, {%r1270, %r1264, %r1267, %r1261}, {%r28749, %r28719}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r28825, %r28829}, {%r1270, %r1264, %r1267, %r1261}, {%r28756, %r28728}, {%r28638, %r28638};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28800, %r28801, %r28801, %r29354;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28860, %r28805, %r28805, %r28800;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28808, %r28809, %r28809, %r29353;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28859, %r28813, %r28813, %r28808;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28816, %r28817, %r28817, %r29352;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28858, %r28821, %r28821, %r28816;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28824, %r28825, %r28825, %r29351;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r28857, %r28829, %r28829, %r28824;
	// end inline asm
	add.s32 	%r28855, %r29349, 1;
	setp.ne.s32 	%p192, %r28855, 40;
	@%p192 bra 	LBB0_257;
// %bb.254:                             // %L176307
                                        //   in Loop: Header=BB0_16 Depth=1
	@%p26 bra 	LBB0_256;
// %bb.255:                             // %L176372
                                        //   in Loop: Header=BB0_16 Depth=1
	mul.lo.s32 	%r28832, %r28856, 193536;
	add.s32 	%r28833, %r83, %r28832;
	or.b32  	%r28834, %r28833, %r43;
	add.s32 	%r28835, %r28834, %r84;
	mul.wide.u32 	%rd1455, %r28835, 4;
	add.s64 	%rd1456, %rd4, %rd1455;
	st.global.u32 	[%rd1456], %r28860;
	add.s32 	%r28836, %r28834, %r85;
	mul.wide.u32 	%rd1457, %r28836, 4;
	add.s64 	%rd1458, %rd4, %rd1457;
	st.global.u32 	[%rd1458], %r28859;
	cvt.u64.u32 	%rd1459, %r84;
	cvt.u64.u32 	%rd1460, %r43;
	cvt.u64.u32 	%rd1461, %r83;
	cvt.u64.u32 	%rd1462, %r28832;
	add.s64 	%rd1463, %rd1462, %rd1461;
	add.s64 	%rd1464, %rd1463, %rd1460;
	add.s64 	%rd1465, %rd1464, %rd1459;
	shl.b64 	%rd1466, %rd1465, 2;
	add.s64 	%rd1467, %rd4, %rd1466;
	st.global.u32 	[%rd1467+387072], %r28858;
	cvt.u64.u32 	%rd1468, %r85;
	add.s64 	%rd1469, %rd1464, %rd1468;
	shl.b64 	%rd1470, %rd1469, 2;
	add.s64 	%rd1471, %rd4, %rd1470;
	st.global.u32 	[%rd1471+387072], %r28857;
	bra.uni 	LBB0_256;
LBB0_258:                               // %guard_pass63040
	@%p26 bra 	LBB0_260;
// %bb.259:                             // %L176980
	mul.lo.s32 	%r28838, %r28856, 193536;
	add.s32 	%r28839, %r83, %r28838;
	or.b32  	%r28840, %r28839, %r43;
	add.s32 	%r28841, %r28840, %r84;
	mul.wide.u32 	%rd1472, %r28841, 4;
	add.s64 	%rd1473, %rd4, %rd1472;
	st.global.u32 	[%rd1473], %r28860;
	add.s32 	%r28842, %r28840, %r85;
	mul.wide.u32 	%rd1474, %r28842, 4;
	add.s64 	%rd1475, %rd4, %rd1474;
	st.global.u32 	[%rd1475], %r28859;
	cvt.u64.u32 	%rd1476, %r84;
	cvt.u64.u32 	%rd1477, %r43;
	cvt.u64.u32 	%rd1478, %r83;
	cvt.u64.u32 	%rd1479, %r28838;
	add.s64 	%rd1480, %rd1479, %rd1478;
	add.s64 	%rd1481, %rd1480, %rd1477;
	add.s64 	%rd1482, %rd1481, %rd1476;
	shl.b64 	%rd1483, %rd1482, 2;
	add.s64 	%rd1484, %rd4, %rd1483;
	st.global.u32 	[%rd1484+387072], %r28858;
	cvt.u64.u32 	%rd1485, %r85;
	add.s64 	%rd1486, %rd1481, %rd1485;
	shl.b64 	%rd1487, %rd1486, 2;
	add.s64 	%rd1488, %rd4, %rd1487;
	st.global.u32 	[%rd1488+387072], %r28857;
LBB0_260:                               // %L177493
	mov.u32 	%r28843, 0;
	st.global.u32 	[%rd5], %r28843;
	ret;
LBB0_1:                                 // %L9
	ld.param.u64 	%rd37, [_Z15julia_frb_3402813CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_0];
	mov.u64 	%rd42, exception1;
	cvta.global.u64 	%rd43, %rd42;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd43;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 4
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd37;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 5
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}
