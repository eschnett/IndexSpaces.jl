// PTX CompilerJob of kernel #bb(CuDeviceVector{Int8x4, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Int32, 1}, CuDeviceVector{Int4x8, 1}) for sm_86, minthreads=1024, blocks_per_sm=1, always_inline=false

//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_86
.address_size 64

	// .globl	_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE // -- Begin function _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE
.func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
.noreturn
{
	trap;
}
.func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[8]
)
.noreturn
{
	trap;
}
.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception11323[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE
.visible .entry _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE(
	.param .align 8 .b8 _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_0[8],
	.param .align 8 .b8 _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_1[32],
	.param .align 8 .b8 _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_2[32],
	.param .align 8 .b8 _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_4[32]
)
.reqntid 1024, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<2996>;
	.reg .b64 	%rd<86>;

// %bb.0:                               // %conversion
	ld.param.u64 	%rd31, [_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_0];
	// begin inline asm
	mov.u32 %r64, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p1, %r64, 16511;
	@%p1 bra 	LBB0_2;
	bra.uni 	LBB0_1;
LBB0_2:                                 // %L11
	// begin inline asm
	mov.u32 %r65, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p2, %r65, 82047;
	@%p2 bra 	LBB0_4;
	bra.uni 	LBB0_3;
LBB0_4:                                 // %pass26
	ld.param.u64 	%rd1, [_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_1];
	ld.param.u64 	%rd2, [_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_2];
	ld.param.u64 	%rd3, [_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_3];
	ld.param.u64 	%rd4, [_Z13julia_bb_448013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_4];
	mov.u32 	%r323, %ctaid.x;
	shl.b32 	%r324, %r323, 7;
	mov.u32 	%r325, %tid.x;
	shr.u32 	%r326, %r325, 3;
	bfe.u32 	%r18, %r325, 3, 2;
	mov.u32 	%r327, %tid.y;
	shl.b32 	%r328, %r327, 2;
	and.b32  	%r329, %r328, 124;
	or.b32  	%r330, %r324, %r326;
	and.b32  	%r331, %r330, 3971;
	or.b32  	%r332, %r331, %r329;
	mul.wide.u32 	%rd36, %r332, 4;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.u32 	%r333, [%rd37];
	shl.b32 	%r334, %r323, 17;
	and.b32  	%r335, %r328, 112;
	shr.u32 	%r336, %r325, 2;
	and.b32  	%r337, %r336, 6;
	or.b32  	%r338, %r337, %r335;
	shl.b32 	%r339, %r338, 10;
	shl.b32 	%r340, %r325, 6;
	shl.b32 	%r341, %r325, 3;
	and.b32  	%r342, %r341, 48;
	shl.b32 	%r343, %r327, 8;
	and.b32  	%r344, %r343, 768;
	or.b32  	%r345, %r334, %r340;
	and.b32  	%r346, %r345, 4063296;
	or.b32  	%r347, %r346, %r342;
	or.b32  	%r348, %r347, %r339;
	or.b32  	%r349, %r348, %r344;
	cvt.u64.u32 	%rd38, %r349;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.v4.u32 	{%r67, %r68, %r83, %r84}, [%rd39];
	shl.b32 	%r350, %r323, 15;
	shl.b32 	%r351, %r338, 8;
	shl.b32 	%r352, %r325, 4;
	and.b32  	%r19, %r352, 16;
	shl.b32 	%r353, %r325, 1;
	and.b32  	%r354, %r353, 12;
	shl.b32 	%r355, %r327, 6;
	and.b32  	%r356, %r355, 192;
	or.b32  	%r357, %r350, %r352;
	and.b32  	%r358, %r357, 1015824;
	or.b32  	%r359, %r358, %r354;
	or.b32  	%r360, %r359, %r351;
	or.b32  	%r361, %r360, %r356;
	shl.b32 	%r362, %r361, 2;
	cvt.u64.u32 	%rd40, %r362;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.v4.u32 	{%r75, %r76, %r91, %r92}, [%rd41+8192];
	ld.global.v4.u32 	{%r99, %r100, %r115, %r116}, [%rd39+128];
	ld.global.v4.u32 	{%r107, %r108, %r123, %r124}, [%rd41+8320];
	ld.global.v4.u32 	{%r131, %r132, %r147, %r148}, [%rd41+1024];
	ld.global.v4.u32 	{%r139, %r140, %r155, %r156}, [%rd41+9216];
	ld.global.v4.u32 	{%r163, %r164, %r179, %r180}, [%rd41+1152];
	ld.global.v4.u32 	{%r171, %r172, %r187, %r188}, [%rd41+9344];
	mov.u32 	%r189, 21520;
	// begin inline asm
	prmt.b32 %r195, %r67, %r68, %r189;
	// end inline asm
	mov.u32 	%r193, 30258;
	// begin inline asm
	prmt.b32 %r196, %r67, %r68, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r203, %r75, %r76, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r204, %r75, %r76, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r211, %r83, %r84, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r212, %r83, %r84, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r219, %r91, %r92, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r220, %r91, %r92, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r227, %r99, %r100, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r228, %r99, %r100, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r235, %r107, %r108, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r236, %r107, %r108, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r243, %r115, %r116, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r244, %r115, %r116, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r251, %r123, %r124, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r252, %r123, %r124, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r259, %r131, %r132, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r260, %r131, %r132, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r267, %r139, %r140, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r268, %r139, %r140, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r275, %r147, %r148, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r276, %r147, %r148, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r283, %r155, %r156, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r284, %r155, %r156, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r291, %r163, %r164, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r292, %r163, %r164, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r299, %r171, %r172, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r300, %r171, %r172, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r307, %r179, %r180, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r308, %r179, %r180, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r315, %r187, %r188, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r316, %r187, %r188, %r193;
	// end inline asm
	mov.u32 	%r317, 25152;
	// begin inline asm
	prmt.b32 %r194, %r195, %r196, %r317;
	// end inline asm
	mov.u32 	%r321, 29521;
	// begin inline asm
	prmt.b32 %r198, %r195, %r196, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r202, %r203, %r204, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r206, %r203, %r204, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r210, %r211, %r212, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r214, %r211, %r212, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r218, %r219, %r220, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r222, %r219, %r220, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r226, %r227, %r228, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r230, %r227, %r228, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r234, %r235, %r236, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r238, %r235, %r236, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r242, %r243, %r244, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r246, %r243, %r244, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r250, %r251, %r252, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r254, %r251, %r252, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r258, %r259, %r260, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r262, %r259, %r260, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r266, %r267, %r268, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r270, %r267, %r268, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r274, %r275, %r276, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r278, %r275, %r276, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r282, %r283, %r284, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r286, %r283, %r284, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r290, %r291, %r292, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r294, %r291, %r292, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r298, %r299, %r300, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r302, %r299, %r300, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r306, %r307, %r308, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r310, %r307, %r308, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r314, %r315, %r316, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r318, %r315, %r316, %r321;
	// end inline asm
	and.b32  	%r20, %r325, 2;
	setp.eq.s32 	%p3, %r20, 0;
	selp.b32 	%r363, %r226, %r194, %p3;
	shfl.sync.bfly.b32	%r364, %r363, 2, 31, -1;
	selp.b32 	%r365, %r194, %r364, %p3;
	selp.b32 	%r366, %r364, %r226, %p3;
	selp.b32 	%r367, %r234, %r202, %p3;
	shfl.sync.bfly.b32	%r368, %r367, 2, 31, -1;
	selp.b32 	%r369, %r202, %r368, %p3;
	selp.b32 	%r370, %r368, %r234, %p3;
	selp.b32 	%r371, %r230, %r198, %p3;
	shfl.sync.bfly.b32	%r372, %r371, 2, 31, -1;
	selp.b32 	%r373, %r198, %r372, %p3;
	selp.b32 	%r374, %r372, %r230, %p3;
	selp.b32 	%r375, %r238, %r206, %p3;
	shfl.sync.bfly.b32	%r376, %r375, 2, 31, -1;
	selp.b32 	%r377, %r206, %r376, %p3;
	selp.b32 	%r378, %r376, %r238, %p3;
	selp.b32 	%r379, %r242, %r210, %p3;
	shfl.sync.bfly.b32	%r380, %r379, 2, 31, -1;
	selp.b32 	%r381, %r210, %r380, %p3;
	selp.b32 	%r382, %r380, %r242, %p3;
	selp.b32 	%r383, %r250, %r218, %p3;
	shfl.sync.bfly.b32	%r384, %r383, 2, 31, -1;
	selp.b32 	%r385, %r218, %r384, %p3;
	selp.b32 	%r386, %r384, %r250, %p3;
	selp.b32 	%r387, %r246, %r214, %p3;
	shfl.sync.bfly.b32	%r388, %r387, 2, 31, -1;
	selp.b32 	%r389, %r214, %r388, %p3;
	selp.b32 	%r390, %r388, %r246, %p3;
	selp.b32 	%r391, %r254, %r222, %p3;
	shfl.sync.bfly.b32	%r392, %r391, 2, 31, -1;
	selp.b32 	%r393, %r222, %r392, %p3;
	selp.b32 	%r394, %r392, %r254, %p3;
	selp.b32 	%r395, %r290, %r258, %p3;
	shfl.sync.bfly.b32	%r396, %r395, 2, 31, -1;
	selp.b32 	%r397, %r258, %r396, %p3;
	selp.b32 	%r398, %r396, %r290, %p3;
	selp.b32 	%r399, %r298, %r266, %p3;
	shfl.sync.bfly.b32	%r400, %r399, 2, 31, -1;
	selp.b32 	%r401, %r266, %r400, %p3;
	selp.b32 	%r402, %r400, %r298, %p3;
	selp.b32 	%r403, %r294, %r262, %p3;
	shfl.sync.bfly.b32	%r404, %r403, 2, 31, -1;
	selp.b32 	%r405, %r262, %r404, %p3;
	selp.b32 	%r406, %r404, %r294, %p3;
	selp.b32 	%r407, %r302, %r270, %p3;
	shfl.sync.bfly.b32	%r408, %r407, 2, 31, -1;
	selp.b32 	%r409, %r270, %r408, %p3;
	selp.b32 	%r410, %r408, %r302, %p3;
	selp.b32 	%r411, %r306, %r274, %p3;
	shfl.sync.bfly.b32	%r412, %r411, 2, 31, -1;
	selp.b32 	%r413, %r274, %r412, %p3;
	selp.b32 	%r414, %r412, %r306, %p3;
	selp.b32 	%r415, %r314, %r282, %p3;
	shfl.sync.bfly.b32	%r416, %r415, 2, 31, -1;
	selp.b32 	%r417, %r282, %r416, %p3;
	selp.b32 	%r418, %r416, %r314, %p3;
	selp.b32 	%r419, %r310, %r278, %p3;
	shfl.sync.bfly.b32	%r420, %r419, 2, 31, -1;
	selp.b32 	%r421, %r278, %r420, %p3;
	selp.b32 	%r422, %r420, %r310, %p3;
	selp.b32 	%r423, %r318, %r286, %p3;
	shfl.sync.bfly.b32	%r424, %r423, 2, 31, -1;
	selp.b32 	%r425, %r286, %r424, %p3;
	selp.b32 	%r426, %r424, %r318, %p3;
	and.b32  	%r21, %r325, 4;
	setp.eq.s32 	%p4, %r21, 0;
	selp.b32 	%r427, %r397, %r365, %p4;
	shfl.sync.bfly.b32	%r428, %r427, 4, 31, -1;
	selp.b32 	%r22, %r365, %r428, %p4;
	selp.b32 	%r23, %r428, %r397, %p4;
	selp.b32 	%r429, %r401, %r369, %p4;
	shfl.sync.bfly.b32	%r430, %r429, 4, 31, -1;
	selp.b32 	%r24, %r369, %r430, %p4;
	selp.b32 	%r25, %r430, %r401, %p4;
	selp.b32 	%r431, %r405, %r373, %p4;
	shfl.sync.bfly.b32	%r432, %r431, 4, 31, -1;
	selp.b32 	%r26, %r373, %r432, %p4;
	selp.b32 	%r27, %r432, %r405, %p4;
	selp.b32 	%r433, %r409, %r377, %p4;
	shfl.sync.bfly.b32	%r434, %r433, 4, 31, -1;
	selp.b32 	%r28, %r377, %r434, %p4;
	selp.b32 	%r29, %r434, %r409, %p4;
	selp.b32 	%r435, %r413, %r381, %p4;
	shfl.sync.bfly.b32	%r436, %r435, 4, 31, -1;
	selp.b32 	%r30, %r381, %r436, %p4;
	selp.b32 	%r31, %r436, %r413, %p4;
	selp.b32 	%r437, %r417, %r385, %p4;
	shfl.sync.bfly.b32	%r438, %r437, 4, 31, -1;
	selp.b32 	%r32, %r385, %r438, %p4;
	selp.b32 	%r33, %r438, %r417, %p4;
	selp.b32 	%r439, %r421, %r389, %p4;
	shfl.sync.bfly.b32	%r440, %r439, 4, 31, -1;
	selp.b32 	%r34, %r389, %r440, %p4;
	selp.b32 	%r35, %r440, %r421, %p4;
	selp.b32 	%r441, %r425, %r393, %p4;
	shfl.sync.bfly.b32	%r442, %r441, 4, 31, -1;
	selp.b32 	%r36, %r393, %r442, %p4;
	selp.b32 	%r37, %r442, %r425, %p4;
	selp.b32 	%r443, %r398, %r366, %p4;
	shfl.sync.bfly.b32	%r444, %r443, 4, 31, -1;
	selp.b32 	%r38, %r366, %r444, %p4;
	selp.b32 	%r39, %r444, %r398, %p4;
	selp.b32 	%r445, %r402, %r370, %p4;
	shfl.sync.bfly.b32	%r446, %r445, 4, 31, -1;
	selp.b32 	%r40, %r370, %r446, %p4;
	selp.b32 	%r41, %r446, %r402, %p4;
	selp.b32 	%r447, %r406, %r374, %p4;
	shfl.sync.bfly.b32	%r448, %r447, 4, 31, -1;
	selp.b32 	%r42, %r374, %r448, %p4;
	selp.b32 	%r43, %r448, %r406, %p4;
	selp.b32 	%r449, %r410, %r378, %p4;
	shfl.sync.bfly.b32	%r450, %r449, 4, 31, -1;
	selp.b32 	%r44, %r378, %r450, %p4;
	selp.b32 	%r45, %r450, %r410, %p4;
	selp.b32 	%r451, %r414, %r382, %p4;
	shfl.sync.bfly.b32	%r452, %r451, 4, 31, -1;
	selp.b32 	%r46, %r382, %r452, %p4;
	selp.b32 	%r47, %r452, %r414, %p4;
	selp.b32 	%r453, %r418, %r386, %p4;
	shfl.sync.bfly.b32	%r454, %r453, 4, 31, -1;
	selp.b32 	%r48, %r386, %r454, %p4;
	selp.b32 	%r49, %r454, %r418, %p4;
	selp.b32 	%r455, %r422, %r390, %p4;
	shfl.sync.bfly.b32	%r456, %r455, 4, 31, -1;
	selp.b32 	%r50, %r390, %r456, %p4;
	selp.b32 	%r51, %r456, %r422, %p4;
	selp.b32 	%r457, %r426, %r394, %p4;
	shfl.sync.bfly.b32	%r458, %r457, 4, 31, -1;
	selp.b32 	%r52, %r394, %r458, %p4;
	selp.b32 	%r53, %r458, %r426, %p4;
	shl.b32 	%r459, %r323, 11;
	and.b32  	%r54, %r459, 2048;
	shl.b32 	%r460, %r325, 2;
	and.b32  	%r461, %r460, 28;
	shl.b32 	%r462, %r327, 5;
	and.b32  	%r55, %r327, 28;
	shl.b32 	%r463, %r323, 6;
	and.b32  	%r56, %r463, 1920;
	and.b32  	%r464, %r462, 96;
	or.b32  	%r57, %r461, %r464;
	and.b32  	%r465, %r341, 24;
	bfe.u32 	%r466, %r325, 2, 3;
	or.b32  	%r467, %r465, %r464;
	shl.b32 	%r468, %r327, 12;
	and.b32  	%r469, %r468, 12288;
	or.b32  	%r470, %r335, %r466;
	or.b32  	%r471, %r470, %r469;
	or.b32  	%r472, %r469, %r335;
	or.b32  	%r473, %r472, %r466;
	or.b32  	%r474, %r18, %r329;
	and.b32  	%r475, %r325, 7;
	add.s32 	%r476, %r333, -1;
	mov.u32 	%r477, 1;
	shl.b32 	%r478, %r477, %r476;
	setp.gt.u32 	%p5, %r476, 31;
	selp.b32 	%r58, 0, %r478, %p5;
	min.u32 	%r59, %r333, 31;
	and.b32  	%r60, %r325, 1;
	shl.b32 	%r479, %r325, 5;
	and.b32  	%r480, %r479, 64;
	and.b32  	%r481, %r341, 32;
	or.b32  	%r61, %r480, %r481;
	shl.b32 	%r482, %r323, 13;
	shl.b32 	%r62, %r474, 18;
	and.b32  	%r63, %r482, 253952;
	or.b32  	%r483, %r466, 8;
	or.b32  	%r484, %r466, 16;
	or.b32  	%r485, %r466, 24;
	mul.lo.s32 	%r486, %r466, 129;
	add.s32 	%r487, %r467, %r486;
	mul.wide.u32 	%rd42, %r487, 4;
	mov.u64 	%rd43, shmem;
	add.s64 	%rd5, %rd43, %rd42;
	cvt.u64.u32 	%rd44, %r486;
	cvt.u64.u32 	%rd45, %r467;
	add.s64 	%rd46, %rd45, %rd44;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd6, %rd43, %rd47;
	shl.b32 	%r488, %r325, 8;
	and.b32  	%r489, %r488, 768;
	or.b32  	%r490, %r471, %r489;
	mul.wide.u32 	%rd48, %r490, 4;
	add.s64 	%rd49, %rd43, 16512;
	add.s64 	%rd7, %rd49, %rd48;
	or.b32  	%r491, %r489, %r473;
	mul.wide.u32 	%rd50, %r491, 4;
	add.s64 	%rd9, %rd49, %rd50;
	or.b32  	%r492, %r491, 128;
	mul.wide.u32 	%rd51, %r492, 4;
	add.s64 	%rd10, %rd49, %rd51;
	mul.lo.s32 	%r493, %r483, 129;
	add.s32 	%r494, %r467, %r493;
	mul.wide.u32 	%rd52, %r494, 4;
	add.s64 	%rd11, %rd43, %rd52;
	cvt.u64.u32 	%rd53, %r493;
	add.s64 	%rd54, %rd45, %rd53;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd12, %rd43, %rd55;
	or.b32  	%r495, %r491, 1024;
	mul.wide.u32 	%rd56, %r495, 4;
	add.s64 	%rd13, %rd49, %rd56;
	or.b32  	%r496, %r491, 1152;
	mul.wide.u32 	%rd57, %r496, 4;
	add.s64 	%rd14, %rd49, %rd57;
	mul.lo.s32 	%r497, %r484, 129;
	add.s32 	%r498, %r467, %r497;
	mul.wide.u32 	%rd58, %r498, 4;
	add.s64 	%rd15, %rd43, %rd58;
	cvt.u64.u32 	%rd59, %r497;
	add.s64 	%rd60, %rd45, %rd59;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd16, %rd43, %rd61;
	or.b32  	%r499, %r491, 2048;
	mul.wide.u32 	%rd62, %r499, 4;
	add.s64 	%rd17, %rd49, %rd62;
	or.b32  	%r500, %r491, 2176;
	mul.wide.u32 	%rd63, %r500, 4;
	add.s64 	%rd18, %rd49, %rd63;
	mul.lo.s32 	%r501, %r485, 129;
	add.s32 	%r502, %r467, %r501;
	mul.wide.u32 	%rd64, %r502, 4;
	add.s64 	%rd19, %rd43, %rd64;
	cvt.u64.u32 	%rd65, %r501;
	add.s64 	%rd66, %rd45, %rd65;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd20, %rd43, %rd67;
	or.b32  	%r503, %r491, 3072;
	mul.wide.u32 	%rd68, %r503, 4;
	add.s64 	%rd21, %rd49, %rd68;
	or.b32  	%r504, %r491, 3200;
	mul.wide.u32 	%rd69, %r504, 4;
	add.s64 	%rd22, %rd49, %rd69;
	shl.b32 	%r505, %r475, 7;
	or.b32  	%r506, %r505, %r474;
	mul.wide.u32 	%rd70, %r506, 4;
	add.s64 	%rd24, %rd49, %rd70;
	or.b32  	%r507, %r506, 1024;
	mul.wide.u32 	%rd71, %r507, 4;
	add.s64 	%rd26, %rd49, %rd71;
	or.b32  	%r508, %r506, 2048;
	mul.wide.u32 	%rd72, %r508, 4;
	add.s64 	%rd28, %rd49, %rd72;
	or.b32  	%r509, %r506, 3072;
	mul.wide.u32 	%rd73, %r509, 4;
	add.s64 	%rd30, %rd49, %rd73;
	mov.u32 	%r322, 0;
	mov.u32 	%r2988, %r322;
LBB0_5:                                 // %L2186
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_6 Depth 2
	mov.u32 	%r2989, %r322;
	mov.u32 	%r2990, %r322;
	mov.u32 	%r2991, %r322;
	mov.u32 	%r2992, %r322;
	mov.u32 	%r2993, %r322;
	mov.u32 	%r2994, %r322;
	mov.u32 	%r2995, %r322;
LBB0_6:                                 // %L2189
                                        //   Parent Loop BB0_5 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r2625, %r2988, %r2989;
	or.b32  	%r2626, %r2625, %r18;
	or.b32  	%r2627, %r2626, %r55;
	shl.b32 	%r2628, %r2627, 12;
	or.b32  	%r2629, %r2628, %r54;
	or.b32  	%r2630, %r2629, %r56;
	or.b32  	%r2631, %r2630, %r57;
	mul.wide.s32 	%rd74, %r2631, 4;
	add.s64 	%rd75, %rd2, %rd74;
	ld.global.v4.u32 	{%r2632, %r2633, %r2634, %r2635}, [%rd75];
	and.b32  	%r2636, %r2627, 31;
	mul.lo.s32 	%r2637, %r2636, 129;
	add.s32 	%r2638, %r57, %r2637;
	mul.wide.u32 	%rd76, %r2638, 4;
	add.s64 	%rd78, %rd43, %rd76;
	st.shared.u32 	[%rd78], %r2632;
	cvt.u64.u32 	%rd79, %r2637;
	cvt.u64.u32 	%rd80, %r57;
	add.s64 	%rd81, %rd80, %rd79;
	shl.b64 	%rd82, %rd81, 2;
	add.s64 	%rd83, %rd43, %rd82;
	st.shared.u32 	[%rd83+4], %r2633;
	st.shared.u32 	[%rd83+8], %r2634;
	st.shared.u32 	[%rd83+12], %r2635;
	bar.sync 	0;
	ld.shared.u32 	%r512, [%rd5];
	mov.u32 	%r513, 134744072;
	mov.u32 	%r514, 252645135;
	// begin inline asm
	lop3.b32 %r511, %r512, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2639, %r511, 2021161080;
	xor.b32  	%r528, %r2639, -2139062144;
	shr.u32 	%r516, %r512, 4;
	// begin inline asm
	lop3.b32 %r515, %r516, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2640, %r515, 2021161080;
	xor.b32  	%r522, %r2640, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r519, %r520}, {%r26}, {%r522}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r525, %r526}, {%r22}, {%r528}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r531, %r532}, {%r26}, {%r528}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r537, %r538}, {%r22}, {%r522}, {%r531, %r532};
	// end inline asm
	ld.shared.u32 	%r544, [%rd6+4];
	// begin inline asm
	lop3.b32 %r543, %r544, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2641, %r543, 2021161080;
	xor.b32  	%r560, %r2641, -2139062144;
	shr.u32 	%r548, %r544, 4;
	// begin inline asm
	lop3.b32 %r547, %r548, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2642, %r547, 2021161080;
	xor.b32  	%r554, %r2642, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r551, %r552}, {%r34}, {%r554}, {%r519, %r520};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r557, %r558}, {%r30}, {%r560}, {%r525, %r526};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r563, %r564}, {%r34}, {%r560}, {%r537, %r538};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r569, %r570}, {%r30}, {%r554}, {%r563, %r564};
	// end inline asm
	ld.shared.u32 	%r576, [%rd6+8];
	// begin inline asm
	lop3.b32 %r575, %r576, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2643, %r575, 2021161080;
	xor.b32  	%r592, %r2643, -2139062144;
	shr.u32 	%r580, %r576, 4;
	// begin inline asm
	lop3.b32 %r579, %r580, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2644, %r579, 2021161080;
	xor.b32  	%r586, %r2644, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r583, %r584}, {%r42}, {%r586}, {%r551, %r552};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r589, %r590}, {%r38}, {%r592}, {%r557, %r558};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r595, %r596}, {%r42}, {%r592}, {%r569, %r570};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r601, %r602}, {%r38}, {%r586}, {%r595, %r596};
	// end inline asm
	ld.shared.u32 	%r608, [%rd6+12];
	// begin inline asm
	lop3.b32 %r607, %r608, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2645, %r607, 2021161080;
	xor.b32  	%r624, %r2645, -2139062144;
	shr.u32 	%r612, %r608, 4;
	// begin inline asm
	lop3.b32 %r611, %r612, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2646, %r611, 2021161080;
	xor.b32  	%r618, %r2646, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r615, %r616}, {%r50}, {%r618}, {%r583, %r584};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r621, %r622}, {%r46}, {%r624}, {%r589, %r590};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r627, %r628}, {%r50}, {%r624}, {%r601, %r602};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r633, %r634}, {%r46}, {%r618}, {%r627, %r628};
	// end inline asm
	ld.shared.u32 	%r640, [%rd6+16];
	// begin inline asm
	lop3.b32 %r639, %r640, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2647, %r639, 2021161080;
	xor.b32  	%r656, %r2647, -2139062144;
	shr.u32 	%r644, %r640, 4;
	// begin inline asm
	lop3.b32 %r643, %r644, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2648, %r643, 2021161080;
	xor.b32  	%r650, %r2648, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r647, %r648}, {%r27}, {%r650}, {%r615, %r616};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r653, %r654}, {%r23}, {%r656}, {%r621, %r622};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r659, %r660}, {%r27}, {%r656}, {%r633, %r634};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r665, %r666}, {%r23}, {%r650}, {%r659, %r660};
	// end inline asm
	ld.shared.u32 	%r672, [%rd6+20];
	// begin inline asm
	lop3.b32 %r671, %r672, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2649, %r671, 2021161080;
	xor.b32  	%r688, %r2649, -2139062144;
	shr.u32 	%r676, %r672, 4;
	// begin inline asm
	lop3.b32 %r675, %r676, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2650, %r675, 2021161080;
	xor.b32  	%r682, %r2650, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r679, %r680}, {%r35}, {%r682}, {%r647, %r648};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r685, %r686}, {%r31}, {%r688}, {%r653, %r654};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r691, %r692}, {%r35}, {%r688}, {%r665, %r666};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r697, %r698}, {%r31}, {%r682}, {%r691, %r692};
	// end inline asm
	ld.shared.u32 	%r704, [%rd6+24];
	// begin inline asm
	lop3.b32 %r703, %r704, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2651, %r703, 2021161080;
	xor.b32  	%r720, %r2651, -2139062144;
	shr.u32 	%r708, %r704, 4;
	// begin inline asm
	lop3.b32 %r707, %r708, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2652, %r707, 2021161080;
	xor.b32  	%r714, %r2652, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r711, %r712}, {%r43}, {%r714}, {%r679, %r680};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r717, %r718}, {%r39}, {%r720}, {%r685, %r686};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r723, %r724}, {%r43}, {%r720}, {%r697, %r698};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r729, %r730}, {%r39}, {%r714}, {%r723, %r724};
	// end inline asm
	ld.shared.u32 	%r736, [%rd6+28];
	// begin inline asm
	lop3.b32 %r735, %r736, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2653, %r735, 2021161080;
	xor.b32  	%r752, %r2653, -2139062144;
	shr.u32 	%r740, %r736, 4;
	// begin inline asm
	lop3.b32 %r739, %r740, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2654, %r739, 2021161080;
	xor.b32  	%r746, %r2654, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r743, %r744}, {%r51}, {%r746}, {%r711, %r712};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r749, %r750}, {%r47}, {%r752}, {%r717, %r718};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r755, %r756}, {%r51}, {%r752}, {%r729, %r730};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r761, %r762}, {%r47}, {%r746}, {%r755, %r756};
	// end inline asm
	add.s32 	%r2655, %r761, 4;
	shr.s32 	%r769, %r2655, 3;
	sub.s32 	%r2656, %r743, %r749;
	add.s32 	%r2657, %r2656, 4;
	shr.s32 	%r768, %r2657, 3;
	add.s32 	%r2658, %r762, 4;
	shr.s32 	%r772, %r2658, 3;
	sub.s32 	%r2659, %r744, %r750;
	add.s32 	%r2660, %r2659, 4;
	shr.s32 	%r771, %r2660, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r767, %r768, %r769;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r770, %r771, %r772;
	// end inline asm
	st.shared.u32 	[%rd7], %r767;
	st.shared.u32 	[%rd7+512], %r770;
	ld.shared.u32 	%r774, [%rd5];
	// begin inline asm
	lop3.b32 %r773, %r774, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2661, %r773, 2021161080;
	xor.b32  	%r790, %r2661, -2139062144;
	shr.u32 	%r778, %r774, 4;
	// begin inline asm
	lop3.b32 %r777, %r778, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2662, %r777, 2021161080;
	xor.b32  	%r784, %r2662, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r781, %r782}, {%r28}, {%r784}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r787, %r788}, {%r24}, {%r790}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r793, %r794}, {%r28}, {%r790}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r799, %r800}, {%r24}, {%r784}, {%r793, %r794};
	// end inline asm
	ld.shared.u32 	%r806, [%rd6+4];
	// begin inline asm
	lop3.b32 %r805, %r806, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2663, %r805, 2021161080;
	xor.b32  	%r822, %r2663, -2139062144;
	shr.u32 	%r810, %r806, 4;
	// begin inline asm
	lop3.b32 %r809, %r810, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2664, %r809, 2021161080;
	xor.b32  	%r816, %r2664, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r813, %r814}, {%r36}, {%r816}, {%r781, %r782};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r819, %r820}, {%r32}, {%r822}, {%r787, %r788};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r825, %r826}, {%r36}, {%r822}, {%r799, %r800};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r831, %r832}, {%r32}, {%r816}, {%r825, %r826};
	// end inline asm
	ld.shared.u32 	%r838, [%rd6+8];
	// begin inline asm
	lop3.b32 %r837, %r838, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2665, %r837, 2021161080;
	xor.b32  	%r854, %r2665, -2139062144;
	shr.u32 	%r842, %r838, 4;
	// begin inline asm
	lop3.b32 %r841, %r842, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2666, %r841, 2021161080;
	xor.b32  	%r848, %r2666, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r845, %r846}, {%r44}, {%r848}, {%r813, %r814};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r851, %r852}, {%r40}, {%r854}, {%r819, %r820};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r857, %r858}, {%r44}, {%r854}, {%r831, %r832};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r863, %r864}, {%r40}, {%r848}, {%r857, %r858};
	// end inline asm
	ld.shared.u32 	%r870, [%rd6+12];
	// begin inline asm
	lop3.b32 %r869, %r870, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2667, %r869, 2021161080;
	xor.b32  	%r886, %r2667, -2139062144;
	shr.u32 	%r874, %r870, 4;
	// begin inline asm
	lop3.b32 %r873, %r874, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2668, %r873, 2021161080;
	xor.b32  	%r880, %r2668, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r877, %r878}, {%r52}, {%r880}, {%r845, %r846};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r883, %r884}, {%r48}, {%r886}, {%r851, %r852};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r889, %r890}, {%r52}, {%r886}, {%r863, %r864};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r895, %r896}, {%r48}, {%r880}, {%r889, %r890};
	// end inline asm
	ld.shared.u32 	%r902, [%rd6+16];
	// begin inline asm
	lop3.b32 %r901, %r902, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2669, %r901, 2021161080;
	xor.b32  	%r918, %r2669, -2139062144;
	shr.u32 	%r906, %r902, 4;
	// begin inline asm
	lop3.b32 %r905, %r906, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2670, %r905, 2021161080;
	xor.b32  	%r912, %r2670, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r909, %r910}, {%r29}, {%r912}, {%r877, %r878};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r915, %r916}, {%r25}, {%r918}, {%r883, %r884};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r921, %r922}, {%r29}, {%r918}, {%r895, %r896};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r927, %r928}, {%r25}, {%r912}, {%r921, %r922};
	// end inline asm
	ld.shared.u32 	%r934, [%rd6+20];
	// begin inline asm
	lop3.b32 %r933, %r934, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2671, %r933, 2021161080;
	xor.b32  	%r950, %r2671, -2139062144;
	shr.u32 	%r938, %r934, 4;
	// begin inline asm
	lop3.b32 %r937, %r938, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2672, %r937, 2021161080;
	xor.b32  	%r944, %r2672, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r941, %r942}, {%r37}, {%r944}, {%r909, %r910};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r947, %r948}, {%r33}, {%r950}, {%r915, %r916};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r953, %r954}, {%r37}, {%r950}, {%r927, %r928};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r959, %r960}, {%r33}, {%r944}, {%r953, %r954};
	// end inline asm
	ld.shared.u32 	%r966, [%rd6+24];
	// begin inline asm
	lop3.b32 %r965, %r966, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2673, %r965, 2021161080;
	xor.b32  	%r982, %r2673, -2139062144;
	shr.u32 	%r970, %r966, 4;
	// begin inline asm
	lop3.b32 %r969, %r970, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2674, %r969, 2021161080;
	xor.b32  	%r976, %r2674, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r973, %r974}, {%r45}, {%r976}, {%r941, %r942};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r979, %r980}, {%r41}, {%r982}, {%r947, %r948};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r985, %r986}, {%r45}, {%r982}, {%r959, %r960};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r991, %r992}, {%r41}, {%r976}, {%r985, %r986};
	// end inline asm
	ld.shared.u32 	%r998, [%rd6+28];
	// begin inline asm
	lop3.b32 %r997, %r998, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2675, %r997, 2021161080;
	xor.b32  	%r1014, %r2675, -2139062144;
	shr.u32 	%r1002, %r998, 4;
	// begin inline asm
	lop3.b32 %r1001, %r1002, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2676, %r1001, 2021161080;
	xor.b32  	%r1008, %r2676, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1005, %r1006}, {%r53}, {%r1008}, {%r973, %r974};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1011, %r1012}, {%r49}, {%r1014}, {%r979, %r980};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1017, %r1018}, {%r53}, {%r1014}, {%r991, %r992};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1023, %r1024}, {%r49}, {%r1008}, {%r1017, %r1018};
	// end inline asm
	add.s32 	%r2677, %r1023, 4;
	shr.s32 	%r1031, %r2677, 3;
	sub.s32 	%r2678, %r1005, %r1011;
	add.s32 	%r2679, %r2678, 4;
	shr.s32 	%r1030, %r2679, 3;
	add.s32 	%r2680, %r1024, 4;
	shr.s32 	%r1034, %r2680, 3;
	sub.s32 	%r2681, %r1006, %r1012;
	add.s32 	%r2682, %r2681, 4;
	shr.s32 	%r1033, %r2682, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1029, %r1030, %r1031;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1032, %r1033, %r1034;
	// end inline asm
	st.shared.u32 	[%rd9+32], %r1029;
	st.shared.u32 	[%rd10+32], %r1032;
	ld.shared.u32 	%r1036, [%rd11];
	// begin inline asm
	lop3.b32 %r1035, %r1036, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2683, %r1035, 2021161080;
	xor.b32  	%r1052, %r2683, -2139062144;
	shr.u32 	%r1040, %r1036, 4;
	// begin inline asm
	lop3.b32 %r1039, %r1040, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2684, %r1039, 2021161080;
	xor.b32  	%r1046, %r2684, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1043, %r1044}, {%r26}, {%r1046}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1049, %r1050}, {%r22}, {%r1052}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1055, %r1056}, {%r26}, {%r1052}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1061, %r1062}, {%r22}, {%r1046}, {%r1055, %r1056};
	// end inline asm
	ld.shared.u32 	%r1068, [%rd12+4];
	// begin inline asm
	lop3.b32 %r1067, %r1068, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2685, %r1067, 2021161080;
	xor.b32  	%r1084, %r2685, -2139062144;
	shr.u32 	%r1072, %r1068, 4;
	// begin inline asm
	lop3.b32 %r1071, %r1072, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2686, %r1071, 2021161080;
	xor.b32  	%r1078, %r2686, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1075, %r1076}, {%r34}, {%r1078}, {%r1043, %r1044};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1081, %r1082}, {%r30}, {%r1084}, {%r1049, %r1050};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1087, %r1088}, {%r34}, {%r1084}, {%r1061, %r1062};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1093, %r1094}, {%r30}, {%r1078}, {%r1087, %r1088};
	// end inline asm
	ld.shared.u32 	%r1100, [%rd12+8];
	// begin inline asm
	lop3.b32 %r1099, %r1100, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2687, %r1099, 2021161080;
	xor.b32  	%r1116, %r2687, -2139062144;
	shr.u32 	%r1104, %r1100, 4;
	// begin inline asm
	lop3.b32 %r1103, %r1104, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2688, %r1103, 2021161080;
	xor.b32  	%r1110, %r2688, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1107, %r1108}, {%r42}, {%r1110}, {%r1075, %r1076};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1113, %r1114}, {%r38}, {%r1116}, {%r1081, %r1082};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1119, %r1120}, {%r42}, {%r1116}, {%r1093, %r1094};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1125, %r1126}, {%r38}, {%r1110}, {%r1119, %r1120};
	// end inline asm
	ld.shared.u32 	%r1132, [%rd12+12];
	// begin inline asm
	lop3.b32 %r1131, %r1132, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2689, %r1131, 2021161080;
	xor.b32  	%r1148, %r2689, -2139062144;
	shr.u32 	%r1136, %r1132, 4;
	// begin inline asm
	lop3.b32 %r1135, %r1136, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2690, %r1135, 2021161080;
	xor.b32  	%r1142, %r2690, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1139, %r1140}, {%r50}, {%r1142}, {%r1107, %r1108};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1145, %r1146}, {%r46}, {%r1148}, {%r1113, %r1114};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1151, %r1152}, {%r50}, {%r1148}, {%r1125, %r1126};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1157, %r1158}, {%r46}, {%r1142}, {%r1151, %r1152};
	// end inline asm
	ld.shared.u32 	%r1164, [%rd12+16];
	// begin inline asm
	lop3.b32 %r1163, %r1164, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2691, %r1163, 2021161080;
	xor.b32  	%r1180, %r2691, -2139062144;
	shr.u32 	%r1168, %r1164, 4;
	// begin inline asm
	lop3.b32 %r1167, %r1168, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2692, %r1167, 2021161080;
	xor.b32  	%r1174, %r2692, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1171, %r1172}, {%r27}, {%r1174}, {%r1139, %r1140};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1177, %r1178}, {%r23}, {%r1180}, {%r1145, %r1146};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1183, %r1184}, {%r27}, {%r1180}, {%r1157, %r1158};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1189, %r1190}, {%r23}, {%r1174}, {%r1183, %r1184};
	// end inline asm
	ld.shared.u32 	%r1196, [%rd12+20];
	// begin inline asm
	lop3.b32 %r1195, %r1196, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2693, %r1195, 2021161080;
	xor.b32  	%r1212, %r2693, -2139062144;
	shr.u32 	%r1200, %r1196, 4;
	// begin inline asm
	lop3.b32 %r1199, %r1200, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2694, %r1199, 2021161080;
	xor.b32  	%r1206, %r2694, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1203, %r1204}, {%r35}, {%r1206}, {%r1171, %r1172};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1209, %r1210}, {%r31}, {%r1212}, {%r1177, %r1178};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1215, %r1216}, {%r35}, {%r1212}, {%r1189, %r1190};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1221, %r1222}, {%r31}, {%r1206}, {%r1215, %r1216};
	// end inline asm
	ld.shared.u32 	%r1228, [%rd12+24];
	// begin inline asm
	lop3.b32 %r1227, %r1228, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2695, %r1227, 2021161080;
	xor.b32  	%r1244, %r2695, -2139062144;
	shr.u32 	%r1232, %r1228, 4;
	// begin inline asm
	lop3.b32 %r1231, %r1232, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2696, %r1231, 2021161080;
	xor.b32  	%r1238, %r2696, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1235, %r1236}, {%r43}, {%r1238}, {%r1203, %r1204};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1241, %r1242}, {%r39}, {%r1244}, {%r1209, %r1210};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1247, %r1248}, {%r43}, {%r1244}, {%r1221, %r1222};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1253, %r1254}, {%r39}, {%r1238}, {%r1247, %r1248};
	// end inline asm
	ld.shared.u32 	%r1260, [%rd12+28];
	// begin inline asm
	lop3.b32 %r1259, %r1260, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2697, %r1259, 2021161080;
	xor.b32  	%r1276, %r2697, -2139062144;
	shr.u32 	%r1264, %r1260, 4;
	// begin inline asm
	lop3.b32 %r1263, %r1264, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2698, %r1263, 2021161080;
	xor.b32  	%r1270, %r2698, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1267, %r1268}, {%r51}, {%r1270}, {%r1235, %r1236};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1273, %r1274}, {%r47}, {%r1276}, {%r1241, %r1242};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1279, %r1280}, {%r51}, {%r1276}, {%r1253, %r1254};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1285, %r1286}, {%r47}, {%r1270}, {%r1279, %r1280};
	// end inline asm
	add.s32 	%r2699, %r1285, 4;
	shr.s32 	%r1293, %r2699, 3;
	sub.s32 	%r2700, %r1267, %r1273;
	add.s32 	%r2701, %r2700, 4;
	shr.s32 	%r1292, %r2701, 3;
	add.s32 	%r2702, %r1286, 4;
	shr.s32 	%r1296, %r2702, 3;
	sub.s32 	%r2703, %r1268, %r1274;
	add.s32 	%r2704, %r2703, 4;
	shr.s32 	%r1295, %r2704, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1291, %r1292, %r1293;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1294, %r1295, %r1296;
	// end inline asm
	st.shared.u32 	[%rd7+4096], %r1291;
	st.shared.u32 	[%rd7+4608], %r1294;
	ld.shared.u32 	%r1298, [%rd11];
	// begin inline asm
	lop3.b32 %r1297, %r1298, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2705, %r1297, 2021161080;
	xor.b32  	%r1314, %r2705, -2139062144;
	shr.u32 	%r1302, %r1298, 4;
	// begin inline asm
	lop3.b32 %r1301, %r1302, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2706, %r1301, 2021161080;
	xor.b32  	%r1308, %r2706, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1305, %r1306}, {%r28}, {%r1308}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1311, %r1312}, {%r24}, {%r1314}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1317, %r1318}, {%r28}, {%r1314}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1323, %r1324}, {%r24}, {%r1308}, {%r1317, %r1318};
	// end inline asm
	ld.shared.u32 	%r1330, [%rd12+4];
	// begin inline asm
	lop3.b32 %r1329, %r1330, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2707, %r1329, 2021161080;
	xor.b32  	%r1346, %r2707, -2139062144;
	shr.u32 	%r1334, %r1330, 4;
	// begin inline asm
	lop3.b32 %r1333, %r1334, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2708, %r1333, 2021161080;
	xor.b32  	%r1340, %r2708, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1337, %r1338}, {%r36}, {%r1340}, {%r1305, %r1306};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1343, %r1344}, {%r32}, {%r1346}, {%r1311, %r1312};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1349, %r1350}, {%r36}, {%r1346}, {%r1323, %r1324};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1355, %r1356}, {%r32}, {%r1340}, {%r1349, %r1350};
	// end inline asm
	ld.shared.u32 	%r1362, [%rd12+8];
	// begin inline asm
	lop3.b32 %r1361, %r1362, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2709, %r1361, 2021161080;
	xor.b32  	%r1378, %r2709, -2139062144;
	shr.u32 	%r1366, %r1362, 4;
	// begin inline asm
	lop3.b32 %r1365, %r1366, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2710, %r1365, 2021161080;
	xor.b32  	%r1372, %r2710, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1369, %r1370}, {%r44}, {%r1372}, {%r1337, %r1338};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1375, %r1376}, {%r40}, {%r1378}, {%r1343, %r1344};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1381, %r1382}, {%r44}, {%r1378}, {%r1355, %r1356};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1387, %r1388}, {%r40}, {%r1372}, {%r1381, %r1382};
	// end inline asm
	ld.shared.u32 	%r1394, [%rd12+12];
	// begin inline asm
	lop3.b32 %r1393, %r1394, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2711, %r1393, 2021161080;
	xor.b32  	%r1410, %r2711, -2139062144;
	shr.u32 	%r1398, %r1394, 4;
	// begin inline asm
	lop3.b32 %r1397, %r1398, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2712, %r1397, 2021161080;
	xor.b32  	%r1404, %r2712, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1401, %r1402}, {%r52}, {%r1404}, {%r1369, %r1370};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1407, %r1408}, {%r48}, {%r1410}, {%r1375, %r1376};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1413, %r1414}, {%r52}, {%r1410}, {%r1387, %r1388};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1419, %r1420}, {%r48}, {%r1404}, {%r1413, %r1414};
	// end inline asm
	ld.shared.u32 	%r1426, [%rd12+16];
	// begin inline asm
	lop3.b32 %r1425, %r1426, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2713, %r1425, 2021161080;
	xor.b32  	%r1442, %r2713, -2139062144;
	shr.u32 	%r1430, %r1426, 4;
	// begin inline asm
	lop3.b32 %r1429, %r1430, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2714, %r1429, 2021161080;
	xor.b32  	%r1436, %r2714, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1433, %r1434}, {%r29}, {%r1436}, {%r1401, %r1402};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1439, %r1440}, {%r25}, {%r1442}, {%r1407, %r1408};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1445, %r1446}, {%r29}, {%r1442}, {%r1419, %r1420};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1451, %r1452}, {%r25}, {%r1436}, {%r1445, %r1446};
	// end inline asm
	ld.shared.u32 	%r1458, [%rd12+20];
	// begin inline asm
	lop3.b32 %r1457, %r1458, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2715, %r1457, 2021161080;
	xor.b32  	%r1474, %r2715, -2139062144;
	shr.u32 	%r1462, %r1458, 4;
	// begin inline asm
	lop3.b32 %r1461, %r1462, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2716, %r1461, 2021161080;
	xor.b32  	%r1468, %r2716, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1465, %r1466}, {%r37}, {%r1468}, {%r1433, %r1434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1471, %r1472}, {%r33}, {%r1474}, {%r1439, %r1440};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1477, %r1478}, {%r37}, {%r1474}, {%r1451, %r1452};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1483, %r1484}, {%r33}, {%r1468}, {%r1477, %r1478};
	// end inline asm
	ld.shared.u32 	%r1490, [%rd12+24];
	// begin inline asm
	lop3.b32 %r1489, %r1490, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2717, %r1489, 2021161080;
	xor.b32  	%r1506, %r2717, -2139062144;
	shr.u32 	%r1494, %r1490, 4;
	// begin inline asm
	lop3.b32 %r1493, %r1494, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2718, %r1493, 2021161080;
	xor.b32  	%r1500, %r2718, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1497, %r1498}, {%r45}, {%r1500}, {%r1465, %r1466};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1503, %r1504}, {%r41}, {%r1506}, {%r1471, %r1472};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1509, %r1510}, {%r45}, {%r1506}, {%r1483, %r1484};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1515, %r1516}, {%r41}, {%r1500}, {%r1509, %r1510};
	// end inline asm
	ld.shared.u32 	%r1522, [%rd12+28];
	// begin inline asm
	lop3.b32 %r1521, %r1522, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2719, %r1521, 2021161080;
	xor.b32  	%r1538, %r2719, -2139062144;
	shr.u32 	%r1526, %r1522, 4;
	// begin inline asm
	lop3.b32 %r1525, %r1526, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2720, %r1525, 2021161080;
	xor.b32  	%r1532, %r2720, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1529, %r1530}, {%r53}, {%r1532}, {%r1497, %r1498};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1535, %r1536}, {%r49}, {%r1538}, {%r1503, %r1504};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1541, %r1542}, {%r53}, {%r1538}, {%r1515, %r1516};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1547, %r1548}, {%r49}, {%r1532}, {%r1541, %r1542};
	// end inline asm
	add.s32 	%r2721, %r1547, 4;
	shr.s32 	%r1555, %r2721, 3;
	sub.s32 	%r2722, %r1529, %r1535;
	add.s32 	%r2723, %r2722, 4;
	shr.s32 	%r1554, %r2723, 3;
	add.s32 	%r2724, %r1548, 4;
	shr.s32 	%r1558, %r2724, 3;
	sub.s32 	%r2725, %r1530, %r1536;
	add.s32 	%r2726, %r2725, 4;
	shr.s32 	%r1557, %r2726, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1553, %r1554, %r1555;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1556, %r1557, %r1558;
	// end inline asm
	st.shared.u32 	[%rd13+32], %r1553;
	st.shared.u32 	[%rd14+32], %r1556;
	ld.shared.u32 	%r1560, [%rd15];
	// begin inline asm
	lop3.b32 %r1559, %r1560, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2727, %r1559, 2021161080;
	xor.b32  	%r1576, %r2727, -2139062144;
	shr.u32 	%r1564, %r1560, 4;
	// begin inline asm
	lop3.b32 %r1563, %r1564, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2728, %r1563, 2021161080;
	xor.b32  	%r1570, %r2728, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1567, %r1568}, {%r26}, {%r1570}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1573, %r1574}, {%r22}, {%r1576}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1579, %r1580}, {%r26}, {%r1576}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1585, %r1586}, {%r22}, {%r1570}, {%r1579, %r1580};
	// end inline asm
	ld.shared.u32 	%r1592, [%rd16+4];
	// begin inline asm
	lop3.b32 %r1591, %r1592, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2729, %r1591, 2021161080;
	xor.b32  	%r1608, %r2729, -2139062144;
	shr.u32 	%r1596, %r1592, 4;
	// begin inline asm
	lop3.b32 %r1595, %r1596, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2730, %r1595, 2021161080;
	xor.b32  	%r1602, %r2730, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1599, %r1600}, {%r34}, {%r1602}, {%r1567, %r1568};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1605, %r1606}, {%r30}, {%r1608}, {%r1573, %r1574};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1611, %r1612}, {%r34}, {%r1608}, {%r1585, %r1586};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1617, %r1618}, {%r30}, {%r1602}, {%r1611, %r1612};
	// end inline asm
	ld.shared.u32 	%r1624, [%rd16+8];
	// begin inline asm
	lop3.b32 %r1623, %r1624, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2731, %r1623, 2021161080;
	xor.b32  	%r1640, %r2731, -2139062144;
	shr.u32 	%r1628, %r1624, 4;
	// begin inline asm
	lop3.b32 %r1627, %r1628, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2732, %r1627, 2021161080;
	xor.b32  	%r1634, %r2732, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1631, %r1632}, {%r42}, {%r1634}, {%r1599, %r1600};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1637, %r1638}, {%r38}, {%r1640}, {%r1605, %r1606};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1643, %r1644}, {%r42}, {%r1640}, {%r1617, %r1618};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1649, %r1650}, {%r38}, {%r1634}, {%r1643, %r1644};
	// end inline asm
	ld.shared.u32 	%r1656, [%rd16+12];
	// begin inline asm
	lop3.b32 %r1655, %r1656, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2733, %r1655, 2021161080;
	xor.b32  	%r1672, %r2733, -2139062144;
	shr.u32 	%r1660, %r1656, 4;
	// begin inline asm
	lop3.b32 %r1659, %r1660, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2734, %r1659, 2021161080;
	xor.b32  	%r1666, %r2734, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1663, %r1664}, {%r50}, {%r1666}, {%r1631, %r1632};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1669, %r1670}, {%r46}, {%r1672}, {%r1637, %r1638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1675, %r1676}, {%r50}, {%r1672}, {%r1649, %r1650};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1681, %r1682}, {%r46}, {%r1666}, {%r1675, %r1676};
	// end inline asm
	ld.shared.u32 	%r1688, [%rd16+16];
	// begin inline asm
	lop3.b32 %r1687, %r1688, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2735, %r1687, 2021161080;
	xor.b32  	%r1704, %r2735, -2139062144;
	shr.u32 	%r1692, %r1688, 4;
	// begin inline asm
	lop3.b32 %r1691, %r1692, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2736, %r1691, 2021161080;
	xor.b32  	%r1698, %r2736, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1695, %r1696}, {%r27}, {%r1698}, {%r1663, %r1664};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1701, %r1702}, {%r23}, {%r1704}, {%r1669, %r1670};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1707, %r1708}, {%r27}, {%r1704}, {%r1681, %r1682};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1713, %r1714}, {%r23}, {%r1698}, {%r1707, %r1708};
	// end inline asm
	ld.shared.u32 	%r1720, [%rd16+20];
	// begin inline asm
	lop3.b32 %r1719, %r1720, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2737, %r1719, 2021161080;
	xor.b32  	%r1736, %r2737, -2139062144;
	shr.u32 	%r1724, %r1720, 4;
	// begin inline asm
	lop3.b32 %r1723, %r1724, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2738, %r1723, 2021161080;
	xor.b32  	%r1730, %r2738, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1727, %r1728}, {%r35}, {%r1730}, {%r1695, %r1696};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1733, %r1734}, {%r31}, {%r1736}, {%r1701, %r1702};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1739, %r1740}, {%r35}, {%r1736}, {%r1713, %r1714};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1745, %r1746}, {%r31}, {%r1730}, {%r1739, %r1740};
	// end inline asm
	ld.shared.u32 	%r1752, [%rd16+24];
	// begin inline asm
	lop3.b32 %r1751, %r1752, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2739, %r1751, 2021161080;
	xor.b32  	%r1768, %r2739, -2139062144;
	shr.u32 	%r1756, %r1752, 4;
	// begin inline asm
	lop3.b32 %r1755, %r1756, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2740, %r1755, 2021161080;
	xor.b32  	%r1762, %r2740, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1759, %r1760}, {%r43}, {%r1762}, {%r1727, %r1728};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1765, %r1766}, {%r39}, {%r1768}, {%r1733, %r1734};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1771, %r1772}, {%r43}, {%r1768}, {%r1745, %r1746};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1777, %r1778}, {%r39}, {%r1762}, {%r1771, %r1772};
	// end inline asm
	ld.shared.u32 	%r1784, [%rd16+28];
	// begin inline asm
	lop3.b32 %r1783, %r1784, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2741, %r1783, 2021161080;
	xor.b32  	%r1800, %r2741, -2139062144;
	shr.u32 	%r1788, %r1784, 4;
	// begin inline asm
	lop3.b32 %r1787, %r1788, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2742, %r1787, 2021161080;
	xor.b32  	%r1794, %r2742, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1791, %r1792}, {%r51}, {%r1794}, {%r1759, %r1760};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1797, %r1798}, {%r47}, {%r1800}, {%r1765, %r1766};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1803, %r1804}, {%r51}, {%r1800}, {%r1777, %r1778};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1809, %r1810}, {%r47}, {%r1794}, {%r1803, %r1804};
	// end inline asm
	add.s32 	%r2743, %r1809, 4;
	shr.s32 	%r1817, %r2743, 3;
	sub.s32 	%r2744, %r1791, %r1797;
	add.s32 	%r2745, %r2744, 4;
	shr.s32 	%r1816, %r2745, 3;
	add.s32 	%r2746, %r1810, 4;
	shr.s32 	%r1820, %r2746, 3;
	sub.s32 	%r2747, %r1792, %r1798;
	add.s32 	%r2748, %r2747, 4;
	shr.s32 	%r1819, %r2748, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1815, %r1816, %r1817;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r1818, %r1819, %r1820;
	// end inline asm
	st.shared.u32 	[%rd7+8192], %r1815;
	st.shared.u32 	[%rd7+8704], %r1818;
	ld.shared.u32 	%r1822, [%rd15];
	// begin inline asm
	lop3.b32 %r1821, %r1822, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2749, %r1821, 2021161080;
	xor.b32  	%r1838, %r2749, -2139062144;
	shr.u32 	%r1826, %r1822, 4;
	// begin inline asm
	lop3.b32 %r1825, %r1826, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2750, %r1825, 2021161080;
	xor.b32  	%r1832, %r2750, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1829, %r1830}, {%r28}, {%r1832}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1835, %r1836}, {%r24}, {%r1838}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1841, %r1842}, {%r28}, {%r1838}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1847, %r1848}, {%r24}, {%r1832}, {%r1841, %r1842};
	// end inline asm
	ld.shared.u32 	%r1854, [%rd16+4];
	// begin inline asm
	lop3.b32 %r1853, %r1854, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2751, %r1853, 2021161080;
	xor.b32  	%r1870, %r2751, -2139062144;
	shr.u32 	%r1858, %r1854, 4;
	// begin inline asm
	lop3.b32 %r1857, %r1858, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2752, %r1857, 2021161080;
	xor.b32  	%r1864, %r2752, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1861, %r1862}, {%r36}, {%r1864}, {%r1829, %r1830};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1867, %r1868}, {%r32}, {%r1870}, {%r1835, %r1836};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1873, %r1874}, {%r36}, {%r1870}, {%r1847, %r1848};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1879, %r1880}, {%r32}, {%r1864}, {%r1873, %r1874};
	// end inline asm
	ld.shared.u32 	%r1886, [%rd16+8];
	// begin inline asm
	lop3.b32 %r1885, %r1886, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2753, %r1885, 2021161080;
	xor.b32  	%r1902, %r2753, -2139062144;
	shr.u32 	%r1890, %r1886, 4;
	// begin inline asm
	lop3.b32 %r1889, %r1890, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2754, %r1889, 2021161080;
	xor.b32  	%r1896, %r2754, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1893, %r1894}, {%r44}, {%r1896}, {%r1861, %r1862};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1899, %r1900}, {%r40}, {%r1902}, {%r1867, %r1868};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1905, %r1906}, {%r44}, {%r1902}, {%r1879, %r1880};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1911, %r1912}, {%r40}, {%r1896}, {%r1905, %r1906};
	// end inline asm
	ld.shared.u32 	%r1918, [%rd16+12];
	// begin inline asm
	lop3.b32 %r1917, %r1918, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2755, %r1917, 2021161080;
	xor.b32  	%r1934, %r2755, -2139062144;
	shr.u32 	%r1922, %r1918, 4;
	// begin inline asm
	lop3.b32 %r1921, %r1922, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2756, %r1921, 2021161080;
	xor.b32  	%r1928, %r2756, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1925, %r1926}, {%r52}, {%r1928}, {%r1893, %r1894};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1931, %r1932}, {%r48}, {%r1934}, {%r1899, %r1900};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1937, %r1938}, {%r52}, {%r1934}, {%r1911, %r1912};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1943, %r1944}, {%r48}, {%r1928}, {%r1937, %r1938};
	// end inline asm
	ld.shared.u32 	%r1950, [%rd16+16];
	// begin inline asm
	lop3.b32 %r1949, %r1950, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2757, %r1949, 2021161080;
	xor.b32  	%r1966, %r2757, -2139062144;
	shr.u32 	%r1954, %r1950, 4;
	// begin inline asm
	lop3.b32 %r1953, %r1954, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2758, %r1953, 2021161080;
	xor.b32  	%r1960, %r2758, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1957, %r1958}, {%r29}, {%r1960}, {%r1925, %r1926};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1963, %r1964}, {%r25}, {%r1966}, {%r1931, %r1932};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1969, %r1970}, {%r29}, {%r1966}, {%r1943, %r1944};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1975, %r1976}, {%r25}, {%r1960}, {%r1969, %r1970};
	// end inline asm
	ld.shared.u32 	%r1982, [%rd16+20];
	// begin inline asm
	lop3.b32 %r1981, %r1982, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2759, %r1981, 2021161080;
	xor.b32  	%r1998, %r2759, -2139062144;
	shr.u32 	%r1986, %r1982, 4;
	// begin inline asm
	lop3.b32 %r1985, %r1986, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2760, %r1985, 2021161080;
	xor.b32  	%r1992, %r2760, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1989, %r1990}, {%r37}, {%r1992}, {%r1957, %r1958};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r1995, %r1996}, {%r33}, {%r1998}, {%r1963, %r1964};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2001, %r2002}, {%r37}, {%r1998}, {%r1975, %r1976};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2007, %r2008}, {%r33}, {%r1992}, {%r2001, %r2002};
	// end inline asm
	ld.shared.u32 	%r2014, [%rd16+24];
	// begin inline asm
	lop3.b32 %r2013, %r2014, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2761, %r2013, 2021161080;
	xor.b32  	%r2030, %r2761, -2139062144;
	shr.u32 	%r2018, %r2014, 4;
	// begin inline asm
	lop3.b32 %r2017, %r2018, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2762, %r2017, 2021161080;
	xor.b32  	%r2024, %r2762, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2021, %r2022}, {%r45}, {%r2024}, {%r1989, %r1990};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2027, %r2028}, {%r41}, {%r2030}, {%r1995, %r1996};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2033, %r2034}, {%r45}, {%r2030}, {%r2007, %r2008};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2039, %r2040}, {%r41}, {%r2024}, {%r2033, %r2034};
	// end inline asm
	ld.shared.u32 	%r2046, [%rd16+28];
	// begin inline asm
	lop3.b32 %r2045, %r2046, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2763, %r2045, 2021161080;
	xor.b32  	%r2062, %r2763, -2139062144;
	shr.u32 	%r2050, %r2046, 4;
	// begin inline asm
	lop3.b32 %r2049, %r2050, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2764, %r2049, 2021161080;
	xor.b32  	%r2056, %r2764, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2053, %r2054}, {%r53}, {%r2056}, {%r2021, %r2022};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2059, %r2060}, {%r49}, {%r2062}, {%r2027, %r2028};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2065, %r2066}, {%r53}, {%r2062}, {%r2039, %r2040};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2071, %r2072}, {%r49}, {%r2056}, {%r2065, %r2066};
	// end inline asm
	add.s32 	%r2765, %r2071, 4;
	shr.s32 	%r2079, %r2765, 3;
	sub.s32 	%r2766, %r2053, %r2059;
	add.s32 	%r2767, %r2766, 4;
	shr.s32 	%r2078, %r2767, 3;
	add.s32 	%r2768, %r2072, 4;
	shr.s32 	%r2082, %r2768, 3;
	sub.s32 	%r2769, %r2054, %r2060;
	add.s32 	%r2770, %r2769, 4;
	shr.s32 	%r2081, %r2770, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2077, %r2078, %r2079;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2080, %r2081, %r2082;
	// end inline asm
	st.shared.u32 	[%rd17+32], %r2077;
	st.shared.u32 	[%rd18+32], %r2080;
	ld.shared.u32 	%r2084, [%rd19];
	// begin inline asm
	lop3.b32 %r2083, %r2084, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2771, %r2083, 2021161080;
	xor.b32  	%r2100, %r2771, -2139062144;
	shr.u32 	%r2088, %r2084, 4;
	// begin inline asm
	lop3.b32 %r2087, %r2088, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2772, %r2087, 2021161080;
	xor.b32  	%r2094, %r2772, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2091, %r2092}, {%r26}, {%r2094}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2097, %r2098}, {%r22}, {%r2100}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2103, %r2104}, {%r26}, {%r2100}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2109, %r2110}, {%r22}, {%r2094}, {%r2103, %r2104};
	// end inline asm
	ld.shared.u32 	%r2116, [%rd20+4];
	// begin inline asm
	lop3.b32 %r2115, %r2116, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2773, %r2115, 2021161080;
	xor.b32  	%r2132, %r2773, -2139062144;
	shr.u32 	%r2120, %r2116, 4;
	// begin inline asm
	lop3.b32 %r2119, %r2120, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2774, %r2119, 2021161080;
	xor.b32  	%r2126, %r2774, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2123, %r2124}, {%r34}, {%r2126}, {%r2091, %r2092};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2129, %r2130}, {%r30}, {%r2132}, {%r2097, %r2098};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2135, %r2136}, {%r34}, {%r2132}, {%r2109, %r2110};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2141, %r2142}, {%r30}, {%r2126}, {%r2135, %r2136};
	// end inline asm
	ld.shared.u32 	%r2148, [%rd20+8];
	// begin inline asm
	lop3.b32 %r2147, %r2148, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2775, %r2147, 2021161080;
	xor.b32  	%r2164, %r2775, -2139062144;
	shr.u32 	%r2152, %r2148, 4;
	// begin inline asm
	lop3.b32 %r2151, %r2152, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2776, %r2151, 2021161080;
	xor.b32  	%r2158, %r2776, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2155, %r2156}, {%r42}, {%r2158}, {%r2123, %r2124};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2161, %r2162}, {%r38}, {%r2164}, {%r2129, %r2130};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2167, %r2168}, {%r42}, {%r2164}, {%r2141, %r2142};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2173, %r2174}, {%r38}, {%r2158}, {%r2167, %r2168};
	// end inline asm
	ld.shared.u32 	%r2180, [%rd20+12];
	// begin inline asm
	lop3.b32 %r2179, %r2180, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2777, %r2179, 2021161080;
	xor.b32  	%r2196, %r2777, -2139062144;
	shr.u32 	%r2184, %r2180, 4;
	// begin inline asm
	lop3.b32 %r2183, %r2184, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2778, %r2183, 2021161080;
	xor.b32  	%r2190, %r2778, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2187, %r2188}, {%r50}, {%r2190}, {%r2155, %r2156};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2193, %r2194}, {%r46}, {%r2196}, {%r2161, %r2162};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2199, %r2200}, {%r50}, {%r2196}, {%r2173, %r2174};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2205, %r2206}, {%r46}, {%r2190}, {%r2199, %r2200};
	// end inline asm
	ld.shared.u32 	%r2212, [%rd20+16];
	// begin inline asm
	lop3.b32 %r2211, %r2212, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2779, %r2211, 2021161080;
	xor.b32  	%r2228, %r2779, -2139062144;
	shr.u32 	%r2216, %r2212, 4;
	// begin inline asm
	lop3.b32 %r2215, %r2216, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2780, %r2215, 2021161080;
	xor.b32  	%r2222, %r2780, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2219, %r2220}, {%r27}, {%r2222}, {%r2187, %r2188};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2225, %r2226}, {%r23}, {%r2228}, {%r2193, %r2194};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2231, %r2232}, {%r27}, {%r2228}, {%r2205, %r2206};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2237, %r2238}, {%r23}, {%r2222}, {%r2231, %r2232};
	// end inline asm
	ld.shared.u32 	%r2244, [%rd20+20];
	// begin inline asm
	lop3.b32 %r2243, %r2244, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2781, %r2243, 2021161080;
	xor.b32  	%r2260, %r2781, -2139062144;
	shr.u32 	%r2248, %r2244, 4;
	// begin inline asm
	lop3.b32 %r2247, %r2248, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2782, %r2247, 2021161080;
	xor.b32  	%r2254, %r2782, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2251, %r2252}, {%r35}, {%r2254}, {%r2219, %r2220};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2257, %r2258}, {%r31}, {%r2260}, {%r2225, %r2226};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2263, %r2264}, {%r35}, {%r2260}, {%r2237, %r2238};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2269, %r2270}, {%r31}, {%r2254}, {%r2263, %r2264};
	// end inline asm
	ld.shared.u32 	%r2276, [%rd20+24];
	// begin inline asm
	lop3.b32 %r2275, %r2276, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2783, %r2275, 2021161080;
	xor.b32  	%r2292, %r2783, -2139062144;
	shr.u32 	%r2280, %r2276, 4;
	// begin inline asm
	lop3.b32 %r2279, %r2280, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2784, %r2279, 2021161080;
	xor.b32  	%r2286, %r2784, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2283, %r2284}, {%r43}, {%r2286}, {%r2251, %r2252};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2289, %r2290}, {%r39}, {%r2292}, {%r2257, %r2258};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2295, %r2296}, {%r43}, {%r2292}, {%r2269, %r2270};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2301, %r2302}, {%r39}, {%r2286}, {%r2295, %r2296};
	// end inline asm
	ld.shared.u32 	%r2308, [%rd20+28];
	// begin inline asm
	lop3.b32 %r2307, %r2308, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2785, %r2307, 2021161080;
	xor.b32  	%r2324, %r2785, -2139062144;
	shr.u32 	%r2312, %r2308, 4;
	// begin inline asm
	lop3.b32 %r2311, %r2312, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2786, %r2311, 2021161080;
	xor.b32  	%r2318, %r2786, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2315, %r2316}, {%r51}, {%r2318}, {%r2283, %r2284};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2321, %r2322}, {%r47}, {%r2324}, {%r2289, %r2290};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2327, %r2328}, {%r51}, {%r2324}, {%r2301, %r2302};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2333, %r2334}, {%r47}, {%r2318}, {%r2327, %r2328};
	// end inline asm
	add.s32 	%r2787, %r2333, 4;
	shr.s32 	%r2341, %r2787, 3;
	sub.s32 	%r2788, %r2315, %r2321;
	add.s32 	%r2789, %r2788, 4;
	shr.s32 	%r2340, %r2789, 3;
	add.s32 	%r2790, %r2334, 4;
	shr.s32 	%r2344, %r2790, 3;
	sub.s32 	%r2791, %r2316, %r2322;
	add.s32 	%r2792, %r2791, 4;
	shr.s32 	%r2343, %r2792, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2339, %r2340, %r2341;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2342, %r2343, %r2344;
	// end inline asm
	st.shared.u32 	[%rd7+12288], %r2339;
	st.shared.u32 	[%rd7+12800], %r2342;
	ld.shared.u32 	%r2346, [%rd19];
	// begin inline asm
	lop3.b32 %r2345, %r2346, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2793, %r2345, 2021161080;
	xor.b32  	%r2362, %r2793, -2139062144;
	shr.u32 	%r2350, %r2346, 4;
	// begin inline asm
	lop3.b32 %r2349, %r2350, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2794, %r2349, 2021161080;
	xor.b32  	%r2356, %r2794, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2353, %r2354}, {%r28}, {%r2356}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2359, %r2360}, {%r24}, {%r2362}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2365, %r2366}, {%r28}, {%r2362}, {%r322, %r322};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2371, %r2372}, {%r24}, {%r2356}, {%r2365, %r2366};
	// end inline asm
	ld.shared.u32 	%r2378, [%rd20+4];
	// begin inline asm
	lop3.b32 %r2377, %r2378, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2795, %r2377, 2021161080;
	xor.b32  	%r2394, %r2795, -2139062144;
	shr.u32 	%r2382, %r2378, 4;
	// begin inline asm
	lop3.b32 %r2381, %r2382, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2796, %r2381, 2021161080;
	xor.b32  	%r2388, %r2796, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2385, %r2386}, {%r36}, {%r2388}, {%r2353, %r2354};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2391, %r2392}, {%r32}, {%r2394}, {%r2359, %r2360};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2397, %r2398}, {%r36}, {%r2394}, {%r2371, %r2372};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2403, %r2404}, {%r32}, {%r2388}, {%r2397, %r2398};
	// end inline asm
	ld.shared.u32 	%r2410, [%rd20+8];
	// begin inline asm
	lop3.b32 %r2409, %r2410, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2797, %r2409, 2021161080;
	xor.b32  	%r2426, %r2797, -2139062144;
	shr.u32 	%r2414, %r2410, 4;
	// begin inline asm
	lop3.b32 %r2413, %r2414, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2798, %r2413, 2021161080;
	xor.b32  	%r2420, %r2798, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2417, %r2418}, {%r44}, {%r2420}, {%r2385, %r2386};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2423, %r2424}, {%r40}, {%r2426}, {%r2391, %r2392};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2429, %r2430}, {%r44}, {%r2426}, {%r2403, %r2404};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2435, %r2436}, {%r40}, {%r2420}, {%r2429, %r2430};
	// end inline asm
	ld.shared.u32 	%r2442, [%rd20+12];
	// begin inline asm
	lop3.b32 %r2441, %r2442, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2799, %r2441, 2021161080;
	xor.b32  	%r2458, %r2799, -2139062144;
	shr.u32 	%r2446, %r2442, 4;
	// begin inline asm
	lop3.b32 %r2445, %r2446, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2800, %r2445, 2021161080;
	xor.b32  	%r2452, %r2800, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2449, %r2450}, {%r52}, {%r2452}, {%r2417, %r2418};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2455, %r2456}, {%r48}, {%r2458}, {%r2423, %r2424};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2461, %r2462}, {%r52}, {%r2458}, {%r2435, %r2436};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2467, %r2468}, {%r48}, {%r2452}, {%r2461, %r2462};
	// end inline asm
	ld.shared.u32 	%r2474, [%rd20+16];
	// begin inline asm
	lop3.b32 %r2473, %r2474, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2801, %r2473, 2021161080;
	xor.b32  	%r2490, %r2801, -2139062144;
	shr.u32 	%r2478, %r2474, 4;
	// begin inline asm
	lop3.b32 %r2477, %r2478, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2802, %r2477, 2021161080;
	xor.b32  	%r2484, %r2802, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2481, %r2482}, {%r29}, {%r2484}, {%r2449, %r2450};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2487, %r2488}, {%r25}, {%r2490}, {%r2455, %r2456};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2493, %r2494}, {%r29}, {%r2490}, {%r2467, %r2468};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2499, %r2500}, {%r25}, {%r2484}, {%r2493, %r2494};
	// end inline asm
	ld.shared.u32 	%r2506, [%rd20+20];
	// begin inline asm
	lop3.b32 %r2505, %r2506, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2803, %r2505, 2021161080;
	xor.b32  	%r2522, %r2803, -2139062144;
	shr.u32 	%r2510, %r2506, 4;
	// begin inline asm
	lop3.b32 %r2509, %r2510, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2804, %r2509, 2021161080;
	xor.b32  	%r2516, %r2804, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2513, %r2514}, {%r37}, {%r2516}, {%r2481, %r2482};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2519, %r2520}, {%r33}, {%r2522}, {%r2487, %r2488};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2525, %r2526}, {%r37}, {%r2522}, {%r2499, %r2500};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2531, %r2532}, {%r33}, {%r2516}, {%r2525, %r2526};
	// end inline asm
	ld.shared.u32 	%r2538, [%rd20+24];
	// begin inline asm
	lop3.b32 %r2537, %r2538, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2805, %r2537, 2021161080;
	xor.b32  	%r2554, %r2805, -2139062144;
	shr.u32 	%r2542, %r2538, 4;
	// begin inline asm
	lop3.b32 %r2541, %r2542, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2806, %r2541, 2021161080;
	xor.b32  	%r2548, %r2806, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2545, %r2546}, {%r45}, {%r2548}, {%r2513, %r2514};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2551, %r2552}, {%r41}, {%r2554}, {%r2519, %r2520};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2557, %r2558}, {%r45}, {%r2554}, {%r2531, %r2532};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2563, %r2564}, {%r41}, {%r2548}, {%r2557, %r2558};
	// end inline asm
	ld.shared.u32 	%r2570, [%rd20+28];
	// begin inline asm
	lop3.b32 %r2569, %r2570, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2807, %r2569, 2021161080;
	xor.b32  	%r2586, %r2807, -2139062144;
	shr.u32 	%r2574, %r2570, 4;
	// begin inline asm
	lop3.b32 %r2573, %r2574, %r513, %r514, 40;
	// end inline asm
	add.s32 	%r2808, %r2573, 2021161080;
	xor.b32  	%r2580, %r2808, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2577, %r2578}, {%r53}, {%r2580}, {%r2545, %r2546};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2583, %r2584}, {%r49}, {%r2586}, {%r2551, %r2552};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2589, %r2590}, {%r53}, {%r2586}, {%r2563, %r2564};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r2595, %r2596}, {%r49}, {%r2580}, {%r2589, %r2590};
	// end inline asm
	add.s32 	%r2809, %r2595, 4;
	shr.s32 	%r2603, %r2809, 3;
	sub.s32 	%r2810, %r2577, %r2583;
	add.s32 	%r2811, %r2810, 4;
	shr.s32 	%r2602, %r2811, 3;
	add.s32 	%r2812, %r2596, 4;
	shr.s32 	%r2606, %r2812, 3;
	sub.s32 	%r2813, %r2578, %r2584;
	add.s32 	%r2814, %r2813, 4;
	shr.s32 	%r2605, %r2814, 3;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2601, %r2602, %r2603;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r2604, %r2605, %r2606;
	// end inline asm
	st.shared.u32 	[%rd21+32], %r2601;
	st.shared.u32 	[%rd22+32], %r2604;
	bar.sync 	0;
	ld.shared.u32 	%r2815, [%rd24];
	ld.shared.u32 	%r2816, [%rd24+16384];
	ld.shared.u32 	%r2817, [%rd24+32768];
	ld.shared.u32 	%r2818, [%rd24+49152];
	ld.shared.u32 	%r2819, [%rd26];
	ld.shared.u32 	%r2820, [%rd26+16384];
	ld.shared.u32 	%r2821, [%rd26+32768];
	ld.shared.u32 	%r2822, [%rd26+49152];
	ld.shared.u32 	%r2823, [%rd28];
	ld.shared.u32 	%r2824, [%rd28+16384];
	ld.shared.u32 	%r2825, [%rd28+32768];
	ld.shared.u32 	%r2826, [%rd28+49152];
	ld.shared.u32 	%r2827, [%rd30];
	ld.shared.u32 	%r2828, [%rd30+16384];
	ld.shared.u32 	%r2829, [%rd30+32768];
	ld.shared.u32 	%r2830, [%rd30+49152];
	cvt.s32.s16 	%r2831, %r2815;
	shr.s32 	%r2832, %r2815, 16;
	cvt.s32.s16 	%r2833, %r2816;
	shr.s32 	%r2834, %r2816, 16;
	cvt.s32.s16 	%r2835, %r2817;
	shr.s32 	%r2836, %r2817, 16;
	cvt.s32.s16 	%r2837, %r2818;
	shr.s32 	%r2838, %r2818, 16;
	cvt.s32.s16 	%r2839, %r2819;
	shr.s32 	%r2840, %r2819, 16;
	cvt.s32.s16 	%r2841, %r2820;
	shr.s32 	%r2842, %r2820, 16;
	cvt.s32.s16 	%r2843, %r2821;
	shr.s32 	%r2844, %r2821, 16;
	cvt.s32.s16 	%r2845, %r2822;
	shr.s32 	%r2846, %r2822, 16;
	cvt.s32.s16 	%r2847, %r2823;
	shr.s32 	%r2848, %r2823, 16;
	cvt.s32.s16 	%r2849, %r2824;
	shr.s32 	%r2850, %r2824, 16;
	cvt.s32.s16 	%r2851, %r2825;
	shr.s32 	%r2852, %r2825, 16;
	cvt.s32.s16 	%r2853, %r2826;
	shr.s32 	%r2854, %r2826, 16;
	cvt.s32.s16 	%r2855, %r2827;
	shr.s32 	%r2856, %r2827, 16;
	cvt.s32.s16 	%r2857, %r2828;
	shr.s32 	%r2858, %r2828, 16;
	cvt.s32.s16 	%r2859, %r2829;
	shr.s32 	%r2860, %r2829, 16;
	cvt.s32.s16 	%r2861, %r2830;
	shr.s32 	%r2862, %r2830, 16;
	add.s32 	%r2863, %r2831, %r58;
	add.s32 	%r2864, %r2863, %r2833;
	add.s32 	%r2865, %r2864, %r2835;
	add.s32 	%r2866, %r2865, %r2837;
	shr.s32 	%r2867, %r2866, %r59;
	add.s32 	%r2868, %r2832, %r58;
	add.s32 	%r2869, %r2868, %r2834;
	add.s32 	%r2870, %r2869, %r2836;
	add.s32 	%r2871, %r2870, %r2838;
	shr.s32 	%r2872, %r2871, %r59;
	add.s32 	%r2873, %r2839, %r58;
	add.s32 	%r2874, %r2873, %r2841;
	add.s32 	%r2875, %r2874, %r2843;
	add.s32 	%r2876, %r2875, %r2845;
	shr.s32 	%r2877, %r2876, %r59;
	add.s32 	%r2878, %r2840, %r58;
	add.s32 	%r2879, %r2878, %r2842;
	add.s32 	%r2880, %r2879, %r2844;
	add.s32 	%r2881, %r2880, %r2846;
	shr.s32 	%r2882, %r2881, %r59;
	add.s32 	%r2883, %r2847, %r58;
	add.s32 	%r2884, %r2883, %r2849;
	add.s32 	%r2885, %r2884, %r2851;
	add.s32 	%r2886, %r2885, %r2853;
	shr.s32 	%r2887, %r2886, %r59;
	add.s32 	%r2888, %r2848, %r58;
	add.s32 	%r2889, %r2888, %r2850;
	add.s32 	%r2890, %r2889, %r2852;
	add.s32 	%r2891, %r2890, %r2854;
	shr.s32 	%r2892, %r2891, %r59;
	add.s32 	%r2893, %r2855, %r58;
	add.s32 	%r2894, %r2893, %r2857;
	add.s32 	%r2895, %r2894, %r2859;
	add.s32 	%r2896, %r2895, %r2861;
	shr.s32 	%r2897, %r2896, %r59;
	add.s32 	%r2898, %r2856, %r58;
	add.s32 	%r2899, %r2898, %r2858;
	add.s32 	%r2900, %r2899, %r2860;
	add.s32 	%r2901, %r2900, %r2862;
	shr.s32 	%r2902, %r2901, %r59;
	max.s32 	%r2903, %r2867, -7;
	min.s32 	%r2612, %r2903, 7;
	max.s32 	%r2904, %r2872, -7;
	min.s32 	%r2619, %r2904, 7;
	max.s32 	%r2905, %r2877, -7;
	min.s32 	%r2611, %r2905, 7;
	max.s32 	%r2906, %r2882, -7;
	min.s32 	%r2618, %r2906, 7;
	max.s32 	%r2907, %r2887, -7;
	min.s32 	%r2609, %r2907, 7;
	max.s32 	%r2908, %r2892, -7;
	min.s32 	%r2616, %r2908, 7;
	max.s32 	%r2909, %r2897, -7;
	min.s32 	%r2608, %r2909, 7;
	max.s32 	%r2910, %r2902, -7;
	min.s32 	%r2615, %r2910, 7;
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2607, %r2608, %r2609, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2610, %r2611, %r2612, %r2607;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2614, %r2615, %r2616, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r2617, %r2618, %r2619, %r2614;
	// end inline asm
	shl.b32 	%r2624, %r2617, 4;
	// begin inline asm
	lop3.b32 %r2921, %r514, %r2610, %r2624, 202;
	// end inline asm
	setp.eq.s32 	%p6, %r2989, 0;
	selp.b32 	%r2994, %r2921, %r2994, %p6;
	selp.b32 	%r2995, %r2921, %r2995, %p6;
	setp.eq.s32 	%p7, %r2989, 32;
	selp.b32 	%r2990, %r2921, %r2990, %p7;
	selp.b32 	%r2991, %r2921, %r2991, %p7;
	setp.eq.s32 	%p8, %r2989, 64;
	selp.b32 	%r2992, %r2921, %r2992, %p8;
	selp.b32 	%r2993, %r2921, %r2993, %p8;
	add.s32 	%r2989, %r2989, 32;
	setp.ne.s32 	%p9, %r2989, 128;
	@%p9 bra 	LBB0_6;
// %bb.7:                               // %L15078
                                        //   in Loop: Header=BB0_5 Depth=1
	setp.eq.s32 	%p10, %r60, 0;
	// begin inline asm
	prmt.b32 %r2911, %r2994, %r2990, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2915, %r2995, %r2991, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2919, %r2992, %r2921, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2923, %r2993, %r2921, %r321;
	// end inline asm
	selp.b32 	%r2959, %r2915, %r2911, %p10;
	shfl.sync.bfly.b32	%r2960, %r2959, 1, 31, -1;
	selp.b32 	%r2928, %r2911, %r2960, %p10;
	selp.b32 	%r2929, %r2960, %r2915, %p10;
	selp.b32 	%r2961, %r2923, %r2919, %p10;
	shfl.sync.bfly.b32	%r2962, %r2961, 1, 31, -1;
	selp.b32 	%r2936, %r2919, %r2962, %p10;
	selp.b32 	%r2937, %r2962, %r2923, %p10;
	// begin inline asm
	prmt.b32 %r2927, %r2928, %r2929, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2931, %r2928, %r2929, %r321;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2935, %r2936, %r2937, %r317;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2939, %r2936, %r2937, %r321;
	// end inline asm
	selp.b32 	%r2963, %r2935, %r2927, %p3;
	shfl.sync.bfly.b32	%r2964, %r2963, 2, 31, -1;
	selp.b32 	%r2944, %r2927, %r2964, %p3;
	selp.b32 	%r2945, %r2964, %r2935, %p3;
	selp.b32 	%r2965, %r2939, %r2931, %p3;
	shfl.sync.bfly.b32	%r2966, %r2965, 2, 31, -1;
	selp.b32 	%r2952, %r2931, %r2966, %p3;
	selp.b32 	%r2953, %r2966, %r2939, %p3;
	// begin inline asm
	prmt.b32 %r2943, %r2944, %r2945, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2947, %r2944, %r2945, %r193;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2951, %r2952, %r2953, %r189;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2955, %r2952, %r2953, %r193;
	// end inline asm
	selp.b32 	%r2967, %r2951, %r2943, %p4;
	shfl.sync.bfly.b32	%r2968, %r2967, 4, 31, -1;
	selp.b32 	%r2969, %r2943, %r2968, %p4;
	selp.b32 	%r2970, %r2968, %r2951, %p4;
	selp.b32 	%r2971, %r2955, %r2947, %p4;
	shfl.sync.bfly.b32	%r2972, %r2971, 4, 31, -1;
	selp.b32 	%r2973, %r2947, %r2972, %p4;
	selp.b32 	%r2974, %r2972, %r2955, %p4;
	selp.b32 	%r2975, %r2973, %r2969, %p10;
	shfl.sync.bfly.b32	%r2976, %r2975, 1, 31, -1;
	selp.b32 	%r2977, %r2969, %r2976, %p10;
	selp.b32 	%r2978, %r2976, %r2973, %p10;
	selp.b32 	%r2979, %r2974, %r2970, %p10;
	shfl.sync.bfly.b32	%r2980, %r2979, 1, 31, -1;
	selp.b32 	%r2981, %r2970, %r2980, %p10;
	selp.b32 	%r2982, %r2980, %r2974, %p10;
	or.b32  	%r2983, %r61, %r2988;
	or.b32  	%r2984, %r2983, %r19;
	shr.u32 	%r2985, %r2984, 2;
	add.s32 	%r2986, %r63, %r2985;
	add.s32 	%r2987, %r2986, %r62;
	mul.wide.u32 	%rd84, %r2987, 4;
	add.s64 	%rd85, %rd4, %rd84;
	st.global.v4.u32 	[%rd85], {%r2977, %r2981, %r2978, %r2982};
	add.s32 	%r17, %r2988, 128;
	setp.ne.s32 	%p13, %r2988, 32640;
	mov.u32 	%r2988, %r17;
	@%p13 bra 	LBB0_5;
// %bb.8:                               // %L15419
	ret;
LBB0_1:                                 // %L9
	mov.u64 	%rd32, exception11323;
	cvta.global.u64 	%rd33, %rd32;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 0
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd31;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 1
	// begin inline asm
	exit;
	// end inline asm
LBB0_3:                                 // %L28
	mov.u64 	%rd34, exception11323;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 2
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd31;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 3
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}
