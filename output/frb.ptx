CHORD FRB beamformer
Compiling kernel...
// PTX CompilerJob of kernel #frb(CuDeviceVector{Int32, 1}, CuDeviceVector{Float16x2, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Float16x2, 1}, CuDeviceVector{Int32, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Float16x2, 1}) for sm_86, minthreads=768, blocks_per_sm=1

//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_86
.address_size 64

	// .globl	_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE // -- Begin function _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE
.extern .func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
;
.extern .func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[8]
)
;
.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception1[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE
.visible .entry _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE(
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_0[8],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_1[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_2[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_4[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_5[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_6[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_7[32],
	.param .align 8 .b8 _Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_8[32]
)
.reqntid 768, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<83>;
	.reg .b16 	%rs<297>;
	.reg .f32 	%f<202>;
	.reg .b32 	%r<3578>;
	.reg .b64 	%rd<454>;

// %bb.0:                               // %conversion
	// begin inline asm
	mov.u32 %r341, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p1, %r341, 76895;
	@%p1 bra 	LBB0_2;
	bra.uni 	LBB0_1;
LBB0_2:                                 // %L11
	ld.param.u64 	%rd12, [_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_5];
	mov.u32 	%r1, %tid.y;
	shl.b32 	%r2, %r1, 5;
	mov.u32 	%r3, %tid.x;
	or.b32  	%r348, %r3, %r2;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r349, %r4, 768, %r348;
	mul.wide.u32 	%rd18, %r349, 4;
	add.s64 	%rd5, %rd12, %rd18;
	mov.u32 	%r350, 1;
	st.global.u32 	[%rd5], %r350;
	and.b32  	%r5, %r3, 3;
	shr.u32 	%r6, %r3, 2;
	mul.lo.s32 	%r351, %r5, %r6;
	and.b32  	%r352, %r351, 7;
	cvt.rn.f32.u32 	%f18, %r352;
	mov.f32 	%f19, 0f40800000;
	div.approx.f32 	%f20, %f18, %f19;
	add.f32 	%f21, %f20, %f20;
	mov.b32 	%r353, %f21;
	and.b32  	%r354, %r353, -2147483648;
	or.b32  	%r355, %r354, 1056964608;
	mov.b32 	%f22, %r355;
	add.f32 	%f23, %f21, %f22;
	cvt.rzi.f32.f32 	%f24, %f23;
	abs.f32 	%f25, %f21;
	setp.gt.f32 	%p2, %f25, 0f4B000000;
	selp.f32 	%f26, %f21, %f24, %p2;
	cvt.rzi.f32.f32 	%f27, %f21;
	setp.lt.f32 	%p3, %f25, 0f3F000000;
	selp.f32 	%f28, %f27, %f26, %p3;
	cvt.rzi.s32.f32 	%r356, %f28;
	fma.rn.f32 	%f29, %f28, 0fBF000000, %f20;
	mul.f32 	%f30, %f29, 0f34222169;
	fma.rn.f32 	%f31, %f29, 0f40490FDA, %f30;
	mul.f32 	%f32, %f31, %f31;
	fma.rn.f32 	%f33, %f32, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f34, %f33, %f32, 0f3D2AAABB;
	fma.rn.f32 	%f35, %f34, %f32, 0fBEFFFFFF;
	fma.rn.f32 	%f36, %f35, %f32, 0f3F800000;
	fma.rn.f32 	%f37, %f32, %f31, 0f00000000;
	fma.rn.f32 	%f38, %f32, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f39, %f38, %f32, 0fBE2AAAA8;
	fma.rn.f32 	%f40, %f39, %f37, %f31;
	and.b32  	%r357, %r356, 1;
	setp.eq.b32 	%p4, %r357, 1;
	selp.f32 	%f41, %f36, %f40, %p4;
	selp.f32 	%f42, %f40, %f36, %p4;
	and.b32  	%r358, %r356, 2;
	setp.eq.s32 	%p5, %r358, 0;
	neg.f32 	%f43, %f41;
	selp.f32 	%f44, %f41, %f43, %p5;
	add.s32 	%r359, %r356, 1;
	and.b32  	%r360, %r359, 2;
	mov.f32 	%f200, 0f00000000;
	cvt.rzi.f32.f32 	%f47, %f20;
	setp.eq.f32 	%p7, %f47, %f20;
	mul.f32 	%f48, %f20, 0f00000000;
	shl.b32 	%r9, %r3, 1;
	and.b32  	%r10, %r9, 6;
	or.b32  	%r11, %r10, 1;
	setp.eq.s32 	%p9, %r10, 6;
	mov.f32 	%f192, 0f41C00000;
	mul.lo.s32 	%r3449, %r10, %r6;
	mov.f32 	%f194, %f200;
	mov.f32 	%f195, %f200;
	@%p9 bra 	LBB0_4;
// %bb.3:                               // %L206
	cvt.u16.u32 	%rs1, %r3449;
	and.b16  	%rs2, %rs1, 255;
	mul.lo.s16 	%rs3, %rs2, 171;
	shr.u16 	%rs4, %rs3, 13;
	mul.lo.s16 	%rs5, %rs4, 48;
	sub.s16 	%rs6, %rs1, %rs5;
	and.b16  	%rs7, %rs6, 255;
	cvt.rn.f32.u16 	%f53, %rs7;
	div.approx.f32 	%f55, %f53, %f192;
	add.f32 	%f56, %f55, %f55;
	mov.b32 	%r362, %f56;
	and.b32  	%r363, %r362, -2147483648;
	or.b32  	%r364, %r363, 1056964608;
	mov.b32 	%f57, %r364;
	add.f32 	%f58, %f56, %f57;
	cvt.rzi.f32.f32 	%f59, %f58;
	abs.f32 	%f60, %f56;
	setp.gt.f32 	%p10, %f60, 0f4B000000;
	selp.f32 	%f61, %f56, %f59, %p10;
	cvt.rzi.f32.f32 	%f62, %f56;
	setp.lt.f32 	%p11, %f60, 0f3F000000;
	selp.f32 	%f63, %f62, %f61, %p11;
	cvt.rzi.s32.f32 	%r365, %f63;
	fma.rn.f32 	%f64, %f63, 0fBF000000, %f55;
	mul.f32 	%f65, %f64, 0f34222169;
	fma.rn.f32 	%f66, %f64, 0f40490FDA, %f65;
	mul.f32 	%f67, %f66, %f66;
	fma.rn.f32 	%f68, %f67, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f69, %f68, %f67, 0f3D2AAABB;
	fma.rn.f32 	%f70, %f69, %f67, 0fBEFFFFFF;
	fma.rn.f32 	%f71, %f70, %f67, 0f3F800000;
	fma.rn.f32 	%f72, %f67, %f66, 0f00000000;
	fma.rn.f32 	%f73, %f67, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f74, %f73, %f67, 0fBE2AAAA8;
	fma.rn.f32 	%f75, %f74, %f72, %f66;
	and.b32  	%r366, %r365, 1;
	setp.eq.b32 	%p12, %r366, 1;
	selp.f32 	%f76, %f71, %f75, %p12;
	selp.f32 	%f77, %f75, %f71, %p12;
	and.b32  	%r367, %r365, 2;
	setp.eq.s32 	%p13, %r367, 0;
	neg.f32 	%f78, %f76;
	selp.f32 	%f79, %f76, %f78, %p13;
	add.s32 	%r368, %r365, 1;
	and.b32  	%r369, %r368, 2;
	setp.eq.s32 	%p14, %r369, 0;
	mov.f32 	%f80, 0f00000000;
	sub.f32 	%f81, %f80, %f77;
	selp.f32 	%f82, %f77, %f81, %p14;
	cvt.rzi.f32.f32 	%f83, %f55;
	setp.eq.f32 	%p15, %f83, %f55;
	mul.f32 	%f84, %f55, 0f00000000;
	selp.f32 	%f195, %f84, %f79, %p15;
	abs.f32 	%f85, %f55;
	setp.gt.f32 	%p16, %f85, 0f4B800000;
	add.f32 	%f86, %f195, 0f3F800000;
	selp.f32 	%f194, %f86, %f82, %p16;
LBB0_4:                                 // %L221
	setp.eq.s32 	%p6, %r360, 0;
	sub.f32 	%f45, %f200, %f42;
	selp.f32 	%f49, %f48, %f44, %p7;
	abs.f32 	%f50, %f20;
	setp.gt.u32 	%p17, %r11, 5;
	mul.lo.s32 	%r3450, %r11, %r6;
	mov.f32 	%f196, %f200;
	mov.f32 	%f197, %f200;
	@%p17 bra 	LBB0_6;
// %bb.5:                               // %L225
	cvt.u16.u32 	%rs8, %r3450;
	and.b16  	%rs9, %rs8, 255;
	mul.lo.s16 	%rs10, %rs9, 171;
	shr.u16 	%rs11, %rs10, 13;
	mul.lo.s16 	%rs12, %rs11, 48;
	sub.s16 	%rs13, %rs8, %rs12;
	and.b16  	%rs14, %rs13, 255;
	cvt.rn.f32.u16 	%f88, %rs14;
	div.approx.f32 	%f90, %f88, %f192;
	add.f32 	%f91, %f90, %f90;
	mov.b32 	%r371, %f91;
	and.b32  	%r372, %r371, -2147483648;
	or.b32  	%r373, %r372, 1056964608;
	mov.b32 	%f92, %r373;
	add.f32 	%f93, %f91, %f92;
	cvt.rzi.f32.f32 	%f94, %f93;
	abs.f32 	%f95, %f91;
	setp.gt.f32 	%p18, %f95, 0f4B000000;
	selp.f32 	%f96, %f91, %f94, %p18;
	cvt.rzi.f32.f32 	%f97, %f91;
	setp.lt.f32 	%p19, %f95, 0f3F000000;
	selp.f32 	%f98, %f97, %f96, %p19;
	cvt.rzi.s32.f32 	%r374, %f98;
	fma.rn.f32 	%f99, %f98, 0fBF000000, %f90;
	mul.f32 	%f100, %f99, 0f34222169;
	fma.rn.f32 	%f101, %f99, 0f40490FDA, %f100;
	mul.f32 	%f102, %f101, %f101;
	fma.rn.f32 	%f103, %f102, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f104, %f103, %f102, 0f3D2AAABB;
	fma.rn.f32 	%f105, %f104, %f102, 0fBEFFFFFF;
	fma.rn.f32 	%f106, %f105, %f102, 0f3F800000;
	fma.rn.f32 	%f107, %f102, %f101, 0f00000000;
	fma.rn.f32 	%f108, %f102, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f109, %f108, %f102, 0fBE2AAAA8;
	fma.rn.f32 	%f110, %f109, %f107, %f101;
	and.b32  	%r375, %r374, 1;
	setp.eq.b32 	%p20, %r375, 1;
	selp.f32 	%f111, %f106, %f110, %p20;
	selp.f32 	%f112, %f110, %f106, %p20;
	and.b32  	%r376, %r374, 2;
	setp.eq.s32 	%p21, %r376, 0;
	neg.f32 	%f113, %f111;
	selp.f32 	%f114, %f111, %f113, %p21;
	add.s32 	%r377, %r374, 1;
	and.b32  	%r378, %r377, 2;
	setp.eq.s32 	%p22, %r378, 0;
	mov.f32 	%f115, 0f00000000;
	sub.f32 	%f116, %f115, %f112;
	selp.f32 	%f117, %f112, %f116, %p22;
	cvt.rzi.f32.f32 	%f118, %f90;
	setp.eq.f32 	%p23, %f118, %f90;
	mul.f32 	%f119, %f90, 0f00000000;
	selp.f32 	%f197, %f119, %f114, %p23;
	abs.f32 	%f120, %f90;
	setp.gt.f32 	%p24, %f120, 0f4B800000;
	add.f32 	%f121, %f197, 0f3F800000;
	selp.f32 	%f196, %f121, %f117, %p24;
LBB0_6:                                 // %L240
	selp.f32 	%f46, %f42, %f45, %p6;
	setp.gt.f32 	%p8, %f50, 0f4B800000;
	add.f32 	%f51, %f49, 0f3F800000;
	setp.gt.u32 	%p26, %r3, 23;
	or.pred  	%p27, %p9, %p26;
	mov.f32 	%f193, 0f40400000;
	mov.f32 	%f198, %f200;
	mov.f32 	%f199, %f200;
	@%p27 bra 	LBB0_8;
// %bb.7:                               // %L282
	cvt.u16.u32 	%rs15, %r3449;
	and.b16  	%rs16, %rs15, 255;
	mul.lo.s16 	%rs17, %rs16, 171;
	shr.u16 	%rs18, %rs17, 10;
	mul.lo.s16 	%rs19, %rs18, 6;
	sub.s16 	%rs20, %rs15, %rs19;
	and.b16  	%rs21, %rs20, 255;
	cvt.rn.f32.u16 	%f123, %rs21;
	div.approx.f32 	%f125, %f123, %f193;
	add.f32 	%f126, %f125, %f125;
	mov.b32 	%r386, %f126;
	and.b32  	%r387, %r386, -2147483648;
	or.b32  	%r388, %r387, 1056964608;
	mov.b32 	%f127, %r388;
	add.f32 	%f128, %f126, %f127;
	cvt.rzi.f32.f32 	%f129, %f128;
	abs.f32 	%f130, %f126;
	setp.gt.f32 	%p28, %f130, 0f4B000000;
	selp.f32 	%f131, %f126, %f129, %p28;
	cvt.rzi.f32.f32 	%f132, %f126;
	setp.lt.f32 	%p29, %f130, 0f3F000000;
	selp.f32 	%f133, %f132, %f131, %p29;
	cvt.rzi.s32.f32 	%r389, %f133;
	fma.rn.f32 	%f134, %f133, 0fBF000000, %f125;
	mul.f32 	%f135, %f134, 0f34222169;
	fma.rn.f32 	%f136, %f134, 0f40490FDA, %f135;
	mul.f32 	%f137, %f136, %f136;
	fma.rn.f32 	%f138, %f137, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f139, %f138, %f137, 0f3D2AAABB;
	fma.rn.f32 	%f140, %f139, %f137, 0fBEFFFFFF;
	fma.rn.f32 	%f141, %f140, %f137, 0f3F800000;
	fma.rn.f32 	%f142, %f137, %f136, 0f00000000;
	fma.rn.f32 	%f143, %f137, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f144, %f143, %f137, 0fBE2AAAA8;
	fma.rn.f32 	%f145, %f144, %f142, %f136;
	and.b32  	%r390, %r389, 1;
	setp.eq.b32 	%p30, %r390, 1;
	selp.f32 	%f146, %f141, %f145, %p30;
	selp.f32 	%f147, %f145, %f141, %p30;
	and.b32  	%r391, %r389, 2;
	setp.eq.s32 	%p31, %r391, 0;
	neg.f32 	%f148, %f146;
	selp.f32 	%f149, %f146, %f148, %p31;
	add.s32 	%r392, %r389, 1;
	and.b32  	%r393, %r392, 2;
	setp.eq.s32 	%p32, %r393, 0;
	mov.f32 	%f150, 0f00000000;
	sub.f32 	%f151, %f150, %f147;
	selp.f32 	%f152, %f147, %f151, %p32;
	cvt.rzi.f32.f32 	%f153, %f125;
	setp.eq.f32 	%p33, %f153, %f125;
	mul.f32 	%f154, %f125, 0f00000000;
	selp.f32 	%f199, %f154, %f149, %p33;
	abs.f32 	%f155, %f125;
	setp.gt.f32 	%p34, %f155, 0f4B800000;
	add.f32 	%f156, %f199, 0f3F800000;
	selp.f32 	%f198, %f156, %f152, %p34;
LBB0_8:                                 // %L297
	selp.f32 	%f52, %f51, %f46, %p8;
	or.pred  	%p37, %p17, %p26;
	mov.f32 	%f201, %f200;
	@%p37 bra 	LBB0_10;
// %bb.9:                               // %L305
	cvt.u16.u32 	%rs22, %r3450;
	and.b16  	%rs23, %rs22, 255;
	mul.lo.s16 	%rs24, %rs23, 171;
	shr.u16 	%rs25, %rs24, 10;
	mul.lo.s16 	%rs26, %rs25, 6;
	sub.s16 	%rs27, %rs22, %rs26;
	and.b16  	%rs28, %rs27, 255;
	cvt.rn.f32.u16 	%f158, %rs28;
	div.approx.f32 	%f160, %f158, %f193;
	add.f32 	%f161, %f160, %f160;
	mov.b32 	%r395, %f161;
	and.b32  	%r396, %r395, -2147483648;
	or.b32  	%r397, %r396, 1056964608;
	mov.b32 	%f162, %r397;
	add.f32 	%f163, %f161, %f162;
	cvt.rzi.f32.f32 	%f164, %f163;
	abs.f32 	%f165, %f161;
	setp.gt.f32 	%p38, %f165, 0f4B000000;
	selp.f32 	%f166, %f161, %f164, %p38;
	cvt.rzi.f32.f32 	%f167, %f161;
	setp.lt.f32 	%p39, %f165, 0f3F000000;
	selp.f32 	%f168, %f167, %f166, %p39;
	cvt.rzi.s32.f32 	%r398, %f168;
	fma.rn.f32 	%f169, %f168, 0fBF000000, %f160;
	mul.f32 	%f170, %f169, 0f34222169;
	fma.rn.f32 	%f171, %f169, 0f40490FDA, %f170;
	mul.f32 	%f172, %f171, %f171;
	fma.rn.f32 	%f173, %f172, 0f37CBAC00, 0fBAB607ED;
	fma.rn.f32 	%f174, %f173, %f172, 0f3D2AAABB;
	fma.rn.f32 	%f175, %f174, %f172, 0fBEFFFFFF;
	fma.rn.f32 	%f176, %f175, %f172, 0f3F800000;
	fma.rn.f32 	%f177, %f172, %f171, 0f00000000;
	fma.rn.f32 	%f178, %f172, 0fB94D4153, 0f3C0885E4;
	fma.rn.f32 	%f179, %f178, %f172, 0fBE2AAAA8;
	fma.rn.f32 	%f180, %f179, %f177, %f171;
	and.b32  	%r399, %r398, 1;
	setp.eq.b32 	%p40, %r399, 1;
	selp.f32 	%f181, %f176, %f180, %p40;
	selp.f32 	%f182, %f180, %f176, %p40;
	and.b32  	%r400, %r398, 2;
	setp.eq.s32 	%p41, %r400, 0;
	neg.f32 	%f183, %f181;
	selp.f32 	%f184, %f181, %f183, %p41;
	add.s32 	%r401, %r398, 1;
	and.b32  	%r402, %r401, 2;
	setp.eq.s32 	%p42, %r402, 0;
	mov.f32 	%f185, 0f00000000;
	sub.f32 	%f186, %f185, %f182;
	selp.f32 	%f187, %f182, %f186, %p42;
	cvt.rzi.f32.f32 	%f188, %f160;
	setp.eq.f32 	%p43, %f188, %f160;
	mul.f32 	%f189, %f160, 0f00000000;
	selp.f32 	%f201, %f189, %f184, %p43;
	abs.f32 	%f190, %f160;
	setp.gt.f32 	%p44, %f190, 0f4B800000;
	add.f32 	%f191, %f201, 0f3F800000;
	selp.f32 	%f200, %f191, %f187, %p44;
LBB0_10:                                // %L320
	mov.b32 	%r344, %f52;
	mov.b32 	%r347, %f49;
	mov.b32 	%r404, %f198;
	mov.b32 	%r405, %f200;
	mov.b32 	%r410, %f199;
	mov.b32 	%r411, %f201;
	mov.u32 	%r3452, 999999999;
	cvt.u16.u32 	%rs295, %r3;
	@%p26 bra 	LBB0_12;
// %bb.11:                              // %L363
	ld.param.u64 	%rd1, [_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_1];
	and.b16  	%rs30, %rs295, 255;
	mul.lo.s16 	%rs31, %rs30, 171;
	shr.u16 	%rs32, %rs31, 12;
	mul.lo.s16 	%rs33, %rs32, 24;
	sub.s16 	%rs34, %rs295, %rs33;
	cvt.u32.u16 	%r416, %rs34;
	and.b32  	%r417, %r416, 255;
	mad.lo.s32 	%r418, %r417, 24, %r1;
	cvt.u16.u32 	%rs35, %r418;
	mul.hi.u16 	%rs36, %rs35, -7281;
	shr.u16 	%rs37, %rs36, 9;
	mul.lo.s16 	%rs38, %rs37, 576;
	sub.s16 	%rs39, %rs35, %rs38;
	cvt.u32.u16 	%r419, %rs39;
	mul.wide.u32 	%rd19, %r419, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r3452, [%rd20];
LBB0_12:                                // %L440
	xor.b32  	%r343, %r347, -2147483648;
	mov.b32 	%r380, %f194;
	mov.b32 	%r381, %f196;
	mov.b32 	%r383, %f195;
	mov.b32 	%r384, %f197;
	xor.b32  	%r407, %r410, -2147483648;
	xor.b32  	%r408, %r411, -2147483648;
	and.b32  	%r20, %r3, 24;
	setp.ne.s32 	%p46, %r20, 24;
	cvt.u16.u32 	%rs296, %r6;
	@%p46 bra 	LBB0_14;
// %bb.13:                              // %L440.L785_crit_edge
	cvt.u16.u32 	%rs52, %r1;
	and.b16  	%rs53, %rs52, 255;
	mul.lo.s16 	%rs54, %rs53, 171;
	shr.u16 	%rs55, %rs54, 10;
	mul.lo.s16 	%rs56, %rs55, -6;
	add.s16 	%rs57, %rs56, %rs52;
	cvt.u32.u16 	%r437, %rs57;
	and.b32  	%r3456, %r437, 255;
	mul.lo.s16 	%rs59, %rs296, 171;
	shr.u16 	%rs60, %rs59, 10;
	mul.lo.s16 	%rs61, %rs60, 6;
	sub.s16 	%rs62, %rs296, %rs61;
	cvt.u32.u16 	%r438, %rs62;
	and.b32  	%r3455, %r438, 255;
	cvt.u32.u16 	%r3454, %rs55;
	mul.wide.u16 	%r3453, %rs55, 6;
	mov.u32 	%r3457, 0;
	mov.u32 	%r3458, %r3457;
	bra.uni 	LBB0_15;
LBB0_14:                                // %L469
	ld.param.u64 	%rd2, [_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_2];
	mul.lo.s32 	%r420, %r5, 144;
	cvt.u16.u32 	%rs40, %r1;
	and.b16  	%rs41, %rs40, 255;
	mul.lo.s16 	%rs42, %rs41, 171;
	shr.u16 	%rs43, %rs42, 10;
	mul.lo.s16 	%rs44, %rs43, -6;
	add.s16 	%rs45, %rs44, %rs40;
	cvt.u32.u16 	%r422, %rs45;
	and.b32  	%r3456, %r422, 255;
	or.b32  	%r423, %r420, %r3456;
	mul.lo.s32 	%r424, %r4, 576;
	add.s32 	%r425, %r423, %r424;
	cvt.u32.u16 	%r3454, %rs43;
	mul.wide.u16 	%r3453, %rs43, 6;
	add.s32 	%r426, %r425, %r3453;
	mul.lo.s16 	%rs47, %rs296, 171;
	shr.u16 	%rs48, %rs47, 10;
	mul.lo.s16 	%rs49, %rs48, 6;
	sub.s16 	%rs50, %rs296, %rs49;
	cvt.u32.u16 	%r427, %rs50;
	and.b32  	%r3455, %r427, 255;
	and.b16  	%rs51, %rs50, 255;
	mul.wide.u16 	%r428, %rs51, 24;
	add.s32 	%r429, %r426, %r428;
	mul.wide.u32 	%rd21, %r429, 4;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.u32 	%r3457, [%rd22];
	add.s32 	%r430, %r420, 48384;
	or.b32  	%r431, %r430, %r3456;
	add.s32 	%r432, %r431, %r424;
	add.s32 	%r433, %r432, %r3453;
	add.s32 	%r434, %r433, %r428;
	mul.wide.u32 	%rd23, %r434, 4;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.u32 	%r3458, [%rd24];
LBB0_15:                                // %L785
	ld.param.u64 	%rd3, [_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_3];
	ld.param.u64 	%rd4, [_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_4];
	// begin inline asm
	cvt.rn.f16x2.f32 %r342, %r344, %r343;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r345, %r347, %r344;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r379, %r381, %r380;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r382, %r384, %r383;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r403, %r405, %r404;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r406, %r408, %r407;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r409, %r411, %r410;
	// end inline asm
	// begin inline asm
	cvt.rn.f16x2.f32 %r412, %r405, %r404;
	// end inline asm
	mov.u32 	%r3451, 0;
	shl.b32 	%r440, %r3, 2;
	and.b32  	%r441, %r440, 60;
	shr.u32 	%r37, %r3, 4;
	mul.lo.s32 	%r442, %r37, 10752;
	or.b32  	%r38, %r442, %r441;
	shl.b32 	%r39, %r4, 7;
	or.b32  	%r443, %r440, %r442;
	or.b32  	%r40, %r443, 64;
	and.b32  	%r444, %r9, 30;
	or.b32  	%r445, %r37, %r444;
	mul.lo.s32 	%r41, %r445, 257;
	or.b32  	%r446, %r9, %r37;
	or.b32  	%r447, %r446, 32;
	mul.lo.s32 	%r42, %r447, 257;
	shr.u32 	%r448, %r1, 3;
	and.b32  	%r449, %r2, 224;
	mad.lo.s32 	%r43, %r448, 257, %r449;
	and.b16  	%rs64, %rs295, 255;
	mul.lo.s16 	%rs65, %rs64, 171;
	shr.u16 	%rs66, %rs65, 12;
	mul.lo.s16 	%rs67, %rs66, 24;
	sub.s16 	%rs68, %rs295, %rs67;
	cvt.u32.u16 	%r450, %rs68;
	and.b32  	%r44, %r450, 255;
	add.s32 	%r451, %r1, 24;
	shr.u32 	%r452, %r451, 3;
	mad.lo.s32 	%r45, %r452, 257, %r449;
	add.s32 	%r453, %r1, 48;
	shr.u32 	%r454, %r453, 3;
	mad.lo.s32 	%r46, %r454, 257, %r449;
	add.s32 	%r455, %r1, 72;
	shr.u32 	%r456, %r455, 3;
	mad.lo.s32 	%r47, %r456, 257, %r449;
	or.b32  	%r457, %r448, 12;
	mad.lo.s32 	%r48, %r457, 257, %r449;
	add.s32 	%r458, %r1, 120;
	shr.u32 	%r459, %r458, 3;
	mad.lo.s32 	%r49, %r459, 257, %r449;
	add.s32 	%r460, %r1, 144;
	shr.u32 	%r461, %r460, 3;
	mad.lo.s32 	%r50, %r461, 257, %r449;
	add.s32 	%r462, %r1, 168;
	shr.u32 	%r463, %r462, 3;
	mad.lo.s32 	%r51, %r463, 257, %r449;
	or.b32  	%r464, %r448, 24;
	mad.lo.s32 	%r52, %r464, 257, %r449;
	add.s32 	%r465, %r1, 216;
	shr.u32 	%r466, %r465, 3;
	mad.lo.s32 	%r53, %r466, 257, %r449;
	add.s32 	%r467, %r1, 240;
	shr.u32 	%r468, %r467, 3;
	mad.lo.s32 	%r54, %r468, 257, %r449;
	add.s32 	%r469, %r1, 264;
	shr.u32 	%r470, %r469, 3;
	mad.lo.s32 	%r55, %r470, 257, %r449;
	or.b32  	%r471, %r448, 36;
	mad.lo.s32 	%r56, %r471, 257, %r449;
	add.s32 	%r472, %r1, 312;
	shr.u32 	%r473, %r472, 3;
	mad.lo.s32 	%r57, %r473, 257, %r449;
	add.s32 	%r474, %r1, 336;
	shr.u32 	%r475, %r474, 3;
	mad.lo.s32 	%r58, %r475, 257, %r449;
	add.s32 	%r476, %r1, 360;
	shr.u32 	%r477, %r476, 3;
	mad.lo.s32 	%r59, %r477, 257, %r449;
	or.b32  	%r478, %r448, 48;
	mad.lo.s32 	%r60, %r478, 257, %r449;
	add.s32 	%r479, %r1, 408;
	shr.u32 	%r480, %r479, 3;
	mad.lo.s32 	%r61, %r480, 257, %r449;
	add.s32 	%r481, %r1, 432;
	shr.u32 	%r482, %r481, 3;
	mad.lo.s32 	%r62, %r482, 257, %r449;
	add.s32 	%r483, %r1, 456;
	shr.u32 	%r484, %r483, 3;
	mad.lo.s32 	%r63, %r484, 257, %r449;
	or.b32  	%r485, %r448, 60;
	mad.lo.s32 	%r64, %r485, 257, %r449;
	add.s32 	%r486, %r1, 504;
	bfe.u32 	%r487, %r486, 3, 6;
	mad.lo.s32 	%r65, %r487, 257, %r449;
	mul.lo.s32 	%r66, %r3456, 33;
	mul.lo.s32 	%r67, %r5, 4806;
	mul.lo.s32 	%r68, %r3455, 801;
	mul.lo.s32 	%r69, %r3454, 198;
	and.b32  	%r488, %r3, 1;
	mul.lo.s32 	%r489, %r488, 4112;
	bfe.u32 	%r490, %r3, 1, 1;
	mul.lo.s32 	%r491, %r490, 2056;
	add.s32 	%r492, %r491, %r489;
	add.s32 	%r493, %r492, %r3456;
	and.b32  	%r494, %r6, 1;
	mul.lo.s32 	%r495, %r494, 1028;
	add.s32 	%r496, %r493, %r495;
	mul.lo.s32 	%r497, %r37, 257;
	add.s32 	%r498, %r496, %r497;
	bfe.u32 	%r499, %r3, 3, 1;
	mul.lo.s32 	%r500, %r499, 514;
	add.s32 	%r501, %r498, %r500;
	add.s32 	%r70, %r501, %r3453;
	mul.wide.u32 	%rd25, %r70, 4;
	mov.u64 	%rd26, shmem;
	add.s64 	%rd6, %rd26, %rd25;
	cvt.u64.u32 	%rd27, %r3453;
	cvt.u64.u32 	%rd28, %r500;
	cvt.u64.u32 	%rd29, %r497;
	cvt.u64.u32 	%rd30, %r495;
	cvt.u64.u32 	%rd31, %r3456;
	cvt.u64.u32 	%rd32, %r491;
	cvt.u64.u32 	%rd33, %r489;
	add.s64 	%rd34, %rd33, %rd32;
	add.s64 	%rd35, %rd34, %rd31;
	add.s64 	%rd36, %rd35, %rd30;
	add.s64 	%rd37, %rd36, %rd29;
	add.s64 	%rd38, %rd37, %rd28;
	add.s64 	%rd39, %rd38, %rd27;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd7, %rd26, %rd40;
	and.b32  	%r502, %r1, 1;
	mul.lo.s32 	%r503, %r502, 4112;
	bfe.u32 	%r504, %r1, 1, 1;
	mul.lo.s32 	%r505, %r504, 2056;
	add.s32 	%r506, %r505, %r503;
	add.s32 	%r507, %r506, %r3455;
	bfe.u32 	%r508, %r1, 2, 1;
	mul.lo.s32 	%r509, %r508, 1028;
	add.s32 	%r510, %r507, %r509;
	shr.u32 	%r511, %r1, 4;
	mul.lo.s32 	%r512, %r511, 257;
	add.s32 	%r513, %r510, %r512;
	bfe.u32 	%r514, %r1, 3, 1;
	mul.lo.s32 	%r515, %r514, 514;
	add.s32 	%r516, %r513, %r515;
	mul.lo.s32 	%r517, %r5, 6;
	add.s32 	%r71, %r516, %r517;
	mul.wide.u32 	%rd41, %r71, 4;
	add.s64 	%rd8, %rd26, %rd41;
	cvt.u64.u32 	%rd42, %r517;
	cvt.u64.u32 	%rd43, %r515;
	cvt.u64.u32 	%rd44, %r512;
	cvt.u64.u32 	%rd45, %r509;
	cvt.u64.u32 	%rd46, %r3455;
	cvt.u64.u32 	%rd47, %r505;
	cvt.u64.u32 	%rd48, %r503;
	add.s64 	%rd49, %rd48, %rd47;
	add.s64 	%rd50, %rd49, %rd46;
	add.s64 	%rd51, %rd50, %rd45;
	add.s64 	%rd52, %rd51, %rd44;
	add.s64 	%rd53, %rd52, %rd43;
	add.s64 	%rd54, %rd53, %rd42;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd9, %rd26, %rd55;
	cvt.u16.u32 	%rs69, %r1;
	shl.b16 	%rs70, %rs69, 1;
	and.b16  	%rs71, %rs70, 254;
	mul.lo.s16 	%rs72, %rs71, 171;
	shr.u16 	%rs73, %rs72, 13;
	mul.lo.s16 	%rs74, %rs73, 48;
	sub.s16 	%rs75, %rs70, %rs74;
	and.b16  	%rs76, %rs75, 254;
	mul.wide.u16 	%r72, %rs76, 24;
	mul.lo.s32 	%r73, %r4, 1152;
	or.b16  	%rs77, %rs70, 1;
	and.b16  	%rs78, %rs77, 255;
	mul.lo.s16 	%rs79, %rs78, 171;
	shr.u16 	%rs80, %rs79, 13;
	mul.lo.s16 	%rs81, %rs80, 48;
	sub.s16 	%rs82, %rs77, %rs81;
	and.b16  	%rs83, %rs82, 255;
	mul.wide.u16 	%r74, %rs83, 24;
	cvt.u64.u32 	%rd56, %r69;
	cvt.u64.u32 	%rd57, %r68;
	cvt.u64.u32 	%rd58, %r67;
	cvt.u64.u32 	%rd59, %r66;
	add.s64 	%rd60, %rd59, %rd58;
	add.s64 	%rd61, %rd60, %rd57;
	add.s64 	%rd62, %rd61, %rd56;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd10, %rd26, %rd63;
	setp.gt.u32 	%p47, %r1, 7;
	setp.eq.s32 	%p48, %r37, 1;
	setp.lt.u32 	%p50, %r3, 24;
	mov.u32 	%r3459, %r3451;
	mov.u32 	%r3492, %r3451;
	mov.u32 	%r3493, %r3451;
	mov.u32 	%r3494, %r3451;
	mov.u32 	%r3495, %r3451;
	mov.u32 	%r3496, %r3451;
	mov.u32 	%r3497, %r3451;
	bra.uni 	LBB0_16;
LBB0_59:                                // %guard_pass16241
                                        //   in Loop: Header=BB0_16 Depth=1
	add.s32 	%r340, %r3459, 48;
	setp.ne.s32 	%p81, %r3459, 2016;
	mov.u32 	%r3459, %r340;
	@%p81 bra 	LBB0_16;
	bra.uni 	LBB0_60;
LBB0_16:                                // %L788
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_18 Depth 2
	mul.hi.u32 	%r711, %r3459, -1431655765;
	shr.u32 	%r712, %r711, 5;
	mul.lo.s32 	%r82, %r712, 48;
	add.s32 	%r713, %r1, %r82;
	cvt.u16.u32 	%rs84, %r713;
	mul.hi.s16 	%rs85, %rs84, 16257;
	shr.u16 	%rs86, %rs85, 15;
	shr.s16 	%rs87, %rs85, 9;
	add.s16 	%rs88, %rs87, %rs86;
	mul.lo.s16 	%rs89, %rs88, 2064;
	sub.s16 	%rs90, %rs84, %rs89;
	mul.wide.s16 	%r714, %rs90, 21504;
	add.s32 	%r715, %r38, %r714;
	add.s32 	%r716, %r715, %r39;
	mul.wide.s32 	%rd64, %r716, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.global.v4.u32 	{%r717, %r718, %r719, %r720}, [%rd65];
	add.s32 	%r721, %r714, %r40;
	add.s32 	%r722, %r721, %r39;
	mul.wide.s32 	%rd66, %r722, 4;
	add.s64 	%rd67, %rd3, %rd66;
	ld.global.v4.u32 	{%r723, %r724, %r725, %r726}, [%rd67];
	add.s16 	%rs91, %rs84, 24;
	mul.hi.s16 	%rs92, %rs91, 16257;
	shr.u16 	%rs93, %rs92, 15;
	shr.s16 	%rs94, %rs92, 9;
	add.s16 	%rs95, %rs94, %rs93;
	mul.lo.s16 	%rs96, %rs95, 2064;
	sub.s16 	%rs97, %rs91, %rs96;
	mul.wide.s16 	%r727, %rs97, 21504;
	add.s32 	%r728, %r38, %r727;
	add.s32 	%r729, %r728, %r39;
	mul.wide.s32 	%rd68, %r729, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.global.v4.u32 	{%r730, %r731, %r732, %r733}, [%rd69];
	add.s32 	%r734, %r727, %r40;
	add.s32 	%r735, %r734, %r39;
	mul.wide.s32 	%rd70, %r735, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.global.v4.u32 	{%r736, %r737, %r738, %r739}, [%rd71];
	selp.b32 	%r740, %r717, %r719, %p48;
	shfl.sync.bfly.b32	%r741, %r740, 16, 31, -1;
	selp.b32 	%r520, %r741, %r717, %p48;
	selp.b32 	%r525, %r719, %r741, %p48;
	selp.b32 	%r742, %r718, %r720, %p48;
	shfl.sync.bfly.b32	%r743, %r742, 16, 31, -1;
	selp.b32 	%r528, %r743, %r718, %p48;
	selp.b32 	%r533, %r720, %r743, %p48;
	selp.b32 	%r744, %r723, %r725, %p48;
	shfl.sync.bfly.b32	%r745, %r744, 16, 31, -1;
	selp.b32 	%r536, %r745, %r723, %p48;
	selp.b32 	%r541, %r725, %r745, %p48;
	selp.b32 	%r746, %r724, %r726, %p48;
	shfl.sync.bfly.b32	%r747, %r746, 16, 31, -1;
	selp.b32 	%r544, %r747, %r724, %p48;
	selp.b32 	%r549, %r726, %r747, %p48;
	selp.b32 	%r748, %r730, %r732, %p48;
	shfl.sync.bfly.b32	%r749, %r748, 16, 31, -1;
	selp.b32 	%r552, %r749, %r730, %p48;
	selp.b32 	%r557, %r732, %r749, %p48;
	selp.b32 	%r750, %r731, %r733, %p48;
	shfl.sync.bfly.b32	%r751, %r750, 16, 31, -1;
	selp.b32 	%r560, %r751, %r731, %p48;
	selp.b32 	%r565, %r733, %r751, %p48;
	selp.b32 	%r752, %r736, %r738, %p48;
	shfl.sync.bfly.b32	%r753, %r752, 16, 31, -1;
	selp.b32 	%r568, %r753, %r736, %p48;
	selp.b32 	%r573, %r738, %r753, %p48;
	selp.b32 	%r754, %r737, %r739, %p48;
	shfl.sync.bfly.b32	%r755, %r754, 16, 31, -1;
	selp.b32 	%r576, %r755, %r737, %p48;
	selp.b32 	%r581, %r739, %r755, %p48;
	shl.b32 	%r521, %r525, 4;
	mov.u32 	%r519, 252645135;
	// begin inline asm
	lop3.b32 %r583, %r519, %r520, %r521, 202;
	// end inline asm
	shr.u32 	%r524, %r520, 4;
	// begin inline asm
	lop3.b32 %r599, %r519, %r524, %r525, 202;
	// end inline asm
	shl.b32 	%r529, %r533, 4;
	// begin inline asm
	lop3.b32 %r591, %r519, %r528, %r529, 202;
	// end inline asm
	shr.u32 	%r532, %r528, 4;
	// begin inline asm
	lop3.b32 %r607, %r519, %r532, %r533, 202;
	// end inline asm
	shl.b32 	%r537, %r541, 4;
	// begin inline asm
	lop3.b32 %r615, %r519, %r536, %r537, 202;
	// end inline asm
	shr.u32 	%r540, %r536, 4;
	// begin inline asm
	lop3.b32 %r631, %r519, %r540, %r541, 202;
	// end inline asm
	shl.b32 	%r545, %r549, 4;
	// begin inline asm
	lop3.b32 %r623, %r519, %r544, %r545, 202;
	// end inline asm
	shr.u32 	%r548, %r544, 4;
	// begin inline asm
	lop3.b32 %r639, %r519, %r548, %r549, 202;
	// end inline asm
	shl.b32 	%r553, %r557, 4;
	// begin inline asm
	lop3.b32 %r584, %r519, %r552, %r553, 202;
	// end inline asm
	shr.u32 	%r556, %r552, 4;
	// begin inline asm
	lop3.b32 %r600, %r519, %r556, %r557, 202;
	// end inline asm
	shl.b32 	%r561, %r565, 4;
	// begin inline asm
	lop3.b32 %r592, %r519, %r560, %r561, 202;
	// end inline asm
	shr.u32 	%r564, %r560, 4;
	// begin inline asm
	lop3.b32 %r608, %r519, %r564, %r565, 202;
	// end inline asm
	shl.b32 	%r569, %r573, 4;
	// begin inline asm
	lop3.b32 %r616, %r519, %r568, %r569, 202;
	// end inline asm
	shr.u32 	%r572, %r568, 4;
	// begin inline asm
	lop3.b32 %r632, %r519, %r572, %r573, 202;
	// end inline asm
	shl.b32 	%r577, %r581, 4;
	// begin inline asm
	lop3.b32 %r624, %r519, %r576, %r577, 202;
	// end inline asm
	shr.u32 	%r580, %r576, 4;
	// begin inline asm
	lop3.b32 %r640, %r519, %r580, %r581, 202;
	// end inline asm
	mov.u32 	%r585, 25152;
	// begin inline asm
	prmt.b32 %r647, %r583, %r584, %r585;
	// end inline asm
	mov.u32 	%r589, 29521;
	// begin inline asm
	prmt.b32 %r679, %r583, %r584, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r655, %r591, %r592, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r687, %r591, %r592, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r648, %r599, %r600, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r680, %r599, %r600, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r656, %r607, %r608, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r688, %r607, %r608, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r663, %r615, %r616, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r695, %r615, %r616, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r671, %r623, %r624, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r703, %r623, %r624, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r664, %r631, %r632, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r696, %r631, %r632, %r589;
	// end inline asm
	// begin inline asm
	prmt.b32 %r672, %r639, %r640, %r585;
	// end inline asm
	// begin inline asm
	prmt.b32 %r704, %r639, %r640, %r589;
	// end inline asm
	mov.u32 	%r705, 21520;
	// begin inline asm
	prmt.b32 %r646, %r647, %r648, %r705;
	// end inline asm
	mov.u32 	%r709, 30258;
	// begin inline asm
	prmt.b32 %r650, %r647, %r648, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r654, %r655, %r656, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r658, %r655, %r656, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r662, %r663, %r664, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r666, %r663, %r664, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r670, %r671, %r672, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r674, %r671, %r672, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r678, %r679, %r680, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r682, %r679, %r680, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r686, %r687, %r688, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r690, %r687, %r688, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r694, %r695, %r696, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r698, %r695, %r696, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r702, %r703, %r704, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r706, %r703, %r704, %r709;
	// end inline asm
	mul.hi.s16 	%rs98, %rs84, 10923;
	shr.u16 	%rs99, %rs98, 15;
	shr.s16 	%rs100, %rs98, 2;
	add.s16 	%rs101, %rs100, %rs99;
	mul.lo.s16 	%rs102, %rs101, 24;
	sub.s16 	%rs103, %rs84, %rs102;
	cvt.s32.s16 	%r756, %rs103;
	add.s32 	%r757, %r41, %r756;
	mul.wide.s32 	%rd72, %r757, 4;
	add.s64 	%rd74, %rd26, %rd72;
	st.shared.u32 	[%rd74], %r646;
	st.shared.u32 	[%rd74+512], %r654;
	st.shared.u32 	[%rd74+256], %r650;
	st.shared.u32 	[%rd74+768], %r658;
	add.s32 	%r758, %r42, %r756;
	mul.wide.s32 	%rd75, %r758, 4;
	add.s64 	%rd76, %rd26, %rd75;
	st.shared.u32 	[%rd76], %r662;
	st.shared.u32 	[%rd76+512], %r670;
	st.shared.u32 	[%rd76+256], %r666;
	st.shared.u32 	[%rd76+768], %r674;
	st.shared.u32 	[%rd74+128], %r678;
	st.shared.u32 	[%rd74+640], %r686;
	st.shared.u32 	[%rd74+384], %r682;
	st.shared.u32 	[%rd74+896], %r690;
	st.shared.u32 	[%rd76+128], %r694;
	st.shared.u32 	[%rd76+640], %r702;
	st.shared.u32 	[%rd76+384], %r698;
	st.shared.u32 	[%rd76+896], %r706;
	bar.sync 	0;
	add.s32 	%r759, %r82, %r44;
	cvt.u16.u32 	%rs104, %r759;
	mul.hi.s16 	%rs105, %rs104, 10923;
	shr.u16 	%rs106, %rs105, 15;
	shr.s16 	%rs107, %rs105, 2;
	add.s16 	%rs108, %rs107, %rs106;
	mul.lo.s16 	%rs109, %rs108, 24;
	sub.s16 	%rs110, %rs104, %rs109;
	cvt.s32.s16 	%r760, %rs110;
	add.s32 	%r761, %r43, %r760;
	mul.wide.s32 	%rd77, %r761, 4;
	add.s64 	%rd78, %rd26, %rd77;
	ld.shared.u32 	%r762, [%rd78];
	add.s32 	%r763, %r45, %r760;
	mul.wide.u32 	%rd79, %r763, 4;
	add.s64 	%rd80, %rd26, %rd79;
	ld.shared.u32 	%r764, [%rd80];
	add.s32 	%r765, %r46, %r760;
	mul.wide.u32 	%rd81, %r765, 4;
	add.s64 	%rd82, %rd26, %rd81;
	ld.shared.u32 	%r766, [%rd82];
	add.s32 	%r767, %r47, %r760;
	mul.wide.u32 	%rd83, %r767, 4;
	add.s64 	%rd84, %rd26, %rd83;
	ld.shared.u32 	%r768, [%rd84];
	add.s32 	%r769, %r48, %r760;
	mul.wide.s32 	%rd85, %r769, 4;
	add.s64 	%rd86, %rd26, %rd85;
	ld.shared.u32 	%r770, [%rd86];
	add.s32 	%r771, %r49, %r760;
	mul.wide.u32 	%rd87, %r771, 4;
	add.s64 	%rd88, %rd26, %rd87;
	ld.shared.u32 	%r772, [%rd88];
	add.s32 	%r773, %r50, %r760;
	mul.wide.u32 	%rd89, %r773, 4;
	add.s64 	%rd90, %rd26, %rd89;
	ld.shared.u32 	%r774, [%rd90];
	add.s32 	%r775, %r51, %r760;
	mul.wide.u32 	%rd91, %r775, 4;
	add.s64 	%rd92, %rd26, %rd91;
	ld.shared.u32 	%r776, [%rd92];
	add.s32 	%r777, %r52, %r760;
	mul.wide.s32 	%rd93, %r777, 4;
	add.s64 	%rd94, %rd26, %rd93;
	ld.shared.u32 	%r778, [%rd94];
	add.s32 	%r779, %r53, %r760;
	mul.wide.u32 	%rd95, %r779, 4;
	add.s64 	%rd96, %rd26, %rd95;
	ld.shared.u32 	%r780, [%rd96];
	add.s32 	%r781, %r54, %r760;
	mul.wide.u32 	%rd97, %r781, 4;
	add.s64 	%rd98, %rd26, %rd97;
	ld.shared.u32 	%r782, [%rd98];
	add.s32 	%r783, %r55, %r760;
	mul.wide.u32 	%rd99, %r783, 4;
	add.s64 	%rd100, %rd26, %rd99;
	ld.shared.u32 	%r784, [%rd100];
	add.s32 	%r785, %r56, %r760;
	mul.wide.s32 	%rd101, %r785, 4;
	add.s64 	%rd102, %rd26, %rd101;
	ld.shared.u32 	%r786, [%rd102];
	add.s32 	%r787, %r57, %r760;
	mul.wide.u32 	%rd103, %r787, 4;
	add.s64 	%rd104, %rd26, %rd103;
	ld.shared.u32 	%r788, [%rd104];
	add.s32 	%r789, %r58, %r760;
	mul.wide.u32 	%rd105, %r789, 4;
	add.s64 	%rd106, %rd26, %rd105;
	ld.shared.u32 	%r790, [%rd106];
	add.s32 	%r791, %r59, %r760;
	mul.wide.u32 	%rd107, %r791, 4;
	add.s64 	%rd108, %rd26, %rd107;
	ld.shared.u32 	%r792, [%rd108];
	add.s32 	%r793, %r60, %r760;
	mul.wide.s32 	%rd109, %r793, 4;
	add.s64 	%rd110, %rd26, %rd109;
	ld.shared.u32 	%r794, [%rd110];
	add.s32 	%r795, %r61, %r760;
	mul.wide.u32 	%rd111, %r795, 4;
	add.s64 	%rd112, %rd26, %rd111;
	ld.shared.u32 	%r796, [%rd112];
	add.s32 	%r797, %r62, %r760;
	mul.wide.u32 	%rd113, %r797, 4;
	add.s64 	%rd114, %rd26, %rd113;
	ld.shared.u32 	%r798, [%rd114];
	add.s32 	%r799, %r63, %r760;
	mul.wide.u32 	%rd115, %r799, 4;
	add.s64 	%rd116, %rd26, %rd115;
	ld.shared.u32 	%r800, [%rd116];
	add.s32 	%r801, %r64, %r760;
	mul.wide.s32 	%rd117, %r801, 4;
	add.s64 	%rd118, %rd26, %rd117;
	ld.shared.u32 	%r802, [%rd118];
	add.s32 	%r803, %r65, %r760;
	mul.wide.s32 	%rd119, %r803, 4;
	add.s64 	%rd120, %rd26, %rd119;
	ld.shared.u32 	%r804, [%rd120];
	bar.sync 	0;
	shfl.sync.idx.b32	%r805, %r3452, 0, 31, -1;
	shfl.sync.idx.b32	%r806, %r3452, 1, 31, -1;
	shfl.sync.idx.b32	%r807, %r3452, 2, 31, -1;
	shfl.sync.idx.b32	%r808, %r3452, 3, 31, -1;
	shfl.sync.idx.b32	%r809, %r3452, 4, 31, -1;
	shfl.sync.idx.b32	%r810, %r3452, 5, 31, -1;
	shfl.sync.idx.b32	%r811, %r3452, 6, 31, -1;
	shfl.sync.idx.b32	%r812, %r3452, 7, 31, -1;
	shfl.sync.idx.b32	%r813, %r3452, 8, 31, -1;
	shfl.sync.idx.b32	%r814, %r3452, 9, 31, -1;
	shfl.sync.idx.b32	%r815, %r3452, 10, 31, -1;
	shfl.sync.idx.b32	%r816, %r3452, 11, 31, -1;
	shfl.sync.idx.b32	%r817, %r3452, 12, 31, -1;
	shfl.sync.idx.b32	%r818, %r3452, 13, 31, -1;
	shfl.sync.idx.b32	%r819, %r3452, 14, 31, -1;
	shfl.sync.idx.b32	%r820, %r3452, 15, 31, -1;
	shfl.sync.idx.b32	%r821, %r3452, 16, 31, -1;
	shfl.sync.idx.b32	%r822, %r3452, 17, 31, -1;
	shfl.sync.idx.b32	%r823, %r3452, 18, 31, -1;
	shfl.sync.idx.b32	%r824, %r3452, 19, 31, -1;
	shfl.sync.idx.b32	%r825, %r3452, 20, 31, -1;
	shfl.sync.idx.b32	%r826, %r3452, 21, 31, -1;
	shfl.sync.idx.b32	%r827, %r3452, 22, 31, -1;
	shfl.sync.idx.b32	%r828, %r3452, 23, 31, -1;
	add.s32 	%r829, %r805, %r760;
	mul.wide.s32 	%rd121, %r829, 4;
	add.s64 	%rd122, %rd26, %rd121;
	st.shared.u32 	[%rd122], %r762;
	add.s32 	%r830, %r806, %r760;
	mul.wide.s32 	%rd123, %r830, 4;
	add.s64 	%rd124, %rd26, %rd123;
	st.shared.u32 	[%rd124], %r764;
	add.s32 	%r831, %r807, %r760;
	mul.wide.s32 	%rd125, %r831, 4;
	add.s64 	%rd126, %rd26, %rd125;
	st.shared.u32 	[%rd126], %r766;
	add.s32 	%r832, %r808, %r760;
	mul.wide.s32 	%rd127, %r832, 4;
	add.s64 	%rd128, %rd26, %rd127;
	st.shared.u32 	[%rd128], %r768;
	add.s32 	%r833, %r809, %r760;
	mul.wide.s32 	%rd129, %r833, 4;
	add.s64 	%rd130, %rd26, %rd129;
	st.shared.u32 	[%rd130], %r770;
	add.s32 	%r834, %r810, %r760;
	mul.wide.s32 	%rd131, %r834, 4;
	add.s64 	%rd132, %rd26, %rd131;
	st.shared.u32 	[%rd132], %r772;
	add.s32 	%r835, %r811, %r760;
	mul.wide.s32 	%rd133, %r835, 4;
	add.s64 	%rd134, %rd26, %rd133;
	st.shared.u32 	[%rd134], %r774;
	add.s32 	%r836, %r812, %r760;
	mul.wide.s32 	%rd135, %r836, 4;
	add.s64 	%rd136, %rd26, %rd135;
	st.shared.u32 	[%rd136], %r776;
	add.s32 	%r837, %r813, %r760;
	mul.wide.s32 	%rd137, %r837, 4;
	add.s64 	%rd138, %rd26, %rd137;
	st.shared.u32 	[%rd138], %r778;
	add.s32 	%r838, %r814, %r760;
	mul.wide.s32 	%rd139, %r838, 4;
	add.s64 	%rd140, %rd26, %rd139;
	st.shared.u32 	[%rd140], %r780;
	add.s32 	%r839, %r815, %r760;
	mul.wide.s32 	%rd141, %r839, 4;
	add.s64 	%rd142, %rd26, %rd141;
	st.shared.u32 	[%rd142], %r782;
	add.s32 	%r840, %r816, %r760;
	mul.wide.s32 	%rd143, %r840, 4;
	add.s64 	%rd144, %rd26, %rd143;
	st.shared.u32 	[%rd144], %r784;
	add.s32 	%r841, %r817, %r760;
	mul.wide.s32 	%rd145, %r841, 4;
	add.s64 	%rd146, %rd26, %rd145;
	st.shared.u32 	[%rd146], %r786;
	add.s32 	%r842, %r818, %r760;
	mul.wide.s32 	%rd147, %r842, 4;
	add.s64 	%rd148, %rd26, %rd147;
	st.shared.u32 	[%rd148], %r788;
	add.s32 	%r843, %r819, %r760;
	mul.wide.s32 	%rd149, %r843, 4;
	add.s64 	%rd150, %rd26, %rd149;
	st.shared.u32 	[%rd150], %r790;
	add.s32 	%r844, %r820, %r760;
	mul.wide.s32 	%rd151, %r844, 4;
	add.s64 	%rd152, %rd26, %rd151;
	st.shared.u32 	[%rd152], %r792;
	add.s32 	%r845, %r821, %r760;
	mul.wide.s32 	%rd153, %r845, 4;
	add.s64 	%rd154, %rd26, %rd153;
	st.shared.u32 	[%rd154], %r794;
	add.s32 	%r846, %r822, %r760;
	mul.wide.s32 	%rd155, %r846, 4;
	add.s64 	%rd156, %rd26, %rd155;
	st.shared.u32 	[%rd156], %r796;
	add.s32 	%r847, %r823, %r760;
	mul.wide.s32 	%rd157, %r847, 4;
	add.s64 	%rd158, %rd26, %rd157;
	st.shared.u32 	[%rd158], %r798;
	add.s32 	%r848, %r824, %r760;
	mul.wide.s32 	%rd159, %r848, 4;
	add.s64 	%rd160, %rd26, %rd159;
	st.shared.u32 	[%rd160], %r800;
	add.s32 	%r849, %r825, %r760;
	mul.wide.s32 	%rd161, %r849, 4;
	add.s64 	%rd162, %rd26, %rd161;
	st.shared.u32 	[%rd162], %r802;
	selp.b32 	%r850, 0, %r804, %p47;
	add.s32 	%r851, %r826, %r760;
	mul.wide.s32 	%rd163, %r851, 4;
	add.s64 	%rd164, %rd26, %rd163;
	st.shared.u32 	[%rd164], %r850;
	add.s32 	%r852, %r827, %r760;
	mul.wide.s32 	%rd165, %r852, 4;
	add.s64 	%rd166, %rd26, %rd165;
	st.shared.u32 	[%rd166], %r3451;
	add.s32 	%r853, %r828, %r760;
	mul.wide.s32 	%rd167, %r853, 4;
	add.s64 	%rd168, %rd26, %rd167;
	st.shared.u32 	[%rd168], %r3451;
	bar.sync 	0;
	mov.u32 	%r3466, %r3451;
	mov.u32 	%r3467, %r3451;
	mov.u32 	%r3468, %r3451;
	mov.u32 	%r3469, %r3451;
	mov.u32 	%r3470, %r3451;
	mov.u32 	%r3471, %r3451;
	mov.u32 	%r3472, %r3451;
	mov.u32 	%r3473, %r3451;
	mov.u32 	%r3474, %r3451;
	mov.u32 	%r3475, %r3451;
	mov.u32 	%r3476, %r3451;
	mov.u32 	%r3477, %r3451;
	mov.u32 	%r3478, %r3451;
	mov.u32 	%r3479, %r3451;
	mov.u32 	%r3480, %r3451;
	mov.u32 	%r3481, %r3451;
	mov.u32 	%r3482, %r3451;
	mov.u32 	%r3483, %r3451;
	mov.u32 	%r3484, %r3451;
	mov.u32 	%r3485, %r3451;
	mov.u32 	%r3486, %r3451;
	mov.u32 	%r3487, %r3451;
	mov.u32 	%r3488, %r3451;
	mov.u32 	%r3489, %r3451;
	@%p46 bra 	LBB0_63;
	bra.uni 	LBB0_17;
LBB0_63:                                // %oksrem3271
                                        //   in Loop: Header=BB0_16 Depth=1
	cvt.u16.u32 	%rs111, %r82;
	mul.hi.s16 	%rs112, %rs111, 10923;
	shr.u16 	%rs113, %rs112, 15;
	shr.s16 	%rs114, %rs112, 2;
	add.s16 	%rs115, %rs114, %rs113;
	mul.lo.s16 	%rs116, %rs115, 24;
	sub.s16 	%rs117, %rs111, %rs116;
	cvt.s32.s16 	%r854, %rs117;
	add.s32 	%r855, %r66, %r854;
	add.s32 	%r856, %r855, %r67;
	add.s32 	%r857, %r856, %r68;
	add.s32 	%r858, %r857, %r69;
	mul.wide.s32 	%rd169, %r858, 4;
	add.s64 	%rd171, %rd26, %rd169;
	ld.shared.u32 	%r3466, [%rd171];
	ld.shared.u32 	%r3467, [%rd10+4];
	ld.shared.u32 	%r3468, [%rd10+8];
	ld.shared.u32 	%r3469, [%rd10+12];
	ld.shared.u32 	%r3470, [%rd10+16];
	ld.shared.u32 	%r3471, [%rd10+20];
	ld.shared.u32 	%r3472, [%rd10+24];
	ld.shared.u32 	%r3473, [%rd10+28];
	ld.shared.u32 	%r3474, [%rd10+32];
	ld.shared.u32 	%r3475, [%rd10+36];
	ld.shared.u32 	%r3476, [%rd10+40];
	ld.shared.u32 	%r3477, [%rd10+44];
	ld.shared.u32 	%r3478, [%rd10+48];
	ld.shared.u32 	%r3479, [%rd10+52];
	ld.shared.u32 	%r3480, [%rd10+56];
	ld.shared.u32 	%r3481, [%rd10+60];
	add.s16 	%rs118, %rs111, 16;
	mul.hi.s16 	%rs119, %rs118, 10923;
	shr.u16 	%rs120, %rs119, 15;
	shr.s16 	%rs121, %rs119, 2;
	add.s16 	%rs122, %rs121, %rs120;
	mul.lo.s16 	%rs123, %rs122, 24;
	sub.s16 	%rs124, %rs118, %rs123;
	cvt.s32.s16 	%r859, %rs124;
	add.s32 	%r860, %r66, %r859;
	add.s32 	%r861, %r860, %r67;
	add.s32 	%r862, %r861, %r68;
	add.s32 	%r863, %r862, %r69;
	mul.wide.s32 	%rd172, %r863, 4;
	add.s64 	%rd173, %rd26, %rd172;
	ld.shared.u32 	%r3482, [%rd173];
	add.s16 	%rs125, %rs111, 17;
	mul.hi.s16 	%rs126, %rs125, 10923;
	shr.u16 	%rs127, %rs126, 15;
	shr.s16 	%rs128, %rs126, 2;
	add.s16 	%rs129, %rs128, %rs127;
	mul.lo.s16 	%rs130, %rs129, 24;
	sub.s16 	%rs131, %rs125, %rs130;
	cvt.s32.s16 	%r864, %rs131;
	add.s32 	%r865, %r66, %r864;
	add.s32 	%r866, %r865, %r67;
	add.s32 	%r867, %r866, %r68;
	add.s32 	%r868, %r867, %r69;
	mul.wide.s32 	%rd174, %r868, 4;
	add.s64 	%rd175, %rd26, %rd174;
	ld.shared.u32 	%r3483, [%rd175];
	add.s16 	%rs132, %rs111, 18;
	mul.hi.s16 	%rs133, %rs132, 10923;
	shr.u16 	%rs134, %rs133, 15;
	shr.s16 	%rs135, %rs133, 2;
	add.s16 	%rs136, %rs135, %rs134;
	mul.lo.s16 	%rs137, %rs136, 24;
	sub.s16 	%rs138, %rs132, %rs137;
	cvt.s32.s16 	%r869, %rs138;
	add.s32 	%r870, %r66, %r869;
	add.s32 	%r871, %r870, %r67;
	add.s32 	%r872, %r871, %r68;
	add.s32 	%r873, %r872, %r69;
	mul.wide.s32 	%rd176, %r873, 4;
	add.s64 	%rd177, %rd26, %rd176;
	ld.shared.u32 	%r3484, [%rd177];
	add.s16 	%rs139, %rs111, 19;
	mul.hi.s16 	%rs140, %rs139, 10923;
	shr.u16 	%rs141, %rs140, 15;
	shr.s16 	%rs142, %rs140, 2;
	add.s16 	%rs143, %rs142, %rs141;
	mul.lo.s16 	%rs144, %rs143, 24;
	sub.s16 	%rs145, %rs139, %rs144;
	cvt.s32.s16 	%r874, %rs145;
	add.s32 	%r875, %r66, %r874;
	add.s32 	%r876, %r875, %r67;
	add.s32 	%r877, %r876, %r68;
	add.s32 	%r878, %r877, %r69;
	mul.wide.s32 	%rd178, %r878, 4;
	add.s64 	%rd179, %rd26, %rd178;
	ld.shared.u32 	%r3485, [%rd179];
	add.s16 	%rs146, %rs111, 20;
	mul.hi.s16 	%rs147, %rs146, 10923;
	shr.u16 	%rs148, %rs147, 15;
	shr.s16 	%rs149, %rs147, 2;
	add.s16 	%rs150, %rs149, %rs148;
	mul.lo.s16 	%rs151, %rs150, 24;
	sub.s16 	%rs152, %rs146, %rs151;
	cvt.s32.s16 	%r879, %rs152;
	add.s32 	%r880, %r66, %r879;
	add.s32 	%r881, %r880, %r67;
	add.s32 	%r882, %r881, %r68;
	add.s32 	%r883, %r882, %r69;
	mul.wide.s32 	%rd180, %r883, 4;
	add.s64 	%rd181, %rd26, %rd180;
	ld.shared.u32 	%r3486, [%rd181];
	add.s16 	%rs153, %rs111, 21;
	mul.hi.s16 	%rs154, %rs153, 10923;
	shr.u16 	%rs155, %rs154, 15;
	shr.s16 	%rs156, %rs154, 2;
	add.s16 	%rs157, %rs156, %rs155;
	mul.lo.s16 	%rs158, %rs157, 24;
	sub.s16 	%rs159, %rs153, %rs158;
	cvt.s32.s16 	%r884, %rs159;
	add.s32 	%r885, %r66, %r884;
	add.s32 	%r886, %r885, %r67;
	add.s32 	%r887, %r886, %r68;
	add.s32 	%r888, %r887, %r69;
	mul.wide.s32 	%rd182, %r888, 4;
	add.s64 	%rd183, %rd26, %rd182;
	ld.shared.u32 	%r3487, [%rd183];
	add.s16 	%rs160, %rs111, 22;
	mul.hi.s16 	%rs161, %rs160, 10923;
	shr.u16 	%rs162, %rs161, 15;
	shr.s16 	%rs163, %rs161, 2;
	add.s16 	%rs164, %rs163, %rs162;
	mul.lo.s16 	%rs165, %rs164, 24;
	sub.s16 	%rs166, %rs160, %rs165;
	cvt.s32.s16 	%r889, %rs166;
	add.s32 	%r890, %r66, %r889;
	add.s32 	%r891, %r890, %r67;
	add.s32 	%r892, %r891, %r68;
	add.s32 	%r893, %r892, %r69;
	mul.wide.s32 	%rd184, %r893, 4;
	add.s64 	%rd185, %rd26, %rd184;
	ld.shared.u32 	%r3488, [%rd185];
	add.s16 	%rs167, %rs111, 23;
	mul.hi.s16 	%rs168, %rs167, 10923;
	shr.u16 	%rs169, %rs168, 15;
	shr.s16 	%rs170, %rs168, 2;
	add.s16 	%rs171, %rs170, %rs169;
	mul.lo.s16 	%rs172, %rs171, 24;
	sub.s16 	%rs173, %rs167, %rs172;
	cvt.s32.s16 	%r894, %rs173;
	add.s32 	%r895, %r66, %r894;
	add.s32 	%r896, %r895, %r67;
	add.s32 	%r897, %r896, %r68;
	add.s32 	%r898, %r897, %r69;
	mul.wide.s32 	%rd186, %r898, 4;
	add.s64 	%rd187, %rd26, %rd186;
	ld.shared.u32 	%r3489, [%rd187];
LBB0_17:                                // %L12784
                                        //   in Loop: Header=BB0_16 Depth=1
	bar.sync 	0;
	add.s32 	%r107, %r82, 1;
	add.s32 	%r108, %r82, 2;
	add.s32 	%r109, %r82, 3;
	add.s32 	%r110, %r82, 25;
	add.s32 	%r111, %r82, 26;
	add.s32 	%r112, %r82, 27;
	mov.u32 	%r3490, 12;
	mov.u32 	%r3491, %r3451;
	bra.uni 	LBB0_18;
LBB0_57:                                // %L39358
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3494, 0;
	mov.u32 	%r3495, %r3494;
	mov.u32 	%r3496, %r3494;
	mov.u32 	%r3497, %r3494;
	mov.u32 	%r3492, %r3494;
LBB0_58:                                // %L39359
                                        //   in Loop: Header=BB0_18 Depth=2
	bar.sync 	0;
	add.s32 	%r3491, %r3491, 1;
	add.s32 	%r3490, %r3490, -4;
	setp.eq.s32 	%p80, %r3490, -12;
	@%p80 bra 	LBB0_59;
LBB0_18:                                // %L12810
                                        //   Parent Loop BB0_16 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	setp.eq.s32 	%p51, %r3490, 12;
	selp.b32 	%r1438, %r3466, 0, %p51;
	setp.eq.s32 	%p52, %r3490, 8;
	selp.b32 	%r1439, %r3470, %r1438, %p52;
	setp.eq.s32 	%p53, %r3490, 4;
	selp.b32 	%r1440, %r3474, %r1439, %p53;
	setp.eq.s32 	%p54, %r3490, 0;
	selp.b32 	%r1441, %r3478, %r1440, %p54;
	setp.eq.s32 	%p55, %r3490, -4;
	selp.b32 	%r1442, %r3482, %r1441, %p55;
	setp.eq.s32 	%p56, %r3490, -8;
	selp.b32 	%r1443, %r3486, %r1442, %p56;
	selp.b32 	%r1444, %r3467, 0, %p51;
	selp.b32 	%r1445, %r3471, %r1444, %p52;
	selp.b32 	%r1446, %r3475, %r1445, %p53;
	selp.b32 	%r1447, %r3479, %r1446, %p54;
	selp.b32 	%r1448, %r3483, %r1447, %p55;
	selp.b32 	%r1449, %r3487, %r1448, %p56;
	selp.b32 	%r1450, %r3468, 0, %p51;
	selp.b32 	%r1451, %r3472, %r1450, %p52;
	selp.b32 	%r1452, %r3476, %r1451, %p53;
	selp.b32 	%r1453, %r3480, %r1452, %p54;
	selp.b32 	%r1454, %r3484, %r1453, %p55;
	selp.b32 	%r1455, %r3488, %r1454, %p56;
	selp.b32 	%r1456, %r3469, 0, %p51;
	selp.b32 	%r1457, %r3473, %r1456, %p52;
	selp.b32 	%r1458, %r3477, %r1457, %p53;
	selp.b32 	%r1459, %r3481, %r1458, %p54;
	selp.b32 	%r1460, %r3485, %r1459, %p55;
	selp.b32 	%r1461, %r3489, %r1460, %p56;
	mov.u16 	%rs211, 25600;
	// begin inline asm
	mov.b32 %r906, {%rs211, %rs211};
	// end inline asm
	mov.u16 	%rs213, 21504;
	// begin inline asm
	mov.b32 %r917, {%rs213, %rs213};
	// end inline asm
	xor.b32  	%r916, %r1443, -2004318072;
	mov.u32 	%r1042, 983055;
	// begin inline asm
	lop3.b32 %r903, %r1042, %r916, %r906, 202;
	// end inline asm
	mov.u16 	%rs217, 18432;
	// begin inline asm
	mov.b32 %r907, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r908, %r906, %r907;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r911, %r903, %r908;
	// end inline asm
	mov.u32 	%r1053, 15728880;
	// begin inline asm
	lop3.b32 %r914, %r1053, %r916, %r917, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r918, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r919, %r917, %r918;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r922, %r914, %r919;
	// end inline asm
	// begin inline asm
	mov.b32 %r952, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r963, {%rs213, %rs213};
	// end inline asm
	xor.b32  	%r962, %r1449, -2004318072;
	// begin inline asm
	lop3.b32 %r949, %r1042, %r962, %r952, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r953, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r954, %r952, %r953;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r957, %r949, %r954;
	// end inline asm
	// begin inline asm
	lop3.b32 %r960, %r1053, %r962, %r963, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r964, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r965, %r963, %r964;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r968, %r960, %r965;
	// end inline asm
	// begin inline asm
	mov.b32 %r998, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r1009, {%rs213, %rs213};
	// end inline asm
	xor.b32  	%r1008, %r1455, -2004318072;
	// begin inline asm
	lop3.b32 %r995, %r1042, %r1008, %r998, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r999, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1000, %r998, %r999;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1003, %r995, %r1000;
	// end inline asm
	// begin inline asm
	lop3.b32 %r1006, %r1053, %r1008, %r1009, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1010, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1011, %r1009, %r1010;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1014, %r1006, %r1011;
	// end inline asm
	// begin inline asm
	mov.b32 %r1044, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r1055, {%rs213, %rs213};
	// end inline asm
	xor.b32  	%r1054, %r1461, -2004318072;
	// begin inline asm
	lop3.b32 %r1041, %r1042, %r1054, %r1044, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1045, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1046, %r1044, %r1045;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1049, %r1041, %r1046;
	// end inline asm
	// begin inline asm
	lop3.b32 %r1052, %r1053, %r1054, %r1055, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r1056, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r1057, %r1055, %r1056;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r1060, %r1052, %r1057;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r911;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1085, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r922;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1088, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r957;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1091, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r968;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1094, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r1003;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1097, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r1014;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1100, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r1049;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1103, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r1060;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r1106, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1169, %r1172}, {%r345, %r342}, {%r1085}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1178, %r1181}, {%r345, %r342}, {%r1088}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1187, %r1190}, {%r345, %r342}, {%r1091}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1196, %r1199}, {%r345, %r342}, {%r1094}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1205, %r1208}, {%r345, %r342}, {%r1097}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1214, %r1217}, {%r345, %r342}, {%r1100}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1223, %r1226}, {%r345, %r342}, {%r1103}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1232, %r1235}, {%r345, %r342}, {%r1106}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1165, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1167, %r1165, %r1169;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1170, %r379, %r1172, %r1167;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1174, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1176, %r1174, %r1178;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1179, %r379, %r1181, %r1176;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1183, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1185, %r1183, %r1187;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1188, %r379, %r1190, %r1185;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1192, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1194, %r1192, %r1196;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1197, %r379, %r1199, %r1194;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1201, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1203, %r1201, %r1205;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1206, %r379, %r1208, %r1203;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1210, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1212, %r1210, %r1214;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1215, %r379, %r1217, %r1212;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1219, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1221, %r1219, %r1223;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1224, %r379, %r1226, %r1221;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1228, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1230, %r1228, %r1232;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1233, %r379, %r1235, %r1230;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1237, %r382, %r1172;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1240, %r379, %r1169, %r1237;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1244, %r382, %r1181;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1247, %r379, %r1178, %r1244;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1251, %r382, %r1190;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1254, %r379, %r1187, %r1251;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1258, %r382, %r1199;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1261, %r379, %r1196, %r1258;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1265, %r382, %r1208;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1268, %r379, %r1205, %r1265;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1272, %r382, %r1217;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1275, %r379, %r1214, %r1272;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1279, %r382, %r1226;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1282, %r379, %r1223, %r1279;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1286, %r382, %r1235;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1289, %r379, %r1232, %r1286;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1374, %r1375}, {%r412, %r406, %r409, %r403}, {%r1240, %r1170}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1382, %r1383}, {%r412, %r406, %r409, %r403}, {%r1247, %r1179}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1390, %r1391}, {%r412, %r406, %r409, %r403}, {%r1254, %r1188}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1398, %r1399}, {%r412, %r406, %r409, %r403}, {%r1261, %r1197}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1406, %r1407}, {%r412, %r406, %r409, %r403}, {%r1268, %r1206}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1414, %r1415}, {%r412, %r406, %r409, %r403}, {%r1275, %r1215}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1422, %r1423}, {%r412, %r406, %r409, %r403}, {%r1282, %r1224}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1430, %r1431}, {%r412, %r406, %r409, %r403}, {%r1289, %r1233}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	prmt.b32 %r1373, %r1374, %r1375, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1377, %r1374, %r1375, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1381, %r1382, %r1383, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1385, %r1382, %r1383, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1389, %r1390, %r1391, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1393, %r1390, %r1391, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1397, %r1398, %r1399, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1401, %r1398, %r1399, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1405, %r1406, %r1407, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1409, %r1406, %r1407, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1413, %r1414, %r1415, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1417, %r1414, %r1415, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1421, %r1422, %r1423, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1425, %r1422, %r1423, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1429, %r1430, %r1431, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1433, %r1430, %r1431, %r709;
	// end inline asm
	st.shared.u32 	[%rd6], %r1373;
	st.shared.u32 	[%rd7+33024], %r1377;
	st.shared.u32 	[%rd7+128], %r1381;
	st.shared.u32 	[%rd7+33152], %r1385;
	cvt.u16.u32 	%rs222, %r3491;
	and.b16  	%rs223, %rs222, 255;
	mul.lo.s16 	%rs224, %rs223, 171;
	shr.u16 	%rs225, %rs224, 10;
	mul.lo.s16 	%rs226, %rs225, 6;
	sub.s16 	%rs227, %rs222, %rs226;
	and.b16  	%rs228, %rs227, 255;
	mul.wide.u16 	%r129, %rs228, 4;
	add.s32 	%r1462, %r107, %r129;
	shr.s32 	%r1463, %r1462, 31;
	shr.u32 	%r1464, %r1463, 30;
	add.s32 	%r1465, %r1462, %r1464;
	and.b32  	%r1466, %r1465, 67108860;
	sub.s32 	%r1467, %r1462, %r1466;
	shl.b32 	%r130, %r1467, 6;
	add.s32 	%r1468, %r70, %r130;
	mul.wide.s32 	%rd188, %r1468, 4;
	add.s64 	%rd190, %rd26, %rd188;
	st.shared.u32 	[%rd190], %r1389;
	add.s32 	%r131, %r130, 8256;
	add.s32 	%r1469, %r70, %r131;
	mul.wide.u32 	%rd191, %r1469, 4;
	add.s64 	%rd192, %rd26, %rd191;
	st.shared.u32 	[%rd192], %r1393;
	or.b32  	%r132, %r130, 32;
	add.s32 	%r1470, %r70, %r132;
	mul.wide.s32 	%rd193, %r1470, 4;
	add.s64 	%rd194, %rd26, %rd193;
	st.shared.u32 	[%rd194], %r1397;
	add.s32 	%r133, %r130, 8288;
	add.s32 	%r1471, %r70, %r133;
	mul.wide.u32 	%rd195, %r1471, 4;
	add.s64 	%rd196, %rd26, %rd195;
	st.shared.u32 	[%rd196], %r1401;
	add.s32 	%r1472, %r108, %r129;
	shr.s32 	%r1473, %r1472, 31;
	shr.u32 	%r1474, %r1473, 30;
	add.s32 	%r1475, %r1472, %r1474;
	and.b32  	%r1476, %r1475, 67108860;
	sub.s32 	%r1477, %r1472, %r1476;
	shl.b32 	%r134, %r1477, 6;
	add.s32 	%r1478, %r70, %r134;
	mul.wide.s32 	%rd197, %r1478, 4;
	add.s64 	%rd198, %rd26, %rd197;
	st.shared.u32 	[%rd198], %r1405;
	add.s32 	%r135, %r134, 8256;
	add.s32 	%r1479, %r70, %r135;
	mul.wide.u32 	%rd199, %r1479, 4;
	add.s64 	%rd200, %rd26, %rd199;
	st.shared.u32 	[%rd200], %r1409;
	or.b32  	%r136, %r134, 32;
	add.s32 	%r1480, %r70, %r136;
	mul.wide.s32 	%rd201, %r1480, 4;
	add.s64 	%rd202, %rd26, %rd201;
	st.shared.u32 	[%rd202], %r1413;
	add.s32 	%r137, %r134, 8288;
	add.s32 	%r1481, %r70, %r137;
	mul.wide.u32 	%rd203, %r1481, 4;
	add.s64 	%rd204, %rd26, %rd203;
	st.shared.u32 	[%rd204], %r1417;
	add.s32 	%r1482, %r109, %r129;
	shr.s32 	%r1483, %r1482, 31;
	shr.u32 	%r1484, %r1483, 30;
	add.s32 	%r1485, %r1482, %r1484;
	and.b32  	%r1486, %r1485, 67108860;
	sub.s32 	%r1487, %r1482, %r1486;
	shl.b32 	%r138, %r1487, 6;
	add.s32 	%r1488, %r70, %r138;
	mul.wide.s32 	%rd205, %r1488, 4;
	add.s64 	%rd206, %rd26, %rd205;
	st.shared.u32 	[%rd206], %r1421;
	add.s32 	%r139, %r138, 8256;
	add.s32 	%r1489, %r70, %r139;
	mul.wide.u32 	%rd207, %r1489, 4;
	add.s64 	%rd208, %rd26, %rd207;
	st.shared.u32 	[%rd208], %r1425;
	or.b32  	%r140, %r138, 32;
	add.s32 	%r1490, %r70, %r140;
	mul.wide.s32 	%rd209, %r1490, 4;
	add.s64 	%rd210, %rd26, %rd209;
	st.shared.u32 	[%rd210], %r1429;
	add.s32 	%r141, %r138, 8288;
	add.s32 	%r1491, %r70, %r141;
	mul.wide.u32 	%rd211, %r1491, 4;
	add.s64 	%rd212, %rd26, %rd211;
	st.shared.u32 	[%rd212], %r1433;
	bar.sync 	0;
	mov.u32 	%r3498, %r3451;
	mov.u32 	%r3499, %r3451;
	mov.u32 	%r3500, %r3451;
	mov.u32 	%r3501, %r3451;
	@%p50 bra 	LBB0_64;
	bra.uni 	LBB0_19;
LBB0_64:                                // %oksrem7108
                                        //   in Loop: Header=BB0_18 Depth=2
	ld.shared.u32 	%r3498, [%rd8];
	ld.shared.u32 	%r3499, [%rd9+33024];
	ld.shared.u32 	%r3500, [%rd9+128];
	ld.shared.u32 	%r3501, [%rd9+33152];
LBB0_19:                                // %L19416
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1524, %r1527}, {%r345, %r342}, {%r3498}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1533, %r1536}, {%r345, %r342}, {%r3499}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1542, %r1545}, {%r345, %r342}, {%r3500}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1551, %r1554}, {%r345, %r342}, {%r3501}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1520, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1522, %r1520, %r1524;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1525, %r379, %r1527, %r1522;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1529, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1531, %r1529, %r1533;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1534, %r379, %r1536, %r1531;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1538, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1540, %r1538, %r1542;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1543, %r379, %r1545, %r1540;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1547, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1549, %r1547, %r1551;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1552, %r379, %r1554, %r1549;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1556, %r382, %r1527;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1559, %r379, %r1524, %r1556;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1563, %r382, %r1536;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1566, %r379, %r1533, %r1563;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1570, %r382, %r1545;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1573, %r379, %r1542, %r1570;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1577, %r382, %r1554;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1580, %r379, %r1551, %r1577;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1625, %r1629}, {%r412, %r406, %r409, %r403}, {%r1559, %r1525}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1633, %r1637}, {%r412, %r406, %r409, %r403}, {%r1566, %r1534}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1641, %r1645}, {%r412, %r406, %r409, %r403}, {%r1573, %r1543}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1649, %r1653}, {%r412, %r406, %r409, %r403}, {%r1580, %r1552}, {%r3451, %r3451};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1624, %r1625, %r1625, %r3497;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3507, %r1629, %r1629, %r1624;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1632, %r1633, %r1633, %r3496;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3506, %r1637, %r1637, %r1632;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1640, %r1641, %r1641, %r3495;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3505, %r1645, %r1645, %r1640;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1648, %r1649, %r1649, %r3494;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3504, %r1653, %r1653, %r1648;
	// end inline asm
	add.s32 	%r3502, %r3492, 1;
	setp.ne.s32 	%p57, %r3502, 40;
	@%p57 bra 	LBB0_23;
// %bb.20:                              // %L19599
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_22;
// %bb.21:                              // %L19664
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r1656, %r3493, 193536;
	add.s32 	%r1657, %r73, %r1656;
	add.s32 	%r1658, %r1657, %r72;
	add.s32 	%r1659, %r1658, %r44;
	mul.wide.u32 	%rd213, %r1659, 4;
	add.s64 	%rd214, %rd4, %rd213;
	st.global.u32 	[%rd214], %r3507;
	add.s32 	%r1660, %r1657, %r74;
	add.s32 	%r1661, %r1660, %r44;
	mul.wide.u32 	%rd215, %r1661, 4;
	add.s64 	%rd216, %rd4, %rd215;
	st.global.u32 	[%rd216], %r3506;
	cvt.u64.u32 	%rd217, %r44;
	cvt.u64.u32 	%rd218, %r72;
	cvt.u64.u32 	%rd219, %r73;
	cvt.u64.u32 	%rd220, %r1656;
	add.s64 	%rd221, %rd220, %rd219;
	add.s64 	%rd222, %rd221, %rd218;
	add.s64 	%rd223, %rd222, %rd217;
	shl.b64 	%rd224, %rd223, 2;
	add.s64 	%rd225, %rd4, %rd224;
	st.global.u32 	[%rd225+387072], %r3505;
	cvt.u64.u32 	%rd226, %r74;
	add.s64 	%rd227, %rd221, %rd226;
	add.s64 	%rd228, %rd227, %rd217;
	shl.b64 	%rd229, %rd228, 2;
	add.s64 	%rd230, %rd4, %rd229;
	st.global.u32 	[%rd230+387072], %r3504;
LBB0_22:                                // %L20177
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3502, 0;
	mov.u32 	%r3504, %r3502;
	mov.u32 	%r3505, %r3502;
	mov.u32 	%r3506, %r3502;
	mov.u32 	%r3507, %r3502;
LBB0_23:                                // %L20178
                                        //   in Loop: Header=BB0_18 Depth=2
	mov.u32 	%r1663, 0;
	mov.u32 	%r3508, %r1663;
	mov.u32 	%r3509, %r1663;
	mov.u32 	%r3510, %r1663;
	mov.u32 	%r3511, %r1663;
	@%p50 bra 	LBB0_65;
	bra.uni 	LBB0_24;
LBB0_65:                                // %oksrem7862
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r1664, %r71, %r130;
	mul.wide.s32 	%rd231, %r1664, 4;
	add.s64 	%rd233, %rd26, %rd231;
	ld.shared.u32 	%r3508, [%rd233];
	add.s32 	%r1665, %r71, %r131;
	mul.wide.u32 	%rd234, %r1665, 4;
	add.s64 	%rd235, %rd26, %rd234;
	ld.shared.u32 	%r3509, [%rd235];
	add.s32 	%r1666, %r71, %r132;
	mul.wide.s32 	%rd236, %r1666, 4;
	add.s64 	%rd237, %rd26, %rd236;
	ld.shared.u32 	%r3510, [%rd237];
	add.s32 	%r1667, %r71, %r133;
	mul.wide.u32 	%rd238, %r1667, 4;
	add.s64 	%rd239, %rd26, %rd238;
	ld.shared.u32 	%r3511, [%rd239];
LBB0_24:                                // %L21385
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1700, %r1703}, {%r345, %r342}, {%r3508}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1709, %r1712}, {%r345, %r342}, {%r3509}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1718, %r1721}, {%r345, %r342}, {%r3510}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1727, %r1730}, {%r345, %r342}, {%r3511}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1696, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1698, %r1696, %r1700;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1701, %r379, %r1703, %r1698;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1705, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1707, %r1705, %r1709;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1710, %r379, %r1712, %r1707;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1714, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1716, %r1714, %r1718;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1719, %r379, %r1721, %r1716;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1723, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1725, %r1723, %r1727;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1728, %r379, %r1730, %r1725;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1732, %r382, %r1703;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1735, %r379, %r1700, %r1732;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1739, %r382, %r1712;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1742, %r379, %r1709, %r1739;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1746, %r382, %r1721;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1749, %r379, %r1718, %r1746;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1753, %r382, %r1730;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1756, %r379, %r1727, %r1753;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1801, %r1805}, {%r412, %r406, %r409, %r403}, {%r1735, %r1701}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1809, %r1813}, {%r412, %r406, %r409, %r403}, {%r1742, %r1710}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1817, %r1821}, {%r412, %r406, %r409, %r403}, {%r1749, %r1719}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1825, %r1829}, {%r412, %r406, %r409, %r403}, {%r1756, %r1728}, {%r1663, %r1663};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1800, %r1801, %r1801, %r3507;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3517, %r1805, %r1805, %r1800;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1808, %r1809, %r1809, %r3506;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3516, %r1813, %r1813, %r1808;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1816, %r1817, %r1817, %r3505;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3515, %r1821, %r1821, %r1816;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1824, %r1825, %r1825, %r3504;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3514, %r1829, %r1829, %r1824;
	// end inline asm
	add.s32 	%r3512, %r3502, 1;
	setp.ne.s32 	%p60, %r3512, 40;
	@%p60 bra 	LBB0_28;
// %bb.25:                              // %L21568
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_27;
// %bb.26:                              // %L21633
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r1832, %r3493, 193536;
	add.s32 	%r1833, %r73, %r1832;
	add.s32 	%r1834, %r1833, %r72;
	add.s32 	%r1835, %r1834, %r44;
	mul.wide.u32 	%rd240, %r1835, 4;
	add.s64 	%rd241, %rd4, %rd240;
	st.global.u32 	[%rd241], %r3517;
	add.s32 	%r1836, %r1833, %r74;
	add.s32 	%r1837, %r1836, %r44;
	mul.wide.u32 	%rd242, %r1837, 4;
	add.s64 	%rd243, %rd4, %rd242;
	st.global.u32 	[%rd243], %r3516;
	cvt.u64.u32 	%rd244, %r44;
	cvt.u64.u32 	%rd245, %r72;
	cvt.u64.u32 	%rd246, %r73;
	cvt.u64.u32 	%rd247, %r1832;
	add.s64 	%rd248, %rd247, %rd246;
	add.s64 	%rd249, %rd248, %rd245;
	add.s64 	%rd250, %rd249, %rd244;
	shl.b64 	%rd251, %rd250, 2;
	add.s64 	%rd252, %rd4, %rd251;
	st.global.u32 	[%rd252+387072], %r3515;
	cvt.u64.u32 	%rd253, %r74;
	add.s64 	%rd254, %rd248, %rd253;
	add.s64 	%rd255, %rd254, %rd244;
	shl.b64 	%rd256, %rd255, 2;
	add.s64 	%rd257, %rd4, %rd256;
	st.global.u32 	[%rd257+387072], %r3514;
LBB0_27:                                // %L22146
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3512, 0;
	mov.u32 	%r3514, %r3512;
	mov.u32 	%r3515, %r3512;
	mov.u32 	%r3516, %r3512;
	mov.u32 	%r3517, %r3512;
LBB0_28:                                // %L22147
                                        //   in Loop: Header=BB0_18 Depth=2
	mov.u32 	%r1839, 0;
	mov.u32 	%r3518, %r1839;
	mov.u32 	%r3519, %r1839;
	mov.u32 	%r3520, %r1839;
	mov.u32 	%r3521, %r1839;
	@%p50 bra 	LBB0_66;
	bra.uni 	LBB0_29;
LBB0_66:                                // %oksrem8616
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r1840, %r71, %r134;
	mul.wide.s32 	%rd258, %r1840, 4;
	add.s64 	%rd260, %rd26, %rd258;
	ld.shared.u32 	%r3518, [%rd260];
	add.s32 	%r1841, %r71, %r135;
	mul.wide.u32 	%rd261, %r1841, 4;
	add.s64 	%rd262, %rd26, %rd261;
	ld.shared.u32 	%r3519, [%rd262];
	add.s32 	%r1842, %r71, %r136;
	mul.wide.s32 	%rd263, %r1842, 4;
	add.s64 	%rd264, %rd26, %rd263;
	ld.shared.u32 	%r3520, [%rd264];
	add.s32 	%r1843, %r71, %r137;
	mul.wide.u32 	%rd265, %r1843, 4;
	add.s64 	%rd266, %rd26, %rd265;
	ld.shared.u32 	%r3521, [%rd266];
LBB0_29:                                // %L23354
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1876, %r1879}, {%r345, %r342}, {%r3518}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1885, %r1888}, {%r345, %r342}, {%r3519}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1894, %r1897}, {%r345, %r342}, {%r3520}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r1903, %r1906}, {%r345, %r342}, {%r3521}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1872, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1874, %r1872, %r1876;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1877, %r379, %r1879, %r1874;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1881, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1883, %r1881, %r1885;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1886, %r379, %r1888, %r1883;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1890, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1892, %r1890, %r1894;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1895, %r379, %r1897, %r1892;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r1899, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1901, %r1899, %r1903;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1904, %r379, %r1906, %r1901;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1908, %r382, %r1879;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1911, %r379, %r1876, %r1908;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1915, %r382, %r1888;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1918, %r379, %r1885, %r1915;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1922, %r382, %r1897;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1925, %r379, %r1894, %r1922;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r1929, %r382, %r1906;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1932, %r379, %r1903, %r1929;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1977, %r1981}, {%r412, %r406, %r409, %r403}, {%r1911, %r1877}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1985, %r1989}, {%r412, %r406, %r409, %r403}, {%r1918, %r1886}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r1993, %r1997}, {%r412, %r406, %r409, %r403}, {%r1925, %r1895}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2001, %r2005}, {%r412, %r406, %r409, %r403}, {%r1932, %r1904}, {%r1839, %r1839};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1976, %r1977, %r1977, %r3517;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3527, %r1981, %r1981, %r1976;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1984, %r1985, %r1985, %r3516;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3526, %r1989, %r1989, %r1984;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r1992, %r1993, %r1993, %r3515;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3525, %r1997, %r1997, %r1992;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2000, %r2001, %r2001, %r3514;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3524, %r2005, %r2005, %r2000;
	// end inline asm
	add.s32 	%r3522, %r3512, 1;
	setp.ne.s32 	%p63, %r3522, 40;
	@%p63 bra 	LBB0_33;
// %bb.30:                              // %L23537
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_32;
// %bb.31:                              // %L23602
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r2008, %r3493, 193536;
	add.s32 	%r2009, %r73, %r2008;
	add.s32 	%r2010, %r2009, %r72;
	add.s32 	%r2011, %r2010, %r44;
	mul.wide.u32 	%rd267, %r2011, 4;
	add.s64 	%rd268, %rd4, %rd267;
	st.global.u32 	[%rd268], %r3527;
	add.s32 	%r2012, %r2009, %r74;
	add.s32 	%r2013, %r2012, %r44;
	mul.wide.u32 	%rd269, %r2013, 4;
	add.s64 	%rd270, %rd4, %rd269;
	st.global.u32 	[%rd270], %r3526;
	cvt.u64.u32 	%rd271, %r44;
	cvt.u64.u32 	%rd272, %r72;
	cvt.u64.u32 	%rd273, %r73;
	cvt.u64.u32 	%rd274, %r2008;
	add.s64 	%rd275, %rd274, %rd273;
	add.s64 	%rd276, %rd275, %rd272;
	add.s64 	%rd277, %rd276, %rd271;
	shl.b64 	%rd278, %rd277, 2;
	add.s64 	%rd279, %rd4, %rd278;
	st.global.u32 	[%rd279+387072], %r3525;
	cvt.u64.u32 	%rd280, %r74;
	add.s64 	%rd281, %rd275, %rd280;
	add.s64 	%rd282, %rd281, %rd271;
	shl.b64 	%rd283, %rd282, 2;
	add.s64 	%rd284, %rd4, %rd283;
	st.global.u32 	[%rd284+387072], %r3524;
LBB0_32:                                // %L24115
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3522, 0;
	mov.u32 	%r3524, %r3522;
	mov.u32 	%r3525, %r3522;
	mov.u32 	%r3526, %r3522;
	mov.u32 	%r3527, %r3522;
LBB0_33:                                // %L24116
                                        //   in Loop: Header=BB0_18 Depth=2
	mov.u32 	%r2015, 0;
	mov.u32 	%r3528, %r2015;
	mov.u32 	%r3529, %r2015;
	mov.u32 	%r3530, %r2015;
	mov.u32 	%r3531, %r2015;
	@%p50 bra 	LBB0_67;
	bra.uni 	LBB0_34;
LBB0_67:                                // %oksrem9370
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r2016, %r71, %r138;
	mul.wide.s32 	%rd285, %r2016, 4;
	add.s64 	%rd287, %rd26, %rd285;
	ld.shared.u32 	%r3528, [%rd287];
	add.s32 	%r2017, %r71, %r139;
	mul.wide.u32 	%rd288, %r2017, 4;
	add.s64 	%rd289, %rd26, %rd288;
	ld.shared.u32 	%r3529, [%rd289];
	add.s32 	%r2018, %r71, %r140;
	mul.wide.s32 	%rd290, %r2018, 4;
	add.s64 	%rd291, %rd26, %rd290;
	ld.shared.u32 	%r3530, [%rd291];
	add.s32 	%r2019, %r71, %r141;
	mul.wide.u32 	%rd292, %r2019, 4;
	add.s64 	%rd293, %rd26, %rd292;
	ld.shared.u32 	%r3531, [%rd293];
LBB0_34:                                // %L25323
                                        //   in Loop: Header=BB0_18 Depth=2
	shr.u32 	%r2217, %r916, 8;
	shr.u32 	%r2263, %r962, 8;
	shr.u32 	%r2309, %r1008, 8;
	shr.u32 	%r2355, %r1054, 8;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2052, %r2055}, {%r345, %r342}, {%r3528}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2061, %r2064}, {%r345, %r342}, {%r3529}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2070, %r2073}, {%r345, %r342}, {%r3530}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2079, %r2082}, {%r345, %r342}, {%r3531}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2048, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2050, %r2048, %r2052;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2053, %r379, %r2055, %r2050;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2057, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2059, %r2057, %r2061;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2062, %r379, %r2064, %r2059;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2066, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2068, %r2066, %r2070;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2071, %r379, %r2073, %r2068;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2075, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2077, %r2075, %r2079;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2080, %r379, %r2082, %r2077;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2084, %r382, %r2055;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2087, %r379, %r2052, %r2084;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2091, %r382, %r2064;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2094, %r379, %r2061, %r2091;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2098, %r382, %r2073;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2101, %r379, %r2070, %r2098;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2105, %r382, %r2082;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2108, %r379, %r2079, %r2105;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2153, %r2157}, {%r412, %r406, %r409, %r403}, {%r2087, %r2053}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2161, %r2165}, {%r412, %r406, %r409, %r403}, {%r2094, %r2062}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2169, %r2173}, {%r412, %r406, %r409, %r403}, {%r2101, %r2071}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2177, %r2181}, {%r412, %r406, %r409, %r403}, {%r2108, %r2080}, {%r2015, %r2015};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2152, %r2153, %r2153, %r3527;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3537, %r2157, %r2157, %r2152;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2160, %r2161, %r2161, %r3526;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3536, %r2165, %r2165, %r2160;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2168, %r2169, %r2169, %r3525;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3535, %r2173, %r2173, %r2168;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2176, %r2177, %r2177, %r3524;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3534, %r2181, %r2181, %r2176;
	// end inline asm
	add.s32 	%r3532, %r3522, 1;
	setp.ne.s32 	%p66, %r3532, 40;
	@%p66 bra 	LBB0_38;
// %bb.35:                              // %L25506
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_37;
// %bb.36:                              // %L25571
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r2184, %r3493, 193536;
	add.s32 	%r2185, %r73, %r2184;
	add.s32 	%r2186, %r2185, %r72;
	add.s32 	%r2187, %r2186, %r44;
	mul.wide.u32 	%rd294, %r2187, 4;
	add.s64 	%rd295, %rd4, %rd294;
	st.global.u32 	[%rd295], %r3537;
	add.s32 	%r2188, %r2185, %r74;
	add.s32 	%r2189, %r2188, %r44;
	mul.wide.u32 	%rd296, %r2189, 4;
	add.s64 	%rd297, %rd4, %rd296;
	st.global.u32 	[%rd297], %r3536;
	cvt.u64.u32 	%rd298, %r44;
	cvt.u64.u32 	%rd299, %r72;
	cvt.u64.u32 	%rd300, %r73;
	cvt.u64.u32 	%rd301, %r2184;
	add.s64 	%rd302, %rd301, %rd300;
	add.s64 	%rd303, %rd302, %rd299;
	add.s64 	%rd304, %rd303, %rd298;
	shl.b64 	%rd305, %rd304, 2;
	add.s64 	%rd306, %rd4, %rd305;
	st.global.u32 	[%rd306+387072], %r3535;
	cvt.u64.u32 	%rd307, %r74;
	add.s64 	%rd308, %rd302, %rd307;
	add.s64 	%rd309, %rd308, %rd298;
	shl.b64 	%rd310, %rd309, 2;
	add.s64 	%rd311, %rd4, %rd310;
	st.global.u32 	[%rd311+387072], %r3534;
LBB0_37:                                // %L26084
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3532, 0;
	mov.u32 	%r3534, %r3532;
	mov.u32 	%r3535, %r3532;
	mov.u32 	%r3536, %r3532;
	mov.u32 	%r3537, %r3532;
LBB0_38:                                // %L26085
                                        //   in Loop: Header=BB0_18 Depth=2
	bar.sync 	0;
	// begin inline asm
	mov.b32 %r2218, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r2229, {%rs213, %rs213};
	// end inline asm
	// begin inline asm
	lop3.b32 %r2215, %r1042, %r2217, %r2218, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2219, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2220, %r2218, %r2219;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2223, %r2215, %r2220;
	// end inline asm
	// begin inline asm
	lop3.b32 %r2226, %r1053, %r2217, %r2229, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2230, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2231, %r2229, %r2230;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2234, %r2226, %r2231;
	// end inline asm
	// begin inline asm
	mov.b32 %r2264, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r2275, {%rs213, %rs213};
	// end inline asm
	// begin inline asm
	lop3.b32 %r2261, %r1042, %r2263, %r2264, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2265, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2266, %r2264, %r2265;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2269, %r2261, %r2266;
	// end inline asm
	// begin inline asm
	lop3.b32 %r2272, %r1053, %r2263, %r2275, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2276, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2277, %r2275, %r2276;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2280, %r2272, %r2277;
	// end inline asm
	// begin inline asm
	mov.b32 %r2310, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r2321, {%rs213, %rs213};
	// end inline asm
	// begin inline asm
	lop3.b32 %r2307, %r1042, %r2309, %r2310, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2311, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2312, %r2310, %r2311;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2315, %r2307, %r2312;
	// end inline asm
	// begin inline asm
	lop3.b32 %r2318, %r1053, %r2309, %r2321, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2322, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2323, %r2321, %r2322;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2326, %r2318, %r2323;
	// end inline asm
	// begin inline asm
	mov.b32 %r2356, {%rs211, %rs211};
	// end inline asm
	// begin inline asm
	mov.b32 %r2367, {%rs213, %rs213};
	// end inline asm
	// begin inline asm
	lop3.b32 %r2353, %r1042, %r2355, %r2356, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2357, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2358, %r2356, %r2357;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2361, %r2353, %r2358;
	// end inline asm
	// begin inline asm
	lop3.b32 %r2364, %r1053, %r2355, %r2367, 202;
	// end inline asm
	// begin inline asm
	mov.b32 %r2368, {%rs217, %rs217};
	// end inline asm
	// begin inline asm
	add.rn.f16x2 %r2369, %r2367, %r2368;
	// end inline asm
	// begin inline asm
	sub.rn.f16x2 %r2372, %r2364, %r2369;
	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r2223;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2375, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r2234;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2378, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r2269;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2381, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r2280;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2384, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r2315;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2387, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r2326;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2390, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3457;
    mov.b32 {%r2im, %r2re}, %r2361;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2393, {%r0im, %r0re};
}

	// end inline asm
	// begin inline asm
	{
    .reg .f16 %r1re, %r1im, %r2re, %r2im, %r1imneg, %retmp, %r0re, %imtmp, %r0im;
    mov.b32 {%r1im, %r1re}, %r3458;
    mov.b32 {%r2im, %r2re}, %r2372;
    mul.f16 %retmp, %r1re, %r2re;
    mul.f16 %imtmp, %r1re, %r2im;
    neg.f16 %r1imneg, %r1im;
    fma.rn.f16 %r0re, %r1imneg, %r2im, %retmp;
    fma.rn.f16 %r0im, %r1im, %r2re, %imtmp;
    mov.b32 %r2396, {%r0im, %r0re};
}

	// end inline asm
	mov.u32 	%r2727, 0;
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2459, %r2462}, {%r345, %r342}, {%r2375}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2468, %r2471}, {%r345, %r342}, {%r2378}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2477, %r2480}, {%r345, %r342}, {%r2381}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2486, %r2489}, {%r345, %r342}, {%r2384}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2495, %r2498}, {%r345, %r342}, {%r2387}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2504, %r2507}, {%r345, %r342}, {%r2390}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2513, %r2516}, {%r345, %r342}, {%r2393}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2522, %r2525}, {%r345, %r342}, {%r2396}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2455, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2457, %r2455, %r2459;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2460, %r379, %r2462, %r2457;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2464, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2466, %r2464, %r2468;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2469, %r379, %r2471, %r2466;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2473, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2475, %r2473, %r2477;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2478, %r379, %r2480, %r2475;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2482, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2484, %r2482, %r2486;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2487, %r379, %r2489, %r2484;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2491, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2493, %r2491, %r2495;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2496, %r379, %r2498, %r2493;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2500, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2502, %r2500, %r2504;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2505, %r379, %r2507, %r2502;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2509, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2511, %r2509, %r2513;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2514, %r379, %r2516, %r2511;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2518, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2520, %r2518, %r2522;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2523, %r379, %r2525, %r2520;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2527, %r382, %r2462;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2530, %r379, %r2459, %r2527;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2534, %r382, %r2471;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2537, %r379, %r2468, %r2534;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2541, %r382, %r2480;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2544, %r379, %r2477, %r2541;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2548, %r382, %r2489;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2551, %r379, %r2486, %r2548;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2555, %r382, %r2498;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2558, %r379, %r2495, %r2555;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2562, %r382, %r2507;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2565, %r379, %r2504, %r2562;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2569, %r382, %r2516;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2572, %r379, %r2513, %r2569;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2576, %r382, %r2525;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2579, %r379, %r2522, %r2576;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2664, %r2665}, {%r412, %r406, %r409, %r403}, {%r2530, %r2460}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2672, %r2673}, {%r412, %r406, %r409, %r403}, {%r2537, %r2469}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2680, %r2681}, {%r412, %r406, %r409, %r403}, {%r2544, %r2478}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2688, %r2689}, {%r412, %r406, %r409, %r403}, {%r2551, %r2487}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2696, %r2697}, {%r412, %r406, %r409, %r403}, {%r2558, %r2496}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2704, %r2705}, {%r412, %r406, %r409, %r403}, {%r2565, %r2505}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2712, %r2713}, {%r412, %r406, %r409, %r403}, {%r2572, %r2514}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2720, %r2721}, {%r412, %r406, %r409, %r403}, {%r2579, %r2523}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	prmt.b32 %r2663, %r2664, %r2665, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2667, %r2664, %r2665, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2671, %r2672, %r2673, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2675, %r2672, %r2673, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2679, %r2680, %r2681, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2683, %r2680, %r2681, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2687, %r2688, %r2689, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2691, %r2688, %r2689, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2695, %r2696, %r2697, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2699, %r2696, %r2697, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2703, %r2704, %r2705, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2707, %r2704, %r2705, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2711, %r2712, %r2713, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2715, %r2712, %r2713, %r709;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2719, %r2720, %r2721, %r705;
	// end inline asm
	// begin inline asm
	prmt.b32 %r2723, %r2720, %r2721, %r709;
	// end inline asm
	st.shared.u32 	[%rd6], %r2663;
	st.shared.u32 	[%rd7+33024], %r2667;
	st.shared.u32 	[%rd7+128], %r2671;
	st.shared.u32 	[%rd7+33152], %r2675;
	add.s32 	%r2728, %r110, %r129;
	cvt.u16.u32 	%rs277, %r2728;
	shr.s16 	%rs278, %rs277, 15;
	shr.u16 	%rs279, %rs278, 14;
	add.s16 	%rs280, %rs277, %rs279;
	and.b16  	%rs281, %rs280, -4;
	sub.s16 	%rs282, %rs277, %rs281;
	mul.wide.s16 	%r206, %rs282, 64;
	add.s32 	%r2729, %r70, %r206;
	mul.wide.s32 	%rd312, %r2729, 4;
	add.s64 	%rd314, %rd26, %rd312;
	st.shared.u32 	[%rd314], %r2679;
	add.s32 	%r207, %r206, 8256;
	add.s32 	%r2730, %r70, %r207;
	mul.wide.u32 	%rd315, %r2730, 4;
	add.s64 	%rd316, %rd26, %rd315;
	st.shared.u32 	[%rd316], %r2683;
	or.b32  	%r208, %r206, 32;
	add.s32 	%r2731, %r70, %r208;
	mul.wide.s32 	%rd317, %r2731, 4;
	add.s64 	%rd318, %rd26, %rd317;
	st.shared.u32 	[%rd318], %r2687;
	add.s32 	%r209, %r206, 8288;
	add.s32 	%r2732, %r70, %r209;
	mul.wide.u32 	%rd319, %r2732, 4;
	add.s64 	%rd320, %rd26, %rd319;
	st.shared.u32 	[%rd320], %r2691;
	add.s32 	%r2733, %r111, %r129;
	cvt.u16.u32 	%rs283, %r2733;
	shr.s16 	%rs284, %rs283, 15;
	shr.u16 	%rs285, %rs284, 14;
	add.s16 	%rs286, %rs283, %rs285;
	and.b16  	%rs287, %rs286, -4;
	sub.s16 	%rs288, %rs283, %rs287;
	mul.wide.s16 	%r210, %rs288, 64;
	add.s32 	%r2734, %r70, %r210;
	mul.wide.s32 	%rd321, %r2734, 4;
	add.s64 	%rd322, %rd26, %rd321;
	st.shared.u32 	[%rd322], %r2695;
	add.s32 	%r211, %r210, 8256;
	add.s32 	%r2735, %r70, %r211;
	mul.wide.u32 	%rd323, %r2735, 4;
	add.s64 	%rd324, %rd26, %rd323;
	st.shared.u32 	[%rd324], %r2699;
	or.b32  	%r212, %r210, 32;
	add.s32 	%r2736, %r70, %r212;
	mul.wide.s32 	%rd325, %r2736, 4;
	add.s64 	%rd326, %rd26, %rd325;
	st.shared.u32 	[%rd326], %r2703;
	add.s32 	%r213, %r210, 8288;
	add.s32 	%r2737, %r70, %r213;
	mul.wide.u32 	%rd327, %r2737, 4;
	add.s64 	%rd328, %rd26, %rd327;
	st.shared.u32 	[%rd328], %r2707;
	add.s32 	%r2738, %r112, %r129;
	cvt.u16.u32 	%rs289, %r2738;
	shr.s16 	%rs290, %rs289, 15;
	shr.u16 	%rs291, %rs290, 14;
	add.s16 	%rs292, %rs289, %rs291;
	and.b16  	%rs293, %rs292, -4;
	sub.s16 	%rs294, %rs289, %rs293;
	mul.wide.s16 	%r214, %rs294, 64;
	add.s32 	%r2739, %r70, %r214;
	mul.wide.s32 	%rd329, %r2739, 4;
	add.s64 	%rd330, %rd26, %rd329;
	st.shared.u32 	[%rd330], %r2711;
	add.s32 	%r215, %r214, 8256;
	add.s32 	%r2740, %r70, %r215;
	mul.wide.u32 	%rd331, %r2740, 4;
	add.s64 	%rd332, %rd26, %rd331;
	st.shared.u32 	[%rd332], %r2715;
	or.b32  	%r216, %r214, 32;
	add.s32 	%r2741, %r70, %r216;
	mul.wide.s32 	%rd333, %r2741, 4;
	add.s64 	%rd334, %rd26, %rd333;
	st.shared.u32 	[%rd334], %r2719;
	add.s32 	%r217, %r214, 8288;
	add.s32 	%r2742, %r70, %r217;
	mul.wide.u32 	%rd335, %r2742, 4;
	add.s64 	%rd336, %rd26, %rd335;
	st.shared.u32 	[%rd336], %r2723;
	bar.sync 	0;
	mov.u32 	%r3538, %r2727;
	mov.u32 	%r3539, %r2727;
	mov.u32 	%r3540, %r2727;
	mov.u32 	%r3541, %r2727;
	@%p50 bra 	LBB0_68;
	bra.uni 	LBB0_39;
LBB0_68:                                // %oksrem12252
                                        //   in Loop: Header=BB0_18 Depth=2
	ld.shared.u32 	%r3541, [%rd8];
	ld.shared.u32 	%r3540, [%rd9+33024];
	ld.shared.u32 	%r3539, [%rd9+128];
	ld.shared.u32 	%r3538, [%rd9+33152];
LBB0_39:                                // %L32690
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2775, %r2778}, {%r345, %r342}, {%r3541}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2784, %r2787}, {%r345, %r342}, {%r3540}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2793, %r2796}, {%r345, %r342}, {%r3539}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2802, %r2805}, {%r345, %r342}, {%r3538}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2771, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2773, %r2771, %r2775;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2776, %r379, %r2778, %r2773;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2780, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2782, %r2780, %r2784;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2785, %r379, %r2787, %r2782;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2789, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2791, %r2789, %r2793;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2794, %r379, %r2796, %r2791;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2798, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2800, %r2798, %r2802;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2803, %r379, %r2805, %r2800;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2807, %r382, %r2778;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2810, %r379, %r2775, %r2807;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2814, %r382, %r2787;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2817, %r379, %r2784, %r2814;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2821, %r382, %r2796;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2824, %r379, %r2793, %r2821;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2828, %r382, %r2805;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2831, %r379, %r2802, %r2828;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2876, %r2880}, {%r412, %r406, %r409, %r403}, {%r2810, %r2776}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2884, %r2888}, {%r412, %r406, %r409, %r403}, {%r2817, %r2785}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2892, %r2896}, {%r412, %r406, %r409, %r403}, {%r2824, %r2794}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r2900, %r2904}, {%r412, %r406, %r409, %r403}, {%r2831, %r2803}, {%r2727, %r2727};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2875, %r2876, %r2876, %r3537;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3547, %r2880, %r2880, %r2875;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2883, %r2884, %r2884, %r3536;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3546, %r2888, %r2888, %r2883;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2891, %r2892, %r2892, %r3535;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3545, %r2896, %r2896, %r2891;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2899, %r2900, %r2900, %r3534;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3544, %r2904, %r2904, %r2899;
	// end inline asm
	add.s32 	%r3542, %r3532, 1;
	setp.ne.s32 	%p69, %r3542, 40;
	@%p69 bra 	LBB0_43;
// %bb.40:                              // %L32873
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_42;
// %bb.41:                              // %L32938
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r2907, %r3493, 193536;
	add.s32 	%r2908, %r73, %r2907;
	add.s32 	%r2909, %r2908, %r72;
	add.s32 	%r2910, %r2909, %r44;
	mul.wide.u32 	%rd337, %r2910, 4;
	add.s64 	%rd338, %rd4, %rd337;
	st.global.u32 	[%rd338], %r3547;
	add.s32 	%r2911, %r2908, %r74;
	add.s32 	%r2912, %r2911, %r44;
	mul.wide.u32 	%rd339, %r2912, 4;
	add.s64 	%rd340, %rd4, %rd339;
	st.global.u32 	[%rd340], %r3546;
	cvt.u64.u32 	%rd341, %r44;
	cvt.u64.u32 	%rd342, %r72;
	cvt.u64.u32 	%rd343, %r73;
	cvt.u64.u32 	%rd344, %r2907;
	add.s64 	%rd345, %rd344, %rd343;
	add.s64 	%rd346, %rd345, %rd342;
	add.s64 	%rd347, %rd346, %rd341;
	shl.b64 	%rd348, %rd347, 2;
	add.s64 	%rd349, %rd4, %rd348;
	st.global.u32 	[%rd349+387072], %r3545;
	cvt.u64.u32 	%rd350, %r74;
	add.s64 	%rd351, %rd345, %rd350;
	add.s64 	%rd352, %rd351, %rd341;
	shl.b64 	%rd353, %rd352, 2;
	add.s64 	%rd354, %rd4, %rd353;
	st.global.u32 	[%rd354+387072], %r3544;
LBB0_42:                                // %L33451
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3542, 0;
	mov.u32 	%r3544, %r3542;
	mov.u32 	%r3545, %r3542;
	mov.u32 	%r3546, %r3542;
	mov.u32 	%r3547, %r3542;
LBB0_43:                                // %L33452
                                        //   in Loop: Header=BB0_18 Depth=2
	mov.u32 	%r2914, 0;
	mov.u32 	%r3548, %r2914;
	mov.u32 	%r3549, %r2914;
	mov.u32 	%r3550, %r2914;
	mov.u32 	%r3551, %r2914;
	@%p50 bra 	LBB0_69;
	bra.uni 	LBB0_44;
LBB0_69:                                // %oksrem13006
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r2915, %r71, %r206;
	mul.wide.s32 	%rd355, %r2915, 4;
	add.s64 	%rd357, %rd26, %rd355;
	ld.shared.u32 	%r3551, [%rd357];
	add.s32 	%r2916, %r71, %r207;
	mul.wide.u32 	%rd358, %r2916, 4;
	add.s64 	%rd359, %rd26, %rd358;
	ld.shared.u32 	%r3550, [%rd359];
	add.s32 	%r2917, %r71, %r208;
	mul.wide.s32 	%rd360, %r2917, 4;
	add.s64 	%rd361, %rd26, %rd360;
	ld.shared.u32 	%r3549, [%rd361];
	add.s32 	%r2918, %r71, %r209;
	mul.wide.u32 	%rd362, %r2918, 4;
	add.s64 	%rd363, %rd26, %rd362;
	ld.shared.u32 	%r3548, [%rd363];
LBB0_44:                                // %L34659
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2951, %r2954}, {%r345, %r342}, {%r3551}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2960, %r2963}, {%r345, %r342}, {%r3550}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2969, %r2972}, {%r345, %r342}, {%r3549}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r2978, %r2981}, {%r345, %r342}, {%r3548}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2947, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2949, %r2947, %r2951;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2952, %r379, %r2954, %r2949;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2956, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2958, %r2956, %r2960;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2961, %r379, %r2963, %r2958;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2965, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2967, %r2965, %r2969;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2970, %r379, %r2972, %r2967;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r2974, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2976, %r2974, %r2978;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2979, %r379, %r2981, %r2976;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2983, %r382, %r2954;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2986, %r379, %r2951, %r2983;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2990, %r382, %r2963;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r2993, %r379, %r2960, %r2990;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r2997, %r382, %r2972;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3000, %r379, %r2969, %r2997;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3004, %r382, %r2981;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3007, %r379, %r2978, %r3004;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3052, %r3056}, {%r412, %r406, %r409, %r403}, {%r2986, %r2952}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3060, %r3064}, {%r412, %r406, %r409, %r403}, {%r2993, %r2961}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3068, %r3072}, {%r412, %r406, %r409, %r403}, {%r3000, %r2970}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3076, %r3080}, {%r412, %r406, %r409, %r403}, {%r3007, %r2979}, {%r2914, %r2914};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3051, %r3052, %r3052, %r3547;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3557, %r3056, %r3056, %r3051;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3059, %r3060, %r3060, %r3546;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3556, %r3064, %r3064, %r3059;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3067, %r3068, %r3068, %r3545;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3555, %r3072, %r3072, %r3067;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3075, %r3076, %r3076, %r3544;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3554, %r3080, %r3080, %r3075;
	// end inline asm
	add.s32 	%r3552, %r3542, 1;
	setp.ne.s32 	%p72, %r3552, 40;
	@%p72 bra 	LBB0_48;
// %bb.45:                              // %L34842
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_47;
// %bb.46:                              // %L34907
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r3083, %r3493, 193536;
	add.s32 	%r3084, %r73, %r3083;
	add.s32 	%r3085, %r3084, %r72;
	add.s32 	%r3086, %r3085, %r44;
	mul.wide.u32 	%rd364, %r3086, 4;
	add.s64 	%rd365, %rd4, %rd364;
	st.global.u32 	[%rd365], %r3557;
	add.s32 	%r3087, %r3084, %r74;
	add.s32 	%r3088, %r3087, %r44;
	mul.wide.u32 	%rd366, %r3088, 4;
	add.s64 	%rd367, %rd4, %rd366;
	st.global.u32 	[%rd367], %r3556;
	cvt.u64.u32 	%rd368, %r44;
	cvt.u64.u32 	%rd369, %r72;
	cvt.u64.u32 	%rd370, %r73;
	cvt.u64.u32 	%rd371, %r3083;
	add.s64 	%rd372, %rd371, %rd370;
	add.s64 	%rd373, %rd372, %rd369;
	add.s64 	%rd374, %rd373, %rd368;
	shl.b64 	%rd375, %rd374, 2;
	add.s64 	%rd376, %rd4, %rd375;
	st.global.u32 	[%rd376+387072], %r3555;
	cvt.u64.u32 	%rd377, %r74;
	add.s64 	%rd378, %rd372, %rd377;
	add.s64 	%rd379, %rd378, %rd368;
	shl.b64 	%rd380, %rd379, 2;
	add.s64 	%rd381, %rd4, %rd380;
	st.global.u32 	[%rd381+387072], %r3554;
LBB0_47:                                // %L35420
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3552, 0;
	mov.u32 	%r3554, %r3552;
	mov.u32 	%r3555, %r3552;
	mov.u32 	%r3556, %r3552;
	mov.u32 	%r3557, %r3552;
LBB0_48:                                // %L35421
                                        //   in Loop: Header=BB0_18 Depth=2
	mov.u32 	%r3090, 0;
	mov.u32 	%r3558, %r3090;
	mov.u32 	%r3559, %r3090;
	mov.u32 	%r3560, %r3090;
	mov.u32 	%r3561, %r3090;
	@%p50 bra 	LBB0_70;
	bra.uni 	LBB0_49;
LBB0_70:                                // %oksrem13760
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3091, %r71, %r210;
	mul.wide.s32 	%rd382, %r3091, 4;
	add.s64 	%rd384, %rd26, %rd382;
	ld.shared.u32 	%r3561, [%rd384];
	add.s32 	%r3092, %r71, %r211;
	mul.wide.u32 	%rd385, %r3092, 4;
	add.s64 	%rd386, %rd26, %rd385;
	ld.shared.u32 	%r3560, [%rd386];
	add.s32 	%r3093, %r71, %r212;
	mul.wide.s32 	%rd387, %r3093, 4;
	add.s64 	%rd388, %rd26, %rd387;
	ld.shared.u32 	%r3559, [%rd388];
	add.s32 	%r3094, %r71, %r213;
	mul.wide.u32 	%rd389, %r3094, 4;
	add.s64 	%rd390, %rd26, %rd389;
	ld.shared.u32 	%r3558, [%rd390];
LBB0_49:                                // %L36628
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3127, %r3130}, {%r345, %r342}, {%r3561}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3136, %r3139}, {%r345, %r342}, {%r3560}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3145, %r3148}, {%r345, %r342}, {%r3559}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3154, %r3157}, {%r345, %r342}, {%r3558}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3123, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3125, %r3123, %r3127;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3128, %r379, %r3130, %r3125;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3132, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3134, %r3132, %r3136;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3137, %r379, %r3139, %r3134;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3141, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3143, %r3141, %r3145;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3146, %r379, %r3148, %r3143;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3150, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3152, %r3150, %r3154;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3155, %r379, %r3157, %r3152;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3159, %r382, %r3130;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3162, %r379, %r3127, %r3159;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3166, %r382, %r3139;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3169, %r379, %r3136, %r3166;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3173, %r382, %r3148;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3176, %r379, %r3145, %r3173;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3180, %r382, %r3157;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3183, %r379, %r3154, %r3180;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3228, %r3232}, {%r412, %r406, %r409, %r403}, {%r3162, %r3128}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3236, %r3240}, {%r412, %r406, %r409, %r403}, {%r3169, %r3137}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3244, %r3248}, {%r412, %r406, %r409, %r403}, {%r3176, %r3146}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3252, %r3256}, {%r412, %r406, %r409, %r403}, {%r3183, %r3155}, {%r3090, %r3090};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3227, %r3228, %r3228, %r3557;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3567, %r3232, %r3232, %r3227;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3235, %r3236, %r3236, %r3556;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3566, %r3240, %r3240, %r3235;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3243, %r3244, %r3244, %r3555;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3565, %r3248, %r3248, %r3243;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3251, %r3252, %r3252, %r3554;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3564, %r3256, %r3256, %r3251;
	// end inline asm
	add.s32 	%r3562, %r3552, 1;
	setp.ne.s32 	%p75, %r3562, 40;
	@%p75 bra 	LBB0_53;
// %bb.50:                              // %L36811
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_52;
// %bb.51:                              // %L36876
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r3259, %r3493, 193536;
	add.s32 	%r3260, %r73, %r3259;
	add.s32 	%r3261, %r3260, %r72;
	add.s32 	%r3262, %r3261, %r44;
	mul.wide.u32 	%rd391, %r3262, 4;
	add.s64 	%rd392, %rd4, %rd391;
	st.global.u32 	[%rd392], %r3567;
	add.s32 	%r3263, %r3260, %r74;
	add.s32 	%r3264, %r3263, %r44;
	mul.wide.u32 	%rd393, %r3264, 4;
	add.s64 	%rd394, %rd4, %rd393;
	st.global.u32 	[%rd394], %r3566;
	cvt.u64.u32 	%rd395, %r44;
	cvt.u64.u32 	%rd396, %r72;
	cvt.u64.u32 	%rd397, %r73;
	cvt.u64.u32 	%rd398, %r3259;
	add.s64 	%rd399, %rd398, %rd397;
	add.s64 	%rd400, %rd399, %rd396;
	add.s64 	%rd401, %rd400, %rd395;
	shl.b64 	%rd402, %rd401, 2;
	add.s64 	%rd403, %rd4, %rd402;
	st.global.u32 	[%rd403+387072], %r3565;
	cvt.u64.u32 	%rd404, %r74;
	add.s64 	%rd405, %rd399, %rd404;
	add.s64 	%rd406, %rd405, %rd395;
	shl.b64 	%rd407, %rd406, 2;
	add.s64 	%rd408, %rd4, %rd407;
	st.global.u32 	[%rd408+387072], %r3564;
LBB0_52:                                // %L37389
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3493, %r3493, 1;
	mov.u32 	%r3562, 0;
	mov.u32 	%r3564, %r3562;
	mov.u32 	%r3565, %r3562;
	mov.u32 	%r3566, %r3562;
	mov.u32 	%r3567, %r3562;
LBB0_53:                                // %L37390
                                        //   in Loop: Header=BB0_18 Depth=2
	mov.u32 	%r3266, 0;
	mov.u32 	%r3568, %r3266;
	mov.u32 	%r3569, %r3266;
	mov.u32 	%r3570, %r3266;
	mov.u32 	%r3571, %r3266;
	@%p50 bra 	LBB0_71;
	bra.uni 	LBB0_54;
LBB0_71:                                // %oksrem14514
                                        //   in Loop: Header=BB0_18 Depth=2
	add.s32 	%r3267, %r71, %r214;
	mul.wide.s32 	%rd409, %r3267, 4;
	add.s64 	%rd411, %rd26, %rd409;
	ld.shared.u32 	%r3571, [%rd411];
	add.s32 	%r3268, %r71, %r215;
	mul.wide.u32 	%rd412, %r3268, 4;
	add.s64 	%rd413, %rd26, %rd412;
	ld.shared.u32 	%r3570, [%rd413];
	add.s32 	%r3269, %r71, %r216;
	mul.wide.s32 	%rd414, %r3269, 4;
	add.s64 	%rd415, %rd26, %rd414;
	ld.shared.u32 	%r3569, [%rd415];
	add.s32 	%r3270, %r71, %r217;
	mul.wide.u32 	%rd416, %r3270, 4;
	add.s64 	%rd417, %rd26, %rd416;
	ld.shared.u32 	%r3568, [%rd417];
LBB0_54:                                // %L38597
                                        //   in Loop: Header=BB0_18 Depth=2
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3303, %r3306}, {%r345, %r342}, {%r3571}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3312, %r3315}, {%r345, %r342}, {%r3570}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3321, %r3324}, {%r345, %r342}, {%r3569}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%r3330, %r3333}, {%r345, %r342}, {%r3568}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3299, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3301, %r3299, %r3303;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3304, %r379, %r3306, %r3301;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3308, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3310, %r3308, %r3312;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3313, %r379, %r3315, %r3310;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3317, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3319, %r3317, %r3321;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3322, %r379, %r3324, %r3319;
	// end inline asm
	// begin inline asm
	neg.f16x2 %r3326, %r382;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3328, %r3326, %r3330;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3331, %r379, %r3333, %r3328;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3335, %r382, %r3306;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3338, %r379, %r3303, %r3335;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3342, %r382, %r3315;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3345, %r379, %r3312, %r3342;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3349, %r382, %r3324;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3352, %r379, %r3321, %r3349;
	// end inline asm
	// begin inline asm
	mul.rn.f16x2 %r3356, %r382, %r3333;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3359, %r379, %r3330, %r3356;
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3404, %r3408}, {%r412, %r406, %r409, %r403}, {%r3338, %r3304}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3412, %r3416}, {%r412, %r406, %r409, %r403}, {%r3345, %r3313}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3420, %r3424}, {%r412, %r406, %r409, %r403}, {%r3352, %r3322}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%r3428, %r3432}, {%r412, %r406, %r409, %r403}, {%r3359, %r3331}, {%r3266, %r3266};
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3403, %r3404, %r3404, %r3567;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3497, %r3408, %r3408, %r3403;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3411, %r3412, %r3412, %r3566;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3496, %r3416, %r3416, %r3411;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3419, %r3420, %r3420, %r3565;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3495, %r3424, %r3424, %r3419;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3427, %r3428, %r3428, %r3564;
	// end inline asm
	// begin inline asm
	fma.rn.f16x2 %r3494, %r3432, %r3432, %r3427;
	// end inline asm
	add.s32 	%r3492, %r3562, 1;
	setp.ne.s32 	%p78, %r3492, 40;
	@%p78 bra 	LBB0_58;
// %bb.55:                              // %L38780
                                        //   in Loop: Header=BB0_18 Depth=2
	@%p26 bra 	LBB0_57;
// %bb.56:                              // %L38845
                                        //   in Loop: Header=BB0_18 Depth=2
	mul.lo.s32 	%r3435, %r3493, 193536;
	add.s32 	%r3436, %r73, %r3435;
	add.s32 	%r3437, %r3436, %r72;
	add.s32 	%r3438, %r3437, %r44;
	mul.wide.u32 	%rd418, %r3438, 4;
	add.s64 	%rd419, %rd4, %rd418;
	st.global.u32 	[%rd419], %r3497;
	add.s32 	%r3439, %r3436, %r74;
	add.s32 	%r3440, %r3439, %r44;
	mul.wide.u32 	%rd420, %r3440, 4;
	add.s64 	%rd421, %rd4, %rd420;
	st.global.u32 	[%rd421], %r3496;
	cvt.u64.u32 	%rd422, %r44;
	cvt.u64.u32 	%rd423, %r72;
	cvt.u64.u32 	%rd424, %r73;
	cvt.u64.u32 	%rd425, %r3435;
	add.s64 	%rd426, %rd425, %rd424;
	add.s64 	%rd427, %rd426, %rd423;
	add.s64 	%rd428, %rd427, %rd422;
	shl.b64 	%rd429, %rd428, 2;
	add.s64 	%rd430, %rd4, %rd429;
	st.global.u32 	[%rd430+387072], %r3495;
	cvt.u64.u32 	%rd431, %r74;
	add.s64 	%rd432, %rd426, %rd431;
	add.s64 	%rd433, %rd432, %rd422;
	shl.b64 	%rd434, %rd433, 2;
	add.s64 	%rd435, %rd4, %rd434;
	st.global.u32 	[%rd435+387072], %r3494;
	bra.uni 	LBB0_57;
LBB0_60:                                // %guard_pass16273
	@%p26 bra 	LBB0_62;
// %bb.61:                              // %L39476
	mul.lo.s32 	%r3442, %r3493, 193536;
	add.s32 	%r3443, %r73, %r3442;
	add.s32 	%r3444, %r3443, %r72;
	add.s32 	%r3445, %r3444, %r44;
	mul.wide.u32 	%rd436, %r3445, 4;
	add.s64 	%rd437, %rd4, %rd436;
	st.global.u32 	[%rd437], %r3497;
	add.s32 	%r3446, %r3443, %r74;
	add.s32 	%r3447, %r3446, %r44;
	mul.wide.u32 	%rd438, %r3447, 4;
	add.s64 	%rd439, %rd4, %rd438;
	st.global.u32 	[%rd439], %r3496;
	cvt.u64.u32 	%rd440, %r44;
	cvt.u64.u32 	%rd441, %r72;
	cvt.u64.u32 	%rd442, %r73;
	cvt.u64.u32 	%rd443, %r3442;
	add.s64 	%rd444, %rd443, %rd442;
	add.s64 	%rd445, %rd444, %rd441;
	add.s64 	%rd446, %rd445, %rd440;
	shl.b64 	%rd447, %rd446, 2;
	add.s64 	%rd448, %rd4, %rd447;
	st.global.u32 	[%rd448+387072], %r3495;
	cvt.u64.u32 	%rd449, %r74;
	add.s64 	%rd450, %rd444, %rd449;
	add.s64 	%rd451, %rd450, %rd440;
	shl.b64 	%rd452, %rd451, 2;
	add.s64 	%rd453, %rd4, %rd452;
	st.global.u32 	[%rd453+387072], %r3494;
LBB0_62:                                // %L39989
	mov.u32 	%r3448, 0;
	st.global.u32 	[%rd5], %r3448;
	ret;
LBB0_1:                                 // %L9
	ld.param.u64 	%rd11, [_Z15julia_frb_1283013CuDeviceArrayI5Int32Li1ELi1EES_I9Float16x2Li1ELi1EES_I6Int4x8Li1ELi1EES_IS1_Li1ELi1EES_IS0_Li1ELi1EES_IS2_Li1ELi1EES_IS2_Li1ELi1EES_IS1_Li1ELi1EE_param_0];
	mov.u64 	%rd16, exception1;
	cvta.global.u64 	%rd17, %rd16;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd17;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 4
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd11;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 5
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}
